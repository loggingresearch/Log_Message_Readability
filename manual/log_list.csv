System,ID,Callsite,Line,Level,Log,Structural,Information,Wording
Cassandra,6,org.apache.cassandra.auth.CassandraAuthorizer.convertLegacyData,455,info,java.lang.String(name)Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). Conversion should not be considered complete,o,o,o
Cassandra,2,org.apache.cassandra.auth.CassandraAuthorizer.revokeAllFrom,154,warn,java.lang.String(name)CassandraAuthorizer failed to revoke all permissions of {}: {}-java.lang.String(name)revokee.getRoleName()-org.apache.cassandra.exceptions.CassandraException(name)e,o,o,o
Cassandra,3,org.apache.cassandra.auth.CassandraAuthorizer.revokeAllOn,191,warn,java.lang.String(name)CassandraAuthorizer failed to revoke all permissions on {}: {}-org.apache.cassandra.auth.IResource(name)droppedResource-org.apache.cassandra.exceptions.CassandraException(name)e,o,o,o
Cassandra,10,org.apache.cassandra.auth.CassandraRoleManager.apply,98,warn,"java.lang.String(name)An invalid value has been detected in the {} table for role {}. If you are unable to login, you may need to disable authentication and confirm that values in that table are accurate-java.lang.String(name)roles-java.lang.String(name)row.getString(""role"")",o,o,o
Cassandra,19,org.apache.cassandra.auth.CassandraRoleManager.convertLegacyData,452,info,java.lang.String(name)Unable to complete conversion of legacy auth data (perhaps not enough nodes are upgraded yet). Conversion should not be considered complete,o,o,o
Cassandra,13,org.apache.cassandra.auth.CassandraRoleManager.scheduleSetupTask,388,trace,"java.lang.String(name)Not all nodes are upgraded to a version that supports Roles yet, rescheduling setup task",o,o,o
Cassandra,34,org.apache.cassandra.auth.jmx.AuthorizationProxy.getRequiredPermission,432,debug,"java.lang.String(name)Access denied, method name {} does not map to any defined permission-java.lang.String(name)methodName",o,o,o
Cassandra,48,org.apache.cassandra.batchlog.LegacyBatchlogMigrator.asyncRemoveFromBatchlog,154,trace,java.lang.String(name)Sending legacy batchlog remove request {} to {}-java.util.UUID(name)uuid-java.net.InetAddress(name)target,o,o,o
Cassandra,50,org.apache.cassandra.cache.AutoSavingCache.loadSaved,271,trace,java.lang.String(name)completed reading ({} ms; {} keys) saved cache {}-long(name)TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start)-int(name)count-java.io.File(name)dataPath,o,o,x
Cassandra,54,org.apache.cassandra.client.RingCache.refreshEndpointMap,96,trace,java.lang.String(name)Error contacting seed list {} {}-java.lang.String(name)ConfigHelper.getOutputInitialAddress(conf)-java.lang.String(name)e.getMessage(),x,o,o
Cassandra,55,org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.handleOrLog,239,error,java.lang.String(name)Error in ThreadPoolExecutor-java.lang.Throwable(name)t,o,x,x
Cassandra,67,org.apache.cassandra.config.CFMetaData.apply,926,debug,java.lang.String(name)application result is {}-org.apache.cassandra.config.CFMetaData(name)this,o,o,x
Cassandra,68,org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig,353,debug,java.lang.String(name)Syncing log with a batch window of {}-double(name)conf.commitlog_sync_batch_window_in_ms,o,o,o
Cassandra,80,org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig,556,warn,java.lang.String(name)Only {} free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots-java.lang.String(name)FBUtilities.prettyPrintMemory(dataFreeBytes),o,o,o
Cassandra,75,org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig,480,warn,java.lang.String(name)Small commitlog volume detected at {}; setting commitlog_total_space_in_mb to {}. You can override this in cassandra.yaml-java.lang.String(name)conf.commitlog_directory-int(name)minSize,o,o,o
Cassandra,77,org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig,507,warn,java.lang.String(name)Small cdc volume detected at {}; setting cdc_total_space_in_mb to {}. You can override this in cassandra.yaml-java.lang.String(name)conf.cdc_raw_directory-int(name)minSize,o,o,o
Cassandra,82,org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig,588,warn,"java.lang.String(name)memtable_cleanup_threshold is set very low [{}], which may cause performance degradation-java.lang.Float(name)conf.memtable_cleanup_threshold",o,o,o
Cassandra,65,org.apache.cassandra.config.YamlConfigurationLoader.loadConfig,108,debug,java.lang.String(name)Loading settings from {}-java.net.URL(name)url,o,o,o
Cassandra,96,org.apache.cassandra.cql3.functions.ScriptBasedUDFunction.ScriptBasedUDFunction,153,info,java.lang.String(name)Failed to compile function '{}' for language {}: -org.apache.cassandra.cql3.functions.FunctionName(name)name-java.lang.String(name)language-java.lang.Throwable(name)e,o,o,o
Cassandra,89,org.apache.cassandra.cql3.QueryProcessor.preloadPreparedStatement,160,warn,java.lang.String(name)prepared statement recreation error: {}-java.lang.String(name)useKeyspaceAndCQL.right-org.apache.cassandra.exceptions.RequestValidationException(name)e,o,o,x
Cassandra,92,org.apache.cassandra.cql3.QueryProcessor.processPrepared,528,trace,java.lang.String(name)[{}] '{}'-int(name)i + 1-java.nio.ByteBuffer(name)variables.get(i),x,x,x
Cassandra,91,org.apache.cassandra.cql3.QueryProcessor.processStatement,220,trace,java.lang.String(name)Process {} @CL.{}-org.apache.cassandra.cql3.CQLStatement(name)statement-org.apache.cassandra.cql3.ConsistencyLevel(name)options.getConsistency(),o,x,x
Cassandra,111,org.apache.cassandra.cql3.statements.CreateIndexStatement.announceMigration,253,trace,java.lang.String(name)Updating index definition for {}-java.lang.String(name)indexName,o,o,o
Cassandra,106,org.apache.cassandra.cql3.statements.CreateViewStatement.announceMigration,330,warn,java.lang.String(name)Creating materialized view {} for {}.{}. Materialized views are experimental and are not recommended for production use.-java.lang.String(name)definition.viewName-java.lang.String(name)cfm.ksName-java.lang.String(name)cfm.cfName,o,o,o
Cassandra,164,org.apache.cassandra.db.ColumnFamilyStore.flushMemtable,1233,debug,"java.lang.String(name)Flushed to {} ({} sstables, {}), biggest {}, smallest {}-java.util.List<SSTableReader>(name)sstables-int(name)sstables.size()-java.lang.String(name)FBUtilities.prettyPrintMemory(totalBytesOnDisk)-java.lang.String(name)FBUtilities.prettyPrintMemory(maxBytesOnDisk)-java.lang.String(name)FBUtilities.prettyPrintMemory(minBytesOnDisk)",o,o,o
Cassandra,166,org.apache.cassandra.db.ColumnFamilyStore.getOverlappingLiveSSTables,1381,trace,java.lang.String(name)Checking for sstables overlapping {}-java.lang.Iterable<SSTableReader>(name)sstables,o,o,o
Cassandra,155,org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables,763,error,"java.lang.String(name)Cannot read sstable {}; other IO error, skipping table-java.util.Entry<Descriptor,Set<Component>>(name)entry-java.io.IOException(name)e",o,o,o
Cassandra,158,org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables,799,error,"java.lang.String(name)Cannot read sstable {}; file system error, skipping table-java.util.Entry<Descriptor,Set<Component>>(name)entry-org.apache.cassandra.io.FSError(name)ex",o,o,o
Cassandra,159,org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables,805,error,"java.lang.String(name)Cannot read sstable {}; other IO error, skipping table-java.util.Entry<Descriptor,Set<Component>>(name)entry-java.io.IOException(name)ex",o,o,o
Cassandra,151,org.apache.cassandra.db.ColumnFamilyStore.scheduleFlush,314,trace,java.lang.String(name)scheduling flush in {} ms-int(name)period,o,o,x
Cassandra,152,org.apache.cassandra.db.ColumnFamilyStore.setCompactionParameters,376,error,java.lang.String(name)Could not set new local compaction strategy-java.lang.Throwable(name)t,o,o,o
Cassandra,167,org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush,1815,trace,java.lang.String(name)Snapshot for {} keyspace data file {} created in {}-org.apache.cassandra.db.Keyspace(name)keyspace-java.lang.String(name)ssTable.getFilename()-java.io.File(name)snapshotDirectory,o,o,o
Cassandra,175,org.apache.cassandra.db.ColumnFamilyStore.truncateBlocking,2242,info,java.lang.String(name)Truncate of {}.{} is complete-java.lang.String(name)keyspace.getName()-java.lang.String(name)name,o,o,o
Cassandra,172,org.apache.cassandra.db.ColumnFamilyStore.truncateBlocking,2189,info,java.lang.String(name)Truncating {}.{}-java.lang.String(name)keyspace.getName()-java.lang.String(name)name,o,x,o
Cassandra,197,org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.stopUnsafe,403,debug,java.lang.String(name)CLSM closing and clearing existing commit log segments...,o,o,x
Cassandra,202,org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments,313,trace,"java.lang.String(name)Not safe to delete{} commit log segment {}; dirty is {}-java.lang.String(name)(iter.hasNext() ? """" : "" active"")-org.apache.cassandra.db.commitlog.CommitLogSegment(name)segment-java.lang.String(name)segment.dirtyString()",x,o,o
Cassandra,203,org.apache.cassandra.db.commitlog.CommitLog.handleCommitError,466,error,java.lang.String(name)message-java.lang.Throwable(name)t,x,x,x
Cassandra,205,org.apache.cassandra.db.commitlog.CommitLogArchiver.maybeArchive,179,warn,"java.lang.String(name)Archiving file {} failed, file may have already been archived.-java.lang.String(name)name-java.io.IOException(name)e",o,o,o
Cassandra,208,org.apache.cassandra.db.commitlog.EncryptedSegment.EncryptedSegment,80,debug,java.lang.String(name)created a new encrypted commit log segment: {}-java.io.File(name)logFile,o,o,x
Cassandra,268,org.apache.cassandra.db.compaction.CompactionManager.afterExecute,1809,info,java.lang.String(name)t.getMessage(),x,x,x
Cassandra,263,org.apache.cassandra.db.compaction.CompactionManager.antiCompactGroup,1559,info,java.lang.String(name)Anticompacting {}-org.apache.cassandra.db.lifecycle.LifecycleTransaction(name)anticompactionGroup,o,x,o
Cassandra,261,org.apache.cassandra.db.compaction.CompactionManager.doAntiCompaction,1538,info,java.lang.String(name)format-int(name)numAnticompact-int(name)antiCompactedSSTableCount,x,x,x
Cassandra,252,org.apache.cassandra.db.compaction.CompactionManager.forceUserDefinedCleanup,880,error,java.lang.String(name)Cleanup cannot run before a node has joined the ring,o,o,o
Cassandra,254,org.apache.cassandra.db.compaction.CompactionManager.forceUserDefinedCleanup,905,error,java.lang.String(name)forceUserDefinedCleanup failed: {}-java.lang.String(name)e.getLocalizedMessage(),o,x,x
Cassandra,253,org.apache.cassandra.db.compaction.CompactionManager.forceUserDefinedCleanup,894,warn,"java.lang.String(name)Will not clean {}, it is not an active sstable-org.apache.cassandra.io.sstable.Descriptor(name)entry.getValue()",o,o,o
Cassandra,251,org.apache.cassandra.db.compaction.CompactionManager.forceUserDefinedCleanup,868,warn,java.lang.String(name)Schema does not exist for file {}. Skipping.-java.lang.String(name)filename,o,o,o
Cassandra,250,org.apache.cassandra.db.compaction.CompactionManager.forceUserDefinedCompaction,842,warn,java.lang.String(name)Schema does not exist for file {}. Skipping.-java.lang.String(name)filename,o,o,o
Cassandra,247,org.apache.cassandra.db.compaction.CompactionManager.relocateSSTables,580,debug,java.lang.String(name)Relocating {}-java.util.Set<SSTableReader>(name)txn.originals(),o,x,o
Cassandra,246,org.apache.cassandra.db.compaction.CompactionManager.relocateSSTables,527,info,java.lang.String(name)Relocate cannot run before a node has joined the ring,o,o,o
Cassandra,242,org.apache.cassandra.db.compaction.CompactionManager.run,252,trace,java.lang.String(name)Checking {}.{}-java.lang.String(name)cfs.keyspace.getName()-java.lang.String(name)cfs.name,o,x,o
Cassandra,238,org.apache.cassandra.db.compaction.CompactionManager.submitBackground,169,trace,java.lang.String(name)Background compaction is still running for {}.{} ({} remaining). Skipping-java.lang.String(name)cfs.keyspace.getName()-java.lang.String(name)cfs.name-int(name)count,o,o,o
Cassandra,283,org.apache.cassandra.db.compaction.CompactionTask.checkAvailableDiskSpace,350,warn,java.lang.String(name)msg,x,x,x
Cassandra,277,org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask,127,trace,java.lang.String(name)No compaction necessary for {}-org.apache.cassandra.db.compaction.LeveledCompactionStrategy(name)this,o,o,o
Cassandra,280,org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners,285,warn,"java.lang.String(name)Live sstable {} from level {} is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.-java.lang.String(name)sstable.getFilename()-int(name)level",o,o,o
Cassandra,279,org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getUserDefinedTask,187,trace,java.lang.String(name)Unable to mark {} for compaction; probably a background compaction got to it first. You can disable background compactions temporarily if this is a problem-java.util.Collection<SSTableReader>(name)sstables,o,o,o
Cassandra,225,org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates,376,trace,java.lang.String(name)Compaction score for level {} is {}-int(name)i-double(name)score,o,o,o
Cassandra,223,org.apache.cassandra.db.compaction.LeveledManifest.repairOverlappingSSTables,234,warn,"java.lang.String(name)At level {}, {} [{}, {}] overlaps {} [{}, {}]. This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0. If you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable-int(name)level-org.apache.cassandra.io.sstable.format.SSTableReader(name)previous-org.apache.cassandra.db.DecoratedKey(name)previous.first-org.apache.cassandra.db.DecoratedKey(name)previous.last-org.apache.cassandra.io.sstable.format.SSTableReader(name)current-org.apache.cassandra.db.DecoratedKey(name)current.first-org.apache.cassandra.db.DecoratedKey(name)current.last",x,o,o
Cassandra,234,org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask,192,warn,"java.lang.String(name)Could not acquire references for compacting SSTables {} which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.-java.util.List<SSTableReader>(name)hottestBucket",o,o,o
Cassandra,308,org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.getWriteDirectory,226,trace,java.lang.String(name)putting compaction results in {}-java.io.File(name)directory,o,o,x
Cassandra,143,org.apache.cassandra.db.Directories.Directories,245,error,java.lang.String(name)Failed to create {} directory-java.io.File(name)dir,o,o,o
Cassandra,140,org.apache.cassandra.db.Directories.verifyFullPermissions,120,error,java.lang.String(name)Doesn't have execute permissions for {} directory-java.lang.String(name)dataDir,o,o,o
Cassandra,141,org.apache.cassandra.db.Directories.verifyFullPermissions,125,error,java.lang.String(name)Doesn't have read permissions for {} directory-java.lang.String(name)dataDir,o,o,o
Cassandra,142,org.apache.cassandra.db.Directories.verifyFullPermissions,130,error,java.lang.String(name)Doesn't have write permissions for {} directory-java.lang.String(name)dataDir,o,o,o
Cassandra,135,org.apache.cassandra.db.Keyspace.applyInternal,527,trace,java.lang.String(name)Could not acquire lock for {} and table {}-java.lang.String(name)ByteBufferUtil.bytesToHex(mutation.key().getKey())-java.lang.String(name)columnFamilyStores.get(cfid).name,o,o,o
Cassandra,138,org.apache.cassandra.db.Keyspace.getIndexColumnFamilyStores,718,info,java.lang.String(name)adding secondary index table {} to operation-java.lang.String(name)indexCfs.metadata.cfName,o,o,x
Cassandra,319,org.apache.cassandra.db.lifecycle.LogReplicaSet.readRecords,130,error,"java.lang.String(name)Mismatched line in file {}: got '{}' expected '{}', giving up-java.lang.String(name)entry.getKey().getFileName()-java.lang.String(name)currentLine-java.lang.String(name)firstLine",o,o,o
Cassandra,321,org.apache.cassandra.db.lifecycle.LogReplicaSet.readRecords,154,error,"java.lang.String(name)Mismatched line in file {}: got '{}' expected '{}', giving up-java.lang.String(name)entry.getKey().getFileName()-java.lang.String(name)currentLine-java.lang.String(name)firstLine",o,o,o
Cassandra,337,org.apache.cassandra.db.lifecycle.LogTransaction.complete,397,error,java.lang.String(name)Failed to complete file transaction id {}-java.util.UUID(name)id()-java.lang.Throwable(name)t,o,o,o
Cassandra,338,org.apache.cassandra.db.lifecycle.LogTransaction.removeUnfinishedLeftovers,483,error,java.lang.String(name)Failed to remove unfinished transaction leftovers for transaction log {}-java.lang.String(name)txn.toString(true)-java.lang.Throwable(name)failure,o,o,o
Cassandra,331,org.apache.cassandra.db.lifecycle.LogTransaction.run,272,error,"java.lang.String(name)Transaction log {} indicates txn was not completed, trying to abort it now-org.apache.cassandra.db.lifecycle.LogFile(name)data",o,o,x
Cassandra,343,org.apache.cassandra.db.marshal.DateType.isCompatibleWith,94,warn,"java.lang.String(name)Changing from TimestampType to DateType is allowed, but be wary that they sort differently for pre-unix-epoch timestamps (negative timestamp values) and thus this change will corrupt your data if you have such negative timestamp. There is no reason to switch from DateType to TimestampType except if you were using DateType in the first place and switched to TimestampType by mistake.",o,x,o
Cassandra,341,org.apache.cassandra.db.marshal.DynamicCompositeType.validateComparator,207,error,java.lang.String(name)Failed with [{}] when decoding the byte buffer in ByteBufferUtil.string()-java.lang.String(name)ce.toString(),o,o,o
Cassandra,342,org.apache.cassandra.db.marshal.DynamicCompositeType.validateComparator,214,error,"java.lang.String(name)Failed to parse value string ""{}"" with exception: [{}]-java.lang.String(name)valueStr-java.lang.String(name)e.toString()",o,o,o
Cassandra,340,org.apache.cassandra.db.marshal.TimestampType.isCompatibleWith,105,warn,"java.lang.String(name)Changing from DateType to TimestampType is allowed, but be wary that they sort differently for pre-unix-epoch timestamps (negative timestamp values) and thus this change will corrupt your data if you have such negative timestamp. So unless you know that you don't have *any* pre-unix-epoch timestamp you should change back to DateType",o,x,o
Cassandra,346,org.apache.cassandra.db.monitoring.MonitoringTask.logFailedOperations,152,debug,java.lang.String(name){} operations timed out in the last {} msecs:{}{}-long(name)failedOperations.num()-long(name)elapsed-java.lang.String(name)LINE_SEPARATOR-java.lang.String(name)failedOperations.getLogMessage(),x,o,o
Cassandra,345,org.apache.cassandra.db.monitoring.MonitoringTask.logFailedOperations,149,warn,"java.lang.String(name)Some operations timed out, details available at debug level (debug.log)",o,o,o
Cassandra,348,org.apache.cassandra.db.monitoring.MonitoringTask.logSlowOperations,173,debug,java.lang.String(name){} operations were slow in the last {} msecs:{}{}-long(name)slowOperations.num()-long(name)elapsed-java.lang.String(name)LINE_SEPARATOR-java.lang.String(name)slowOperations.getLogMessage(),x,o,o
Cassandra,347,org.apache.cassandra.db.monitoring.MonitoringTask.logSlowOperations,170,info,"java.lang.String(name)Some operations were slow, details available at debug level (debug.log)",o,o,o
Cassandra,131,org.apache.cassandra.db.ReadCommand.deserializeIndexMetadata,776,info,"java.lang.String(name)Couldn't find a defined index on {}.{} with the id {}. If an index was just created, this is likely due to the schema not being fully propagated. Local read will proceed without using the index. Please wait for schema agreement after index creation.-java.lang.String(name)cfm.ksName-java.lang.String(name)cfm.cfName-java.util.UUID(name)e.indexId",o,o,o
Cassandra,350,org.apache.cassandra.db.rows.RowIterators.loggingIterator,110,info,java.lang.String(name)[{}] {}-java.lang.String(name)id-java.lang.String(name)row.toString(metadata),x,x,x
Cassandra,351,org.apache.cassandra.db.rows.RowIterators.loggingIterator,117,info,java.lang.String(name)[{}] {}-java.lang.String(name)id-java.lang.String(name)row.toString(metadata),x,x,x
Cassandra,354,org.apache.cassandra.db.rows.UnfilteredRowIterators.loggingIterator,368,info,"java.lang.String(name)[{}] {}-java.lang.String(name)id-java.lang.String(name)row.toString(metadata,fullDetails)",x,x,x
Cassandra,132,org.apache.cassandra.db.TruncateVerbHandler.doVerb,44,error,java.lang.String(name)Error in truncation-java.lang.Exception(name)e,o,x,o
Cassandra,356,org.apache.cassandra.db.view.View.getReadQuery,199,trace,java.lang.String(name)View query: {}-org.apache.cassandra.cql3.statements.RawStatement(name)rawSelect,o,o,o
Cassandra,369,org.apache.cassandra.db.view.ViewBuilder.updateDistributed,208,warn,"java.lang.String(name)Failed to updated the distributed status of view, sleeping 5 minutes before retrying-java.lang.Exception(name)e",o,o,o
Cassandra,359,org.apache.cassandra.db.view.ViewManager.addView,155,warn,java.lang.String(name)Not adding view {} because the base table {} is unknown-java.lang.String(name)definition.viewName-java.util.UUID(name)definition.baseTableId,o,o,o
Cassandra,372,org.apache.cassandra.dht.BootStrapper.getBootstrapTokens,180,warn,java.lang.String(name)Picking random token for a single vnode. You should probably add more vnodes and/or use the automatic token allocation mechanism.,o,o,o
Cassandra,378,org.apache.cassandra.dht.RangeStreamer.fetchAsync,384,info,java.lang.String(name)Some ranges of {} are already available. Skipping streaming those ranges.-java.util.Set<Range<Token>>(name)availableRanges,o,o,o
Cassandra,377,org.apache.cassandra.dht.RangeStreamer.getRangeFetchMap,347,warn,java.lang.String(name)Unable to find sufficient sources for streaming range {} in keyspace {} with RF=1. Keyspace might be missing data.-org.apache.cassandra.dht.Range<Token>(name)range-java.lang.String(name)keyspace,o,o,o
Cassandra,380,org.apache.cassandra.dht.tokenallocator.TokenAllocation.allocateTokens,63,warn,java.lang.String(name)Selected tokens {}-java.util.Collection<Token>(name)tokens,o,o,o
Cassandra,384,org.apache.cassandra.dht.tokenallocator.TokenAllocatorFactory.createTokenAllocator,39,info,java.lang.String(name)Using NoReplicationTokenAllocator.,o,x,o
Cassandra,385,org.apache.cassandra.dht.tokenallocator.TokenAllocatorFactory.createTokenAllocator,42,info,java.lang.String(name)Using ReplicationAwareTokenAllocator.,o,x,o
Cassandra,479,org.apache.cassandra.gms.FailureDetector.interpret,302,debug,java.lang.String(name)PHI for {} : {}-java.net.InetAddress(name)ep-double(name)phi,o,x,x
Cassandra,481,org.apache.cassandra.gms.FailureDetector.interpret,307,trace,java.lang.String(name)mean for {} : {}-java.net.InetAddress(name)ep-double(name)hbWnd.mean(),o,x,x
Cassandra,388,org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb,52,warn,java.lang.String(name)ClusterName mismatch from {} {}!={}-java.net.InetAddress(name)from-java.lang.String(name)gDigestMessage.clusterId-java.lang.String(name)DatabaseDescriptor.getClusterName(),x,o,o
Cassandra,389,org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb,58,warn,java.lang.String(name)Partitioner mismatch from {} {}!={}-java.net.InetAddress(name)from-java.lang.String(name)gDigestMessage.partioner-java.lang.String(name)DatabaseDescriptor.getPartitionerName(),x,o,o
Cassandra,448,org.apache.cassandra.gms.Gossiper.addSavedEndpoint,1504,trace,java.lang.String(name)Adding saved endpoint {} {}-java.net.InetAddress(name)ep-int(name)epState.getHeartBeatState().getGeneration(),x,o,o
Cassandra,404,org.apache.cassandra.gms.Gossiper.advertiseRemoving,526,info,java.lang.String(name)Sleeping for {}ms to ensure {} does not change-int(name)StorageService.RING_DELAY-java.net.InetAddress(name)endpoint,x,o,o
Cassandra,403,org.apache.cassandra.gms.Gossiper.advertiseRemoving,525,info,java.lang.String(name)Removing host: {}-java.util.UUID(name)hostId,o,o,o
Cassandra,440,org.apache.cassandra.gms.Gossiper.applyStateLocally,1185,info,"java.lang.String(name)any30 ? ""There is at least one 3.0 node in the cluster - will store and announce compatible schema version"" : ""There are no 3.0 nodes in the cluster - will store and announce real schema version""",o,o,o
Cassandra,419,org.apache.cassandra.gms.Gossiper.getStateForVersionBiggerThan,886,trace,java.lang.String(name)local heartbeat version {} greater than {} for {}-int(name)localHbVersion-int(name)version-java.net.InetAddress(name)forEndpoint,o,o,x
Cassandra,431,org.apache.cassandra.gms.Gossiper.handleMajorStateChange,1047,info,java.lang.String(name)Node {} is now part of the cluster-java.net.InetAddress(name)ep,o,o,o
Cassandra,454,org.apache.cassandra.gms.Gossiper.maybeFinishShadowRound,1599,debug,"java.lang.String(name)All seeds are in a shadow round, clearing this node to exit its own",o,o,o
Cassandra,451,org.apache.cassandra.gms.Gossiper.maybeFinishShadowRound,1581,warn,"java.lang.String(name)Received an ack from {}, who isn't a seed. Ensure your seed list includes a live node. Exiting shadow round-java.net.InetAddress(name)respondent",o,o,x
Cassandra,424,org.apache.cassandra.gms.Gossiper.realMarkAlive,1010,debug,java.lang.String(name)removing expire time for endpoint : {}-java.net.InetAddress(name)addr,o,o,x
Cassandra,426,org.apache.cassandra.gms.Gossiper.realMarkAlive,1015,trace,java.lang.String(name)Notified {}-java.util.List<IEndpointStateChangeSubscriber>(name)subscribers,o,x,o
Cassandra,400,org.apache.cassandra.gms.Gossiper.removeEndpoint,426,debug,java.lang.String(name)removing endpoint {}-java.net.InetAddress(name)endpoint,o,o,x
Cassandra,401,org.apache.cassandra.gms.Gossiper.replacementQuarantine,457,debug,java.lang.String(name),x,x,x
Cassandra,396,org.apache.cassandra.gms.Gossiper.run,200,error,java.lang.String(name)Gossip error-java.lang.Exception(name)e,o,x,o
Cassandra,486,org.apache.cassandra.hadoop.ConfigHelper.getClientFromAddressList,551,error,java.lang.String(name)-java.io.IOException(name)ioe,x,x,x
Cassandra,501,org.apache.cassandra.hadoop.cql3.CqlInputFormat.call,342,trace,java.lang.String(name)adding {}-org.apache.cassandra.hadoop.ColumnFamilySplit(name)split,o,x,x
Cassandra,496,org.apache.cassandra.hadoop.cql3.CqlRecordReader.initialize,161,trace,java.lang.String(name)cqlQuery {}-java.lang.String(name)cqlQuery,o,x,x
Cassandra,497,org.apache.cassandra.hadoop.cql3.CqlRecordReader.initialize,164,trace,java.lang.String(name)created {}-org.apache.cassandra.hadoop.cql3.RowIterator(name)rowIterator,o,x,x
Cassandra,499,org.apache.cassandra.hadoop.cql3.CqlRecordWriter.closeSession,233,warn,java.lang.String(name)Error closing connection-java.lang.Throwable(name)t,o,x,o
Cassandra,488,org.apache.cassandra.hadoop.cql3.LimitedLocalNodeFirstLocalBalancingPolicy.LimitedLocalNodeFirstLocalBalancingPolicy,74,trace,java.lang.String(name)Created instance with the following replicas: {}-java.util.List(name)Arrays.asList(replicas),o,o,o
Cassandra,490,org.apache.cassandra.hadoop.cql3.LimitedLocalNodeFirstLocalBalancingPolicy.newQueryPlan,130,trace,java.lang.String(name)Using the following hosts order for the new query plan: {} | {}-java.util.List<Host>(name)local-java.util.List<Host>(name)remote,o,o,o
Cassandra,491,org.apache.cassandra.hadoop.cql3.LimitedLocalNodeFirstLocalBalancingPolicy.onAdd,141,trace,java.lang.String(name)Added a new host {}-com.datastax.driver.core.Host(name)host,o,o,o
Cassandra,493,org.apache.cassandra.hadoop.cql3.LimitedLocalNodeFirstLocalBalancingPolicy.onDown,160,trace,java.lang.String(name)The host {} is now down-com.datastax.driver.core.Host(name)host,o,o,o
Cassandra,492,org.apache.cassandra.hadoop.cql3.LimitedLocalNodeFirstLocalBalancingPolicy.onUp,151,trace,java.lang.String(name)The host {} is now up-com.datastax.driver.core.Host(name)host,o,o,o
Cassandra,512,org.apache.cassandra.hints.HintsDispatchExecutor.convert,305,info,java.lang.String(name)Finished converting hints file {}-java.lang.String(name)descriptor.fileName(),o,o,o
Cassandra,528,org.apache.cassandra.hints.HintsReader.readHint,251,warn,java.lang.String(name)Failed to read a hint for {}: {} - digest mismatch for hint at position {} in file {}-java.net.InetAddress(name)StorageService.instance.getEndpointForHostId(descriptor.hostId)-java.util.UUID(name)descriptor.hostId-long(name)input.getPosition() - size - 4-java.lang.String(name)descriptor.fileName(),o,o,o
Cassandra,514,org.apache.cassandra.hints.HintVerbHandler.doVerb,58,trace,java.lang.String(name)Failed to decode and apply a hint for {}: {} - table with id {} is unknown-java.net.InetAddress(name)address-java.util.UUID(name)hostId-java.util.UUID(name)message.payload.unknownTableID,o,o,o
Cassandra,515,org.apache.cassandra.hints.HintVerbHandler.doVerb,73,warn,java.lang.String(name)Failed to validate a hint for {}: {} - skipped-java.net.InetAddress(name)address-java.util.UUID(name)hostId,o,o,o
Cassandra,549,org.apache.cassandra.index.sasi.analyzer.filter.StemmerFactory.load,53,error,java.lang.String(name)Failed to get stemmer constructor-java.lang.Exception(name)e,o,o,o
Cassandra,548,org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer.hasNext,96,error,"java.lang.String(name)""Failed to deserialize value with "" + validator-org.apache.cassandra.serializers.MarshalException(name)e",o,o,o
Cassandra,564,org.apache.cassandra.index.sasi.memory.TrieMemIndex.add,86,info,"java.lang.String(name)Can't add term of column {} to index for key: {}, term size {}, max allowed size {}, use analyzed = true (if not yet set) for that column.-java.lang.String(name)columnIndex.getColumnName()-java.lang.String(name)keyValidator.getString(key.getKey())-java.lang.String(name)FBUtilities.prettyPrintMemory(term.remaining())-java.lang.String(name)FBUtilities.prettyPrintMemory(OnDiskIndexBuilder.MAX_TERM_SIZE)",o,x,o
Cassandra,538,org.apache.cassandra.index.SecondaryIndexManager.unregisterIndex,817,trace,"java.lang.String(name)removed == null ? ""Index {} was not registered"" : ""Removed index {} from registry""-java.lang.String(name)name",o,o,o
Cassandra,592,org.apache.cassandra.io.sstable.format.SSTableReader.mergeCardinalities,351,warn,java.lang.String(name)Could not merge cardinalities-com.clearspring.analytics.stream.cardinality.CardinalityMergeException(name)e,o,o,o
Cassandra,580,org.apache.cassandra.io.sstable.IndexSummaryManager.IndexSummaryManager,80,info,java.lang.String(name)Initializing index summary manager with a memory pool size of {} MB and a resize interval of {} minutes-long(name)indexSummarySizeInMB-int(name)interval,o,o,o
Cassandra,569,org.apache.cassandra.io.sstable.IndexSummaryRedistribution.redistributeSummaries,100,trace,java.lang.String(name)Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB-int(name)redistribute.size()-long(name)memoryPoolBytes / 1024L / 1024L-double(name)total / 1024.0 / 1024.0,o,o,x
Cassandra,611,org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize,81,trace,java.lang.String(name)Load metadata for {}-org.apache.cassandra.io.sstable.Descriptor(name)descriptor,o,o,o
Cassandra,613,org.apache.cassandra.io.sstable.metadata.MetadataSerializer.mutateLevel,131,trace,java.lang.String(name)Mutating {} to level {}-java.lang.String(name)descriptor.filenameFor(Component.STATS)-int(name)newLevel,o,o,o
Cassandra,581,org.apache.cassandra.io.sstable.SSTable.delete,105,debug,java.lang.String(name)Deleting sstable: {}-org.apache.cassandra.io.sstable.Descriptor(name)desc,o,o,o
Cassandra,626,org.apache.cassandra.io.util.FileHandle.updateRegions,423,error,java.lang.String(name)Failed to close mapped regions-java.lang.Throwable(name)err,o,o,o
Cassandra,619,org.apache.cassandra.io.util.FileUtils.close,272,warn,java.lang.String(name)Failed closing stream {}-java.io.Closeable(name)c-java.io.IOException(name)ex,o,o,o
Cassandra,617,org.apache.cassandra.io.util.FileUtils.closeQuietly,237,warn,java.lang.String(name)Failed closing {}-java.io.Closeable(name)c-java.lang.Exception(name)e,o,x,o
Cassandra,648,org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy,274,warn,java.lang.String(name)Ignoring {}-java.lang.String(name)e.getMessage(),o,x,o
Cassandra,634,org.apache.cassandra.locator.SnitchProperties.SnitchProperties,54,warn,java.lang.String(name)Unable to read {}-java.lang.String(name)((configURL != null) ? configURL : RACKDC_PROPERTY_FILENAME),o,o,o
Cassandra,663,org.apache.cassandra.net.IncomingTcpConnection.receiveMessage,204,trace,java.lang.String(name)Received connection from newer protocol version {}. Ignoring message-int(name)version,o,o,o
Cassandra,690,org.apache.cassandra.net.MessagingService.logDroppedMessages,1236,info,java.lang.String(name)log,x,x,x
Cassandra,674,org.apache.cassandra.net.OutboundTcpConnection.connect,482,trace,java.lang.String(name)Target max version is {}; will reconnect with that version-int(name)maxTargetVersion,o,o,o
Cassandra,672,org.apache.cassandra.net.OutboundTcpConnection.connect,452,warn,java.lang.String(name)Failed to set send buffer size on internode socket.-java.net.SocketException(name)se,o,o,o
Cassandra,666,org.apache.cassandra.net.OutboundTcpConnection.run,280,error,java.lang.String(name)error processing a message intended for {}-java.net.InetAddress(name)poolReference.endPoint()-java.lang.Exception(name)e,o,o,x
Cassandra,667,org.apache.cassandra.net.OutboundTcpConnection.writeConnected,350,debug,java.lang.String(name)Error writing to {}-java.net.InetAddress(name)poolReference.endPoint()-java.lang.Throwable(name)e,o,o,o
Cassandra,668,org.apache.cassandra.net.OutboundTcpConnection.writeConnected,369,error,java.lang.String(name)error writing to {}-java.net.InetAddress(name)poolReference.endPoint()-java.lang.Throwable(name)e,o,o,x
Cassandra,652,org.apache.cassandra.net.ResponseVerbHandler.doVerb,38,trace,java.lang.String(name)msg-int(name)id-java.net.InetAddress(name)message.from,x,x,x
Cassandra,734,org.apache.cassandra.repair.RepairJob.sendValidationRequest,172,info,java.lang.String(name)[repair #{}] {}-java.util.UUID(name)desc.sessionId-java.lang.String(name)message,x,x,o
Cassandra,725,org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb,119,error,java.lang.String(name)Table {}.{} was dropped during snapshot phase of repair-java.lang.String(name)desc.keyspace-java.lang.String(name)desc.columnFamily,o,o,o
Cassandra,730,org.apache.cassandra.repair.RepairMessageVerbHandler.logErrorAndSendFailureResponse,177,error,java.lang.String(name)errorMessage,x,x,x
Cassandra,711,org.apache.cassandra.repair.RepairRunnable.runMayThrow,131,error,java.lang.String(name)Repair failed:-java.lang.IllegalArgumentException(name)e,o,x,o
Cassandra,715,org.apache.cassandra.repair.RepairRunnable.runMayThrow,277,error,java.lang.String(name)message-java.lang.Throwable(name)t,x,x,x
Cassandra,712,org.apache.cassandra.repair.RepairRunnable.runMayThrow,139,info,java.lang.String(name)message,x,x,x
Cassandra,714,org.apache.cassandra.repair.RepairRunnable.runMayThrow,261,info,java.lang.String(name)message,x,x,x
Cassandra,716,org.apache.cassandra.repair.RepairRunnable.runMayThrow,343,info,java.lang.String(name)message,x,x,x
Cassandra,703,org.apache.cassandra.repair.RepairSession.start,228,info,java.lang.String(name)[repair #{}] new session: will sync {} on range {} for {}.{}-java.util.UUID(name)getId()-java.lang.String(name)repairedNodes()-java.util.Collection<Range<Token>>(name)ranges-java.lang.String(name)keyspace-java.lang.String(name)Arrays.toString(cfnames),o,o,o
Cassandra,702,org.apache.cassandra.repair.RepairSession.syncComplete,201,debug,java.lang.String(name)[repair #{}] Repair completed between {} and {} on {}-java.util.UUID(name)getId()-java.net.InetAddress(name)nodes.endpoint1-java.net.InetAddress(name)nodes.endpoint2-java.lang.String(name)desc.columnFamily,o,o,o
Cassandra,708,org.apache.cassandra.repair.Validator.complete,242,debug,java.lang.String(name)Validated {} partitions for {}. Partition sizes are:-long(name)validated-java.util.UUID(name)desc.sessionId,o,o,o
Cassandra,709,org.apache.cassandra.repair.Validator.fail,268,error,"java.lang.String(name)Failed creating a merkle tree for {}, {} (see log for details)-org.apache.cassandra.repair.RepairJobDesc(name)desc-java.net.InetAddress(name)initiator",o,x,o
Cassandra,710,org.apache.cassandra.repair.Validator.run,281,info,java.lang.String(name)[repair #{}] Sending completed merkle tree to {} for {}.{}-java.util.UUID(name)desc.sessionId-java.net.InetAddress(name)initiator-java.lang.String(name)desc.keyspace-java.lang.String(name)desc.columnFamily,o,o,o
Cassandra,755,org.apache.cassandra.schema.CompactionParams.validate,155,warn,java.lang.String(name)Compaction strategy {} does not have a static validateOptions method. Validation ignored-java.lang.String(name)klass.getName(),o,o,o
Cassandra,754,org.apache.cassandra.schema.LegacySchemaMigrator.createIndexesFromColumnRows,800,error,java.lang.String(name)Failed to find index name for legacy migration of index on {}.{}-java.lang.String(name)keyspace-java.lang.String(name)table,o,o,o
Cassandra,762,org.apache.cassandra.security.CipherFactory.CipherFactory,98,info,java.lang.String(name)loading secret key for alias {}-java.lang.String(name)alias,o,o,x
Cassandra,759,org.apache.cassandra.security.SSLFactory.filterCipherSuites,221,warn,java.lang.String(name)Filtering out {} as it isn't supported by the socket-java.lang.String(name)Iterables.toString(missing),o,o,o
Cassandra,987,org.apache.cassandra.service.AbstractReadExecutor.makeRequests,116,trace,"java.lang.String(name)reading {} locally-java.lang.String(name)readCommand.isDigestQuery() ? ""digest"" : ""data""",o,o,x
Cassandra,770,org.apache.cassandra.service.ActiveRepairService.convict,781,debug,java.lang.String(name)Removing {} in parent repair sessions-java.util.Set<UUID>(name)toRemove,o,o,o
Cassandra,768,org.apache.cassandra.service.ActiveRepairService.markSSTablesRepairing,561,error,java.lang.String(name)Cannot start multiple repair sessions over the same sstables,o,o,o
Cassandra,769,org.apache.cassandra.service.ActiveRepairService.maybeSnapshot,655,error,java.lang.String(name)Cannot start multiple repair sessions over the same sstables,o,o,o
Cassandra,771,org.apache.cassandra.service.CacheService.initKeyCache,100,info,java.lang.String(name)Initializing key cache with capacity of {} MBs.-long(name)DatabaseDescriptor.getKeyCacheSizeInMB(),o,o,o
Cassandra,790,org.apache.cassandra.service.CassandraDaemon.logSystemInfo,489,info,java.lang.String(name){} {}: {}-java.lang.String(name)pool.getName()-java.lang.management.MemoryType(name)pool.getType()-java.lang.management.MemoryUsage(name)pool.getPeakUsage(),x,x,x
Cassandra,792,org.apache.cassandra.service.CassandraDaemon.logSystemInfo,493,info,java.lang.String(name)JVM Arguments: {}-java.util.List(name)ManagementFactory.getRuntimeMXBean().getInputArguments(),o,o,o
Cassandra,777,org.apache.cassandra.service.CassandraDaemon.setup,228,error,"java.lang.String(name)""Exception in thread "" + t-java.lang.Throwable(name)e",o,x,o
Cassandra,784,org.apache.cassandra.service.CassandraDaemon.setup,370,warn,"java.lang.String(name)Failed to load metrics-reporter-config, file does not exist: {}-java.lang.String(name)metricsReporterConfigFile",o,o,o
Cassandra,781,org.apache.cassandra.service.CassandraDaemon.setup,308,warn,java.lang.String(name)Error loading key or row cache-java.lang.Throwable(name)t,o,o,o
Cassandra,795,org.apache.cassandra.service.CassandraDaemon.start,550,info,java.lang.String(name)Not starting native transport as requested. Use JMX (StorageService->startNativeTransport()) or nodetool (enablebinary) to start it,o,o,o
Cassandra,796,org.apache.cassandra.service.CassandraDaemon.start,556,info,java.lang.String(name)Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it,o,o,o
Cassandra,794,org.apache.cassandra.service.CassandraDaemon.start,537,info,java.lang.String(name)Not starting client transports as bootstrap has not completed,o,o,o
Cassandra,798,org.apache.cassandra.service.CassandraDaemon.stop,588,error,java.lang.String(name)Error shutting down local JMX server: -java.io.IOException(name)e,o,o,o
Cassandra,809,org.apache.cassandra.service.DigestResolver.resolve,70,trace,java.lang.String(name)resolving {} responses-int(name)responses.size(),o,o,x
Cassandra,804,org.apache.cassandra.service.MigrationTask.runMayThrow,67,warn,java.lang.String(name)Can't send schema pull request: node {} is down.-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,989,org.apache.cassandra.service.paxos.PrepareCallback.response,64,trace,java.lang.String(name)Prepare response {} from {}-org.apache.cassandra.service.paxos.PrepareResponse(name)response-java.net.InetAddress(name)message.from,o,o,o
Cassandra,985,org.apache.cassandra.service.ReadCallback.get,147,trace,java.lang.String(name)Read: {} ms.-long(name)TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - queryStartNanoTime),o,x,o
Cassandra,949,org.apache.cassandra.service.StartupChecks.execute,176,info,java.lang.String(name)JMX is enabled to receive remote connections on port: {}-java.lang.String(name)jmxPort,o,o,o
Cassandra,946,org.apache.cassandra.service.StartupChecks.execute,140,info,java.lang.String(name)jemalloc seems to be preloaded from {}-java.lang.String(name)jemalloc,o,o,x
Cassandra,952,org.apache.cassandra.service.StartupChecks.execute,204,warn,"java.lang.String(name)Non-Oracle JVM detected. Some features, such as immediate unmap of compacted SSTables, may not work as intended",o,o,o
Cassandra,960,org.apache.cassandra.service.StorageProxy.mutateMV,799,error,java.lang.String(name)Error applying local view update to keyspace {}: {}-java.lang.String(name)mutation.getKeyspaceName()-org.apache.cassandra.db.Mutation(name)mutation,o,o,o
Cassandra,965,org.apache.cassandra.service.StorageProxy.performLocally,1378,error,java.lang.String(name)Failed to apply mutation locally : {}-java.lang.Exception(name)ex,o,o,o
Cassandra,968,org.apache.cassandra.service.StorageProxy.runMayThrow,1896,error,java.lang.String(name)t.getMessage(),x,x,x
Cassandra,977,org.apache.cassandra.service.StorageProxy.submitHint,2745,debug,java.lang.String(name)Discarding hint for endpoint not part of ring: {}-java.net.InetAddress(name)target,o,o,o
Cassandra,961,org.apache.cassandra.service.StorageProxy.syncWriteToBatchlog,983,trace,java.lang.String(name)Sending batchlog store request {} to {} for {} mutations-java.util.UUID(name)batch.id-java.net.InetAddress(name)target-int(name)batch.size(),o,o,o
Cassandra,974,org.apache.cassandra.service.StorageProxy.truncateBlocking,2489,debug,"java.lang.String(name)Starting a blocking truncate operation on keyspace {}, CF {}-java.lang.String(name)keyspace-java.lang.String(name)cfname",o,o,x
Cassandra,969,org.apache.cassandra.service.StorageProxy.updateConcurrencyFactor,2181,trace,"java.lang.String(name)Didn't get enough response rows; actual rows per range: {}; remaining rows: {}, new concurrent requests: {}-float(name)rowsPerRange-int(name)remainingRows-int(name)concurrencyFactor",o,o,o
Cassandra,864,org.apache.cassandra.service.StorageService.bootstrap,1542,info,java.lang.String(name)Bootstrap completed! for the tokens {}-java.util.Collection<Token>(name)tokens,o,o,x
Cassandra,863,org.apache.cassandra.service.StorageService.bootstrap,1525,info,java.lang.String(name)Resetting bootstrap progress to start fresh,o,o,o
Cassandra,865,org.apache.cassandra.service.StorageService.bootstrap,1548,warn,java.lang.String(name)Error during bootstrap.-java.lang.Throwable(name)e,o,x,o
Cassandra,819,org.apache.cassandra.service.StorageService.checkForEndpointCollision,565,debug,java.lang.String(name)Starting shadow gossip round to check for endpoint collision,o,o,o
Cassandra,896,org.apache.cassandra.service.StorageService.decommission,3968,debug,java.lang.String(name)DECOMMISSIONING,x,x,x
Cassandra,897,org.apache.cassandra.service.StorageService.decommission,3996,info,java.lang.String(name)failed to shutdown message service: {}-java.io.IOError(name)ioe,o,o,x
Cassandra,908,org.apache.cassandra.service.StorageService.forceRemoveCompletion,4361,warn,"java.lang.String(name)No nodes to force removal on, call 'removenode' first",o,o,o
Cassandra,907,org.apache.cassandra.service.StorageService.forceRemoveCompletion,4349,warn,"java.lang.String(name)Removal not confirmed for for {}-java.lang.String(name)StringUtils.join(this.replicatingNodes,"","")",o,o,x
Cassandra,887,org.apache.cassandra.service.StorageService.getChangedRangesForLeaving,2789,debug,java.lang.String(name)Range {} already in all replicas-org.apache.cassandra.dht.Range<Token>(name)range,o,o,o
Cassandra,904,org.apache.cassandra.service.StorageService.getPreferredHintsStreamTarget,4091,warn,java.lang.String(name)Unable to stream hints since no live endpoints seen,o,o,o
Cassandra,876,org.apache.cassandra.service.StorageService.handleStateBootreplacing,2302,debug,"java.lang.String(name)Node {} is replacing {}, tokens {}-java.net.InetAddress(name)newNode-java.net.InetAddress(name)oldNode-java.util.Collection<Token>(name)tokens",o,o,o
Cassandra,873,org.apache.cassandra.service.StorageService.handleStateBootstrap,2251,debug,"java.lang.String(name)Node {} state bootstrapping, token {}-java.net.InetAddress(name)endpoint-java.util.Collection<Token>(name)tokens",o,o,o
Cassandra,874,org.apache.cassandra.service.StorageService.handleStateBootstrap,2264,info,java.lang.String(name)Node {} state jump to bootstrap-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,877,org.apache.cassandra.service.StorageService.handleStateLeaving,2468,debug,"java.lang.String(name)Node {} state leaving, tokens {}-java.net.InetAddress(name)endpoint-java.util.Collection<Token>(name)tokens",o,o,o
Cassandra,878,org.apache.cassandra.service.StorageService.handleStateLeaving,2475,info,java.lang.String(name)Node {} state jump to leaving-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,881,org.apache.cassandra.service.StorageService.handleStateRemoving,2561,debug,java.lang.String(name)Tokens {} removed manually (endpoint was {})-java.util.Collection<Token>(name)removeTokens-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,838,org.apache.cassandra.service.StorageService.joinTokenRing,920,debug,java.lang.String(name)... got ring + schema info,x,x,x
Cassandra,835,org.apache.cassandra.service.StorageService.joinTokenRing,894,debug,java.lang.String(name)Bootstrap variables: {} {} {} {}-boolean(name)DatabaseDescriptor.isAutoBootstrap()-boolean(name)SystemKeyspace.bootstrapInProgress()-boolean(name)SystemKeyspace.bootstrapComplete()-boolean(name)DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddress()),x,o,o
Cassandra,836,org.apache.cassandra.service.StorageService.joinTokenRing,902,info,java.lang.String(name)This node will not auto bootstrap because it is configured to be a seed node.,o,o,o
Cassandra,840,org.apache.cassandra.service.StorageService.joinTokenRing,1029,warn,"java.lang.String(name)Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. {}-org.apache.cassandra.db.BootstrapState(name)SystemKeyspace.getBootstrapState()",o,o,o
Cassandra,842,org.apache.cassandra.service.StorageService.joinTokenRing,1037,warn,"java.lang.String(name)Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. {}-org.apache.cassandra.db.BootstrapState(name)SystemKeyspace.getBootstrapState()",o,o,o
Cassandra,899,org.apache.cassandra.service.StorageService.leaveRing,4029,info,java.lang.String(name)Announcing that I have left the ring for {}ms-int(name)delay,x,o,o
Cassandra,906,org.apache.cassandra.service.StorageService.move,4179,debug,java.lang.String(name)Successfully moved to new token {}-java.lang.Object(name)getLocalTokens().iterator().next(),o,o,o
Cassandra,872,org.apache.cassandra.service.StorageService.onChange,2028,debug,java.lang.String(name)Ignoring state change for dead or unknown endpoint: {}-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,817,org.apache.cassandra.service.StorageService.prepareForReplacement,527,info,java.lang.String(name)Gathering node replacement information for {}-java.net.InetAddress(name)replaceAddress,o,o,o
Cassandra,832,org.apache.cassandra.service.StorageService.prepareToJoin,814,warn,"java.lang.String(name)Writes will not be forwarded to this node during replacement because it has the same address as the node to be replaced ({}). If the previous node has been down for longer than max_hint_window_in_ms, repair must be run after the replacement process in order to make this node consistent.-java.net.InetAddress(name)DatabaseDescriptor.getReplaceAddress()",o,o,o
Cassandra,831,org.apache.cassandra.service.StorageService.prepareToJoin,785,warn,"java.lang.String(name)This node was decommissioned, but overriding by operator request.",o,o,o
Cassandra,910,org.apache.cassandra.service.StorageService.removeNode,4418,warn,java.lang.String(name)Endpoint {} is down and will not receive data for re-replication of {}-java.net.InetAddress(name)ep-java.net.InetAddress(name)endpoint,o,o,o
Cassandra,909,org.apache.cassandra.service.StorageService.removeNode,4395,warn,"java.lang.String(name)Node {} is already being removed, continuing removal anyway-java.net.InetAddress(name)endpoint",o,o,o
Cassandra,885,org.apache.cassandra.service.StorageService.restoreReplicaCount,2746,warn,java.lang.String(name)Streaming to restore replica count failed-java.lang.Throwable(name)t,o,o,o
Cassandra,869,org.apache.cassandra.service.StorageService.resumeBootstrap,1628,info,java.lang.String(name)Resume complete,o,x,o
Cassandra,853,org.apache.cassandra.service.StorageService.setRangeRpcTimeout,1353,info,java.lang.String(name)set range rpc timeout to {} ms-long(name)value,o,o,x
Cassandra,852,org.apache.cassandra.service.StorageService.setReadRpcTimeout,1342,info,java.lang.String(name)set read rpc timeout to {} ms-long(name)value,o,o,x
Cassandra,851,org.apache.cassandra.service.StorageService.setRpcTimeout,1331,info,java.lang.String(name)set rpc timeout to {} ms-long(name)value,o,o,x
Cassandra,858,org.apache.cassandra.service.StorageService.setStreamingSocketTimeout,1408,info,java.lang.String(name)set streaming socket timeout to {} ms-int(name)value,o,o,x
Cassandra,857,org.apache.cassandra.service.StorageService.setTruncateRpcTimeout,1397,info,java.lang.String(name)set truncate rpc timeout to {} ms-long(name)value,o,o,x
Cassandra,854,org.apache.cassandra.service.StorageService.setWriteRpcTimeout,1364,info,java.lang.String(name)set write rpc timeout to {} ms-long(name)value,o,o,x
Cassandra,915,org.apache.cassandra.service.StorageService.streamRanges,4983,debug,"java.lang.String(name)Skipping transferred range {} of keyspace {}, endpoint {}-org.apache.cassandra.dht.Range<Token>(name)range-java.lang.String(name)keyspace-java.net.InetAddress(name)endpoint",o,o,o
Cassandra,901,org.apache.cassandra.service.StorageService.unbootstrap,4054,debug,java.lang.String(name)waiting for batch log processing.,o,o,x
Cassandra,912,org.apache.cassandra.service.StorageService.updateSnitch,4920,info,"java.lang.String(name)Created new dynamic snitch {} with update-interval={}, reset-interval={}, badness-threshold={}-java.lang.String(name)((DynamicEndpointSnitch)newSnitch).subsnitch.getClass().getName()-int(name)DatabaseDescriptor.getDynamicUpdateInterval()-int(name)DatabaseDescriptor.getDynamicResetInterval()-double(name)DatabaseDescriptor.getDynamicBadnessThreshold()",x,o,o
Cassandra,913,org.apache.cassandra.service.StorageService.updateSnitch,4926,info,java.lang.String(name)Created new non-dynamic snitch {}-java.lang.String(name)newSnitch.getClass().getName(),o,o,o
Cassandra,992,org.apache.cassandra.streaming.StreamCoordinator.connectNext,142,debug,java.lang.String(name)Connecting next session {} with {}.-java.util.UUID(name)next.planId()-java.lang.String(name)next.peer.getHostAddress(),o,o,o
Cassandra,995,org.apache.cassandra.streaming.StreamReader.read,105,debug,"java.lang.String(name)[Stream #{}] Start receiving file #{} from {}, repairedAt = {}, size = {}, ks = '{}', table = '{}'.-java.util.UUID(name)session.planId()-int(name)fileSeqNum-java.net.InetAddress(name)session.peer-long(name)repairedAt-long(name)totalSize-java.lang.String(name)cfs.keyspace.getName()-java.lang.String(name)cfs.getColumnFamilyName()",o,o,o
Cassandra,1030,org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete,187,info,java.lang.String(name)[Stream #{}] Session with {} is complete-java.util.UUID(name)session.planId()-java.net.InetAddress(name)session.peer,o,o,o
Cassandra,1027,org.apache.cassandra.streaming.StreamResultFuture.initReceivingSide,116,info,java.lang.String(name)[Stream #{} ID#{}] Creating new streaming plan for {}-java.util.UUID(name)planId-int(name)sessionIndex-java.lang.String(name)description,o,o,o
Cassandra,1032,org.apache.cassandra.streaming.StreamResultFuture.maybeComplete,219,info,java.lang.String(name)[Stream #{}] All sessions completed-java.util.UUID(name)planId,o,o,o
Cassandra,1002,org.apache.cassandra.streaming.StreamSession.init,252,debug,java.lang.String(name)Peer {} does not support keep-alive.-java.net.InetAddress(name)peer,o,o,o
Cassandra,1007,org.apache.cassandra.streaming.StreamSession.logError,586,error,java.lang.String(name)[Stream #{}] Streaming socket timed out. This means the session peer stopped responding or is still processing received data. If there is no sign of failure in the other end or a very dense table is being transferred you may want to increase streaming_socket_timeout_in_ms property. Current value is {}ms.-java.util.UUID(name)planId()-int(name)DatabaseDescriptor.getStreamingSocketTimeout()-java.lang.Throwable(name)e,x,o,o
Cassandra,1006,org.apache.cassandra.streaming.StreamSession.logError,579,error,"java.lang.String(name)[Stream #{}] Did not receive response from peer {}{} for {} secs. Is peer down? If not, maybe try increasing streaming_keep_alive_period_in_secs.-java.util.UUID(name)planId()-java.lang.String(name)peer.getHostAddress()-java.lang.String(name)peer.equals(connecting) ? """" : "" through "" + connecting.getHostAddress()-int(name)2 * DatabaseDescriptor.getStreamingKeepAlivePeriod()-java.lang.Throwable(name)e",x,o,o
Cassandra,1008,org.apache.cassandra.streaming.StreamSession.logError,593,error,"java.lang.String(name)[Stream #{}] Streaming error occurred on session with peer {}{}-java.util.UUID(name)planId()-java.lang.String(name)peer.getHostAddress()-java.lang.String(name)peer.equals(connecting) ? """" : "" through "" + connecting.getHostAddress()-java.lang.Throwable(name)e",x,o,o
Cassandra,1011,org.apache.cassandra.streaming.StreamSession.onRemove,744,error,java.lang.String(name)[Stream #{}] Session failed because remote peer {} has left.-java.util.UUID(name)planId()-java.lang.String(name)peer.getHostAddress(),o,o,o
Cassandra,1004,org.apache.cassandra.streaming.StreamSession.start,266,info,"java.lang.String(name)[Stream #{}] Starting streaming to {}{}-java.util.UUID(name)planId()-java.net.InetAddress(name)peer-java.lang.String(name)peer.equals(connecting) ? """" : "" through "" + connecting",x,o,o
Cassandra,1210,org.apache.cassandra.thrift.Cassandra.getResultHandler,4813,error,java.lang.String(name)Exception writing to internal frame buffer-java.lang.Exception(name)e,o,o,o
Cassandra,1071,org.apache.cassandra.thrift.CassandraServer.add,2171,trace,java.lang.String(name)add,x,x,x
Cassandra,1053,org.apache.cassandra.thrift.CassandraServer.get_count,718,trace,java.lang.String(name)average row column size is {}; using pageSize of {}-int(name)averageColumnSize-int(name)pageSize,o,o,x
Cassandra,1061,org.apache.cassandra.thrift.CassandraServer.get_indexed_slices,1720,trace,java.lang.String(name)scan,x,x,x
Cassandra,1059,org.apache.cassandra.thrift.CassandraServer.remove,1427,trace,java.lang.String(name)remove,x,x,x
Cassandra,1087,org.apache.cassandra.thrift.CustomTThreadPoolServer.serve,99,error,java.lang.String(name)Error occurred during listening.-org.apache.thrift.transport.TTransportException(name)ttx,o,o,o
Cassandra,1048,org.apache.cassandra.thrift.TCustomNonblockingServerSocket.acceptImpl,70,warn,java.lang.String(name)Failed to set send buffer size on Thrift socket.-java.net.SocketException(name)se,o,o,o
Cassandra,1049,org.apache.cassandra.thrift.TCustomNonblockingServerSocket.acceptImpl,82,warn,java.lang.String(name)Failed to set receive buffer size on Thrift socket.-java.net.SocketException(name)se,o,o,o
Cassandra,1043,org.apache.cassandra.thrift.TCustomServerSocket.acceptImpl,129,warn,java.lang.String(name)Failed to set send buffer size on Thrift socket.-java.net.SocketException(name)se,o,o,o
Cassandra,1044,org.apache.cassandra.thrift.TCustomServerSocket.acceptImpl,141,warn,java.lang.String(name)Failed to set receive buffer size on Thrift socket.-java.net.SocketException(name)se,o,o,o
Cassandra,1046,org.apache.cassandra.thrift.TCustomServerSocket.close,176,warn,java.lang.String(name)Could not close server socket.-java.io.IOException(name)iox,o,o,o
Cassandra,1045,org.apache.cassandra.thrift.TCustomServerSocket.listen,160,error,java.lang.String(name)Could not set socket timeout.-java.net.SocketException(name)sx,o,o,o
Cassandra,1079,org.apache.cassandra.thrift.TCustomSocket.initSocket,140,error,java.lang.String(name)Could not configure socket.-java.net.SocketException(name)sx,o,o,o
Cassandra,1078,org.apache.cassandra.thrift.TCustomSocket.TCustomSocket,80,warn,java.lang.String(name)Could not configure socket.-java.net.SocketException(name)sx,o,o,o
Cassandra,1041,org.apache.cassandra.thrift.ThriftServer.stopServer,139,info,java.lang.String(name)Stop listening to thrift clients,o,o,o
Cassandra,1086,org.apache.cassandra.thrift.THsHaDisruptorServer.THsHaDisruptorServer,52,info,java.lang.String(name)Starting up {}-org.apache.cassandra.thrift.THsHaDisruptorServer(name)this,o,o,o
Cassandra,1083,org.apache.cassandra.thrift.TServerCustomFactory.buildTServer,56,info,java.lang.String(name)Using custom half-sync/half-async thrift server on {} : {}-java.lang.String(name)args.addr.getHostName()-int(name)args.addr.getPort(),o,o,o
Cassandra,1084,org.apache.cassandra.thrift.TServerCustomFactory.buildTServer,70,info,java.lang.String(name)Using custom thrift server {} on {} : {}-java.lang.String(name)server.getClass().getName()-java.lang.String(name)args.addr.getHostName()-int(name)args.addr.getPort(),o,o,o
Cassandra,1098,org.apache.cassandra.tools.JsonTransformer.serializeCell,484,error,java.lang.String(name)Failure parsing cell.-java.io.IOException(name)e,o,o,o
Cassandra,1095,org.apache.cassandra.tools.JsonTransformer.serializePartition,227,error,java.lang.String(name)Fatal error parsing partition: {}-java.lang.String(name)key-java.io.IOException(name)e,o,o,o
Cassandra,1103,org.apache.cassandra.tracing.TraceStateImpl.executeMutation,115,warn,"java.lang.String(name)Failed to insert pending future, tracing synchronization may not work",o,o,o
Cassandra,1100,org.apache.cassandra.tracing.TraceStateImpl.waitForPendingEvents,83,trace,java.lang.String(name)Waiting for up to {} seconds for {} trace events to complete-int(name)+WAIT_FOR_PENDING_EVENTS_TIMEOUT_SECS-int(name)pendingFutures.size(),o,o,o
Cassandra,1105,org.apache.cassandra.tracing.Tracing.stopSession,188,trace,java.lang.String(name)request complete,o,x,x
Cassandra,1110,org.apache.cassandra.transport.Message.apply,693,error,java.lang.String(name)message-java.lang.Throwable(name)exception,x,x,x
Cassandra,1109,org.apache.cassandra.transport.Message.apply,687,info,java.lang.String(name)message-java.lang.Throwable(name)exception,x,x,x
Cassandra,1108,org.apache.cassandra.transport.Message.apply,682,trace,java.lang.String(name)message-java.lang.Throwable(name)exception,x,x,x
Cassandra,1122,org.apache.cassandra.transport.messages.ErrorMessage.fromException,384,error,java.lang.String(name)Unexpected exception during request-java.lang.Throwable(name)e,o,o,o
Cassandra,1121,org.apache.cassandra.transport.messages.QueryMessage.execute,129,error,java.lang.String(name)Unexpected error during query-java.lang.Exception(name)e,o,o,o
Cassandra,1113,org.apache.cassandra.transport.Server.start,155,info,java.lang.String(name)Using Netty Version: {}-java.util.Set(name)Version.identify().entrySet(),o,o,o
Cassandra,1120,org.apache.cassandra.transport.SimpleClient.exceptionCaught,336,error,java.lang.String(name)Exception in response-java.lang.Throwable(name)cause,o,x,o
Cassandra,1123,org.apache.cassandra.triggers.CustomClassLoader.addClassPath,89,info,java.lang.String(name)Loading new jar {}-java.lang.String(name)inputJar.getAbsolutePath(),o,o,o
Cassandra,1302,org.apache.cassandra.Util.flakyTest,628,debug,"java.lang.String(name)Test failed again, total num failures: {}-int(name)rerunsFailed-java.lang.AssertionError(name)t",o,o,x
Cassandra,1303,org.apache.cassandra.Util.flakyTest,633,error,java.lang.String(name)Test failed in {} of the {} reruns.-int(name)rerunsFailed-int(name)rerunsOnFailure,o,o,o
Cassandra,1172,org.apache.cassandra.utils.CoalescingStrategies.debugGap,187,info,java.lang.String(name){} gap {}μs-org.apache.cassandra.utils.CoalescingStrategy(name)this-long(name)TimeUnit.NANOSECONDS.toMicros(averageGap),o,x,x
Cassandra,1197,org.apache.cassandra.utils.concurrent.Ref.release,234,error,java.lang.String(name)Error when closing {}-org.apache.cassandra.utils.concurrent.GlobalState(name)globalState-java.lang.Throwable(name)fail,o,o,o
Cassandra,1157,org.apache.cassandra.utils.FBUtilities.cassandraTriggerDir,323,warn,"java.lang.String(name)Trigger directory doesn't exist, please create it and try again.",o,o,o
Cassandra,1158,org.apache.cassandra.utils.FBUtilities.getReleaseVersionString,344,warn,java.lang.String(name)Unable to load version.properties-java.lang.Exception(name)e,o,o,o
Cassandra,1189,org.apache.cassandra.utils.HeapUtils.logProcessOutput,109,info,java.lang.String(name)builder.toString(),x,x,x
Cassandra,1174,org.apache.cassandra.utils.JavaUtils.supportExitOnOutOfMemory,51,error,"java.lang.String(name)""Some JRE information could not be retrieved for the JRE version: "" + jreVersion-java.lang.Exception(name)e",o,x,o
Cassandra,1191,org.apache.cassandra.utils.JVMStabilityInspector.inspectCommitLogThrowable,102,error,java.lang.String(name)Exiting due to error while processing commit log during initialization.-java.lang.Throwable(name)t,o,o,o
Cassandra,1192,org.apache.cassandra.utils.JVMStabilityInspector.userFunctionTimeout,128,error,java.lang.String(name)t.getMessage(),x,x,x
Cassandra,1201,org.apache.cassandra.utils.logging.LoggingSupportFactory.getLoggingSupport,36,warn,java.lang.String(name)You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: {}. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.-java.lang.String(name)loggerFactoryClass,o,x,o
Cassandra,1209,org.apache.cassandra.utils.memory.BufferPool.allocateMoreChunks,283,error,"java.lang.String(name)Buffer pool failed to allocate chunk of {}, current size {} ({}). Attempting to continue; buffers will be allocated in on-heap memory which can degrade performance. Make sure direct memory size (-XX:MaxDirectMemorySize) is large enough to accommodate off-heap memtables and caches.-int(name)1048576-long(name)sizeInBytes()-java.lang.String(name)oom.toString()",o,x,x
Cassandra,1208,org.apache.cassandra.utils.memory.BufferPool.allocateMoreChunks,267,info,"java.lang.String(name)Maximum memory usage reached ({}), cannot allocate chunk of {}-long(name)MEMORY_USAGE_THRESHOLD-int(name)1048576",o,o,o
Cassandra,1207,org.apache.cassandra.utils.memory.BufferPool.maybeTakeFromPool,136,trace,"java.lang.String(name)Requested buffer size {} is bigger than {}, allocating directly-java.lang.String(name)FBUtilities.prettyPrintMemory(size)-java.lang.String(name)FBUtilities.prettyPrintMemory(CHUNK_SIZE)",o,o,o
Cassandra,1206,org.apache.cassandra.utils.memory.BufferPool.takeFromPool,120,trace,java.lang.String(name)Requested buffer size {} has been allocated directly due to lack of capacity-java.lang.String(name)FBUtilities.prettyPrintMemory(size),o,o,o
Cassandra,1205,org.apache.cassandra.utils.memory.SlabAllocator.getRegion,145,trace,java.lang.String(name){} regions now allocated in {}-java.util.concurrent.atomic.AtomicInteger(name)regionCount-org.apache.cassandra.utils.memory.SlabAllocator(name)this,o,o,o
Cassandra,1146,org.apache.cassandra.utils.MerkleTree.difference,260,debug,java.lang.String(name)Range {} fully inconsistent-org.apache.cassandra.utils.TreeDifference(name)active,o,o,o
Cassandra,1129,org.apache.cassandra.utils.Mx4jTool.maybeLoad,61,info,java.lang.String(name)mx4j successfuly loaded,o,o,x
Cassandra,1168,org.apache.cassandra.utils.OutputHandler.debug,47,trace,java.lang.String(name)msg,x,x,x
Cassandra,1167,org.apache.cassandra.utils.OutputHandler.output,42,info,java.lang.String(name)msg,x,x,x
Cassandra,1169,org.apache.cassandra.utils.OutputHandler.warn,52,warn,java.lang.String(name)msg,x,x,x
Cassandra,1170,org.apache.cassandra.utils.OutputHandler.warn,57,warn,java.lang.String(name)msg-java.lang.Throwable(name)th,x,x,x
Cassandra,1181,org.apache.cassandra.utils.SigarLibrary.hasAcceptableAddressSpace,132,warn,java.lang.String(name)Could not determine if VirtualMemoryMax was acceptable. Error message: {}-org.hyperic.sigar.SigarException(name)sigarException,o,o,o
Cassandra,1184,org.apache.cassandra.utils.SigarLibrary.warnIfRunningInDegradedMode,180,info,java.lang.String(name)Checked OS settings and found them configured for optimal performance.,o,o,o
Elasticsearch,178,org.elasticsearch.action.admin.cluster.health.TransportClusterHealthAction.waitForEventsAndExecuteHealth,162,trace,java.lang.String(name)stopped being master while waiting for events with priority [{}]. retrying.-org.elasticsearch.common.Priority(name)request.waitForEvents(),o,o,x
Elasticsearch,185,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendRemoveBanRequest,194,debug,java.lang.String(name)Sending remove ban for tasks with the parent [{}] to the node [{}]-org.elasticsearch.tasks.TaskId(name)request.parentTaskId-java.lang.String(name)node.key,o,o,o
Elasticsearch,184,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest,185,warn,java.lang.String(name)Cannot send ban for tasks with the parent [{}] to the node [{}]-org.elasticsearch.tasks.TaskId(name)request.parentTaskId-java.lang.String(name)node.key,o,o,o
Elasticsearch,182,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.taskOperation,156,trace,java.lang.String(name)task {} is already cancelled-long(name)cancellableTask.getId(),o,o,x
Elasticsearch,195,org.elasticsearch.action.admin.cluster.settings.TransportClusterUpdateSettingsAction.masterOperation,155,debug,java.lang.String(name)failed to preform reroute after cluster settings were updated - current node is no longer a master,o,o,x
Elasticsearch,214,org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase,161,debug,"java.lang.String(name)Partial shards failure (unavailable: {}, successful: {}, skipped: {}, num-shards: {}, phase: {})-int(name)discrepancy-int(name)successfulOps.get()-int(name)skippedOps.get()-int(name)getNumShards()-java.lang.String(name)currentPhase.getName()",o,o,o
Elasticsearch,215,org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase,171,trace,"java.lang.String(name)[{}] Moving to next phase: [{}], based on results from: {} (cluster state version: {})-java.lang.String(name)currentPhase.getName()-java.lang.String(name)nextPhase.getName()-java.lang.String(name)resultsFrom-long(name)clusterStateVersion",o,o,o
Elasticsearch,239,org.elasticsearch.action.support.master.TransportMasterNodeAction.doStart,181,debug,"java.lang.String(name)connection exception while trying to forward request with action name [{}] to master node [{}], scheduling a retry. Error: [{}]-java.lang.String(name)actionName-org.elasticsearch.cluster.node.DiscoveryNode(name)nodes.getMasterNode()-java.lang.String(name)exp.getDetailedMessage()",o,o,x
Elasticsearch,246,org.elasticsearch.action.support.replication.TransportReplicationAction.performRemoteAction,702,trace,"java.lang.String(name)failed to find primary [{}] for request [{}] despite sender thinking it would be here. Local cluster state version [{}]] is older than on sending node (version [{}]), scheduling a retry...-org.elasticsearch.index.shard.ShardId(name)request.shardId()-Request(name)request-long(name)state.version()-long(name)request.routedBasedOnClusterVersion()",o,o,x
Elasticsearch,288,org.elasticsearch.bootstrap.Bootstrap.initializeNatives,128,info,java.lang.String(name)running graceful exit on windows,o,o,x
Elasticsearch,289,org.elasticsearch.bootstrap.Bootstrap.setup,196,warn,java.lang.String(name)Thread got interrupted while waiting for the node to shutdown.,o,o,o
Elasticsearch,268,org.elasticsearch.bootstrap.JNAKernel32Library.JNAKernel32Library,62,warn,java.lang.String(name)JNA not found. native methods and handlers will be disabled.,o,o,x
Elasticsearch,273,org.elasticsearch.bootstrap.JNANatives.tryMlockall,95,warn,java.lang.String(name)This can result in part of the JVM being swapped out.,o,x,o
Elasticsearch,280,org.elasticsearch.bootstrap.JNANatives.trySetMaxFileSize,151,warn,"java.lang.String(name)""unable to retrieve max file size ["" + JNACLibrary.strerror(Native.getLastError()) + ""]""",o,o,x
Elasticsearch,278,org.elasticsearch.bootstrap.JNANatives.trySetMaxNumberOfThreads,129,warn,"java.lang.String(name)""unable to retrieve max number of threads ["" + JNACLibrary.strerror(Native.getLastError()) + ""]""",o,o,x
Elasticsearch,281,org.elasticsearch.bootstrap.JNANatives.tryVirtualLock,188,warn,java.lang.String(name)Unable to lock JVM memory. Failed to set working set size. Error code {}-int(name)Native.getLastError(),o,o,o
Elasticsearch,22,org.elasticsearch.client.sniff.Sniffer.run,141,error,java.lang.String(name)error while sniffing nodes-java.lang.Exception(name)e,o,o,x
Elasticsearch,320,org.elasticsearch.client.transport.TransportClientNodesService.doSample,504,trace,java.lang.String(name)connecting to cluster node [{}]-org.elasticsearch.cluster.node.DiscoveryNode(name)nodeToPing,o,o,x
Elasticsearch,318,org.elasticsearch.client.transport.TransportClientNodesService.run,399,warn,java.lang.String(name)failed to sample-java.lang.Exception(name)e,o,x,x
Elasticsearch,118,org.elasticsearch.cloud.gce.GceInstancesServiceImpl.client,235,warn,java.lang.String(name)unable to start GCE discovery service-java.lang.Exception(name)e,o,o,x
Elasticsearch,110,org.elasticsearch.cloud.gce.GceMetadataService.doStop,105,warn,java.lang.String(name)unable to shutdown GCE Http Transport-java.io.IOException(name)e,o,o,x
Elasticsearch,432,org.elasticsearch.cluster.coordination.CoordinationState.handleJoin,240,debug,"java.lang.String(name)handleJoin: ignored join as joiner has a better last accepted version (expected: <=[{}], actual: [{}]) in term {}-long(name)getLastAcceptedVersionOrMetaDataVersion()-long(name)join.getLastAcceptedVersion()-long(name)lastAcceptedTerm",o,o,o
Elasticsearch,483,org.elasticsearch.cluster.coordination.Coordinator.setInitialConfiguration,871,debug,"java.lang.String(name)skip setting initial configuration as not enough nodes discovered to form a quorum in the initial configuration [knownNodes={}, {}]-java.util.List<DiscoveryNode>(name)knownNodes-org.elasticsearch.cluster.coordination.VotingConfiguration(name)votingConfiguration",o,o,x
Elasticsearch,529,org.elasticsearch.cluster.coordination.ElectionSchedulerFactory.scheduleNextElection,183,debug,java.lang.String(name)scheduling {}-java.lang.Runnable(name)runnable,o,x,x
Elasticsearch,423,org.elasticsearch.cluster.coordination.FollowersChecker.failNode,375,trace,"java.lang.String(name){} no longer running, not marking faulty-org.elasticsearch.cluster.coordination.FollowerChecker(name)FollowerChecker.this",o,o,o
Elasticsearch,382,org.elasticsearch.cluster.coordination.JoinHelper.sendJoinRequest,290,debug,java.lang.String(name)successfully joined {} with {}-org.elasticsearch.cluster.node.DiscoveryNode(name)destination-org.elasticsearch.cluster.coordination.JoinRequest(name)joinRequest,o,o,x
Elasticsearch,387,org.elasticsearch.cluster.coordination.JoinTaskExecutor.execute,136,debug,java.lang.String(name)received a join request for an existing node [{}]-org.elasticsearch.cluster.node.DiscoveryNode(name)joinTask.node(),o,o,x
Elasticsearch,530,org.elasticsearch.cluster.coordination.LagDetector.setAppliedVersion,86,trace,java.lang.String(name)node {} applied version {} but this node's version is not being tracked-org.elasticsearch.cluster.node.DiscoveryNode(name)discoveryNode-long(name)appliedVersion,o,o,x
Elasticsearch,397,org.elasticsearch.cluster.coordination.LeaderChecker.close,211,debug,java.lang.String(name)closed,o,x,x
Elasticsearch,407,org.elasticsearch.cluster.coordination.LeaderChecker.scheduleNextWakeUp,320,trace,java.lang.String(name)scheduling next check of {} for [{}] = {}-org.elasticsearch.cluster.node.DiscoveryNode(name)leader-java.lang.String(name)LEADER_CHECK_INTERVAL_SETTING.getKey()-org.elasticsearch.common.unit.TimeValue(name)leaderCheckInterval,o,o,x
Elasticsearch,496,org.elasticsearch.cluster.coordination.PreVoteCollector.handlePreVoteResponse,179,debug,"java.lang.String(name){} is closed, ignoring {} from {}-org.elasticsearch.cluster.coordination.PreVotingRound(name)this-org.elasticsearch.cluster.coordination.PreVoteResponse(name)response-org.elasticsearch.cluster.node.DiscoveryNode(name)sender",o,o,o
Elasticsearch,500,org.elasticsearch.cluster.coordination.PreVoteCollector.handlePreVoteResponse,215,debug,"java.lang.String(name){} added {} from {}, starting election-org.elasticsearch.cluster.coordination.PreVotingRound(name)this-org.elasticsearch.cluster.coordination.PreVoteResponse(name)response-org.elasticsearch.cluster.node.DiscoveryNode(name)sender",o,o,o
Elasticsearch,328,org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh,272,trace,java.lang.String(name)Skipping ClusterInfoUpdatedJob since it is disabled,o,o,o
Elasticsearch,331,org.elasticsearch.cluster.InternalClusterInfoService.refresh,307,trace,java.lang.String(name)Failed to execute NodeStatsAction for ClusterInfoUpdateJob-java.lang.Exception(name)e,o,o,o
Elasticsearch,334,org.elasticsearch.cluster.InternalClusterInfoService.refresh,337,trace,java.lang.String(name)Failed to execute IndicesStatsAction for ClusterInfoUpdateJob-java.lang.Exception(name)e,o,o,o
Elasticsearch,336,org.elasticsearch.cluster.InternalClusterInfoService.refresh,351,warn,java.lang.String(name)Failed to update node information for ClusterInfoUpdateJob within {} timeout-org.elasticsearch.common.unit.TimeValue(name)fetchTimeout,o,o,o
Elasticsearch,337,org.elasticsearch.cluster.InternalClusterInfoService.refresh,359,warn,java.lang.String(name)Failed to update shard information for ClusterInfoUpdateJob within {} timeout-org.elasticsearch.common.unit.TimeValue(name)fetchTimeout,o,o,o
Elasticsearch,332,org.elasticsearch.cluster.InternalClusterInfoService.refresh,310,warn,java.lang.String(name)Failed to execute NodeStatsAction for ClusterInfoUpdateJob-java.lang.Exception(name)e,o,o,o
Elasticsearch,335,org.elasticsearch.cluster.InternalClusterInfoService.refresh,340,warn,java.lang.String(name)Failed to execute IndicesStatsAction for ClusterInfoUpdateJob-java.lang.Exception(name)e,o,o,o
Elasticsearch,550,org.elasticsearch.cluster.metadata.MetaDataIndexStateService.closeRoutingTable,454,debug,java.lang.String(name)verification of shards before closing {} succeeded but block has been removed in the meantime-org.elasticsearch.index.Index(name)index,o,o,x
Elasticsearch,551,org.elasticsearch.cluster.metadata.MetaDataIndexStateService.closeRoutingTable,463,debug,java.lang.String(name)verification of shards before closing {} succeeded but index is being restored in the meantime-org.elasticsearch.index.Index(name)index,o,o,x
Elasticsearch,552,org.elasticsearch.cluster.metadata.MetaDataIndexStateService.closeRoutingTable,472,debug,java.lang.String(name)verification of shards before closing {} succeeded but index is being snapshot in the meantime-org.elasticsearch.index.Index(name)index,o,o,x
Elasticsearch,556,org.elasticsearch.cluster.metadata.TemplateUpgradeService.clusterChanged,127,info,"java.lang.String(name)Starting template upgrade to version {}, {} templates will be updated and {} will be removed-org.elasticsearch.Version(name)Version.CURRENT-int(name)changes.get().v1().size()-int(name)changes.get().v2().size()",o,o,o
Elasticsearch,561,org.elasticsearch.cluster.metadata.TemplateUpgradeService.tryFinishUpgrade,203,info,java.lang.String(name)Templates were partially upgraded to version {}-org.elasticsearch.Version(name)Version.CURRENT,o,o,o
Elasticsearch,562,org.elasticsearch.cluster.metadata.TemplateUpgradeService.tryFinishUpgrade,205,info,java.lang.String(name)Templates were upgraded successfully to version {}-org.elasticsearch.Version(name)Version.CURRENT,o,o,o
Elasticsearch,347,org.elasticsearch.cluster.NodeConnectionsService.doRun,312,debug,java.lang.String(name)connected to {}-org.elasticsearch.cluster.node.DiscoveryNode(name)discoveryNode,o,o,x
Elasticsearch,611,org.elasticsearch.cluster.routing.allocation.AllocationService.logClusterHealthStateChange,381,info,java.lang.String(name)Cluster health status changed from [{}] to [{}] (reason: [{}]).-org.elasticsearch.cluster.health.ClusterHealthStatus(name)previousHealth-org.elasticsearch.cluster.health.ClusterHealthStatus(name)currentHealth-java.lang.String(name)reason,o,o,o
Elasticsearch,625,org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned,839,trace,java.lang.String(name)No Node found to assign shard [{}]-org.elasticsearch.cluster.routing.ShardRouting(name)shard,o,o,o
Elasticsearch,618,org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.moveShards,638,trace,java.lang.String(name)Moved shard [{}] to node [{}]-org.elasticsearch.cluster.routing.ShardRouting(name)shardRouting-org.elasticsearch.cluster.routing.RoutingNode(name)targetNode.getRoutingNode(),o,o,o
Elasticsearch,626,org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.tryRelocateShard,978,trace,java.lang.String(name)Try relocating shard for index index [{}] from node [{}] to node [{}]-java.lang.String(name)idx-java.lang.String(name)maxNode.getNodeId()-java.lang.String(name)minNode.getNodeId(),o,o,o
Elasticsearch,653,org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canForceAllocatePrimary,221,trace,java.lang.String(name)Shard [{}] can not be forcefully allocated to node [{}] due to [{}].-org.elasticsearch.index.shard.ShardId(name)shardRouting.shardId()-java.lang.String(name)node.nodeId()-java.lang.String(name)decider.getClass().getSimpleName(),o,o,o
Elasticsearch,632,org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate,174,debug,"java.lang.String(name)less than the required {} free bytes threshold ({} free) on node {}, but allowing allocation because primary has never been allocated-org.elasticsearch.common.unit.ByteSizeValue(name)diskThresholdSettings.getFreeBytesThresholdLow()-org.elasticsearch.common.unit.ByteSizeValue(name)freeBytesValue-java.lang.String(name)node.nodeId()",o,o,x
Elasticsearch,633,org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate,185,debug,"java.lang.String(name)less than the required {} free bytes threshold ({} free) on node {}, preventing allocation even though primary has never been allocated-org.elasticsearch.common.unit.ByteSizeValue(name)diskThresholdSettings.getFreeBytesThresholdHigh()-org.elasticsearch.common.unit.ByteSizeValue(name)freeBytesValue-java.lang.String(name)node.nodeId()",o,o,x
Elasticsearch,635,org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate,216,debug,"java.lang.String(name)more than the allowed {} used disk threshold ({} used) on node [{}], but allowing allocation because primary has never been allocated-java.lang.String(name)Strings.format1Decimals(usedDiskThresholdLow,""%"")-java.lang.String(name)Strings.format1Decimals(usedDiskPercentage,""%"")-java.lang.String(name)node.nodeId()",o,o,x
Elasticsearch,602,org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor.onNewInfo,200,debug,"java.lang.String(name){} has gone below a disk threshold, but an automatic reroute has occurred in the last [{}], skipping reroute-java.lang.String(name)node-org.elasticsearch.common.unit.TimeValue(name)diskThresholdSettings.getRerouteInterval()",o,o,o
Elasticsearch,595,org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor.warnAboutDiskIfNeeded,98,warn,"java.lang.String(name)high disk watermark [{}] exceeded on {}, shards will be relocated away from this node-org.elasticsearch.common.unit.ByteSizeValue(name)diskThresholdSettings.getFreeBytesThresholdHigh()-org.elasticsearch.cluster.DiskUsage(name)usage",o,o,x
Elasticsearch,587,org.elasticsearch.cluster.routing.DelayedAllocationService.scheduleIfNeeded,213,trace,java.lang.String(name)no need to reschedule delayed reroute - currently scheduled delayed reroute in [{}] is enough-org.elasticsearch.common.unit.TimeValue(name)nextDelay,o,o,x
Elasticsearch,585,org.elasticsearch.cluster.routing.DelayedAllocationService.scheduleIfNeeded,198,trace,java.lang.String(name)cancelling existing delayed reroute task as delayed reroute has to happen [{}] earlier-org.elasticsearch.common.unit.TimeValue(name)TimeValue.timeValueNanos(existingTask.scheduledTimeToRunInNanos() - newTask.scheduledTimeToRunInNanos()),o,o,x
Elasticsearch,694,org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.circuitBreak,97,debug,java.lang.String(name){}-java.lang.String(name)message,x,x,x
Elasticsearch,702,org.elasticsearch.common.geo.builders.PolygonBuilder.assign,513,debug,java.lang.String(name)Holes: {}-java.lang.String(name)Arrays.toString(holes),o,x,o
Elasticsearch,704,org.elasticsearch.common.geo.builders.PolygonBuilder.assign,570,debug,java.lang.String(name) Component: {}-int(name)component,o,x,o
Elasticsearch,701,org.elasticsearch.common.geo.builders.PolygonBuilder.buildCoordinates,461,debug,java.lang.String(name) {}-java.lang.String(name)Arrays.toString(result[i][j]),x,x,x
Elasticsearch,699,org.elasticsearch.common.geo.builders.PolygonBuilder.component,368,debug,java.lang.String(name)shift: [{}]-double(name)shiftOffset,o,x,x
Elasticsearch,706,org.elasticsearch.common.io.FileSystemUtils.isAccessibleDirectory,117,debug,java.lang.String(name)[{}] directory does not exist.-java.nio.file.Path(name)directory.toAbsolutePath(),o,o,o
Elasticsearch,708,org.elasticsearch.common.io.FileSystemUtils.isAccessibleDirectory,125,debug,java.lang.String(name)[{}] directory is not readable.-java.nio.file.Path(name)directory.toAbsolutePath(),o,o,o
Elasticsearch,713,org.elasticsearch.common.logging.LogConfigurator.configure,240,warn,java.lang.String(name)Some logging configurations have %marker but don't have %node_name. We will automatically add %node_name to the pattern to ease the migration for users who customize log4j2.properties but will stop this behavior in 7.0. You should manually replace `%node_name` with `[%node_name]%marker ` in these locations: {}-java.lang.String(name)deprecatedLocationsString,o,o,o
Elasticsearch,714,org.elasticsearch.common.lucene.LoggerInfoStream.message,43,trace,java.lang.String(name){} {}: {}-java.lang.String(name)Thread.currentThread().getName()-java.lang.String(name)component-java.lang.String(name)message,x,x,x
Elasticsearch,716,org.elasticsearch.common.network.IfConfig.doLogging,107,debug,java.lang.String(name)configuration:{}{}-java.lang.String(name)System.lineSeparator()-java.lang.StringBuilder(name)msg,x,x,x
Elasticsearch,715,org.elasticsearch.common.network.IfConfig.logIfNecessary,48,warn,java.lang.String(name)unable to gather network information-java.io.IOException(name)e,o,o,x
Elasticsearch,717,org.elasticsearch.common.settings.ConsistentSettingsService.areAllConsistent,150,warn,java.lang.String(name)the consistent secure setting [{}] does not exist on the local node but there is a published hash for it-java.lang.String(name)publishedSettingKey,o,o,x
Elasticsearch,721,org.elasticsearch.common.settings.Setting.logSettingUpdate,1359,info,java.lang.String(name)updating [{}]-org.elasticsearch.common.settings.Key(name)setting.key,o,o,x
Elasticsearch,729,org.elasticsearch.common.util.concurrent.ThreadContext.putResponse,557,warn,"java.lang.String(name)""Dropping a warning header, as their total size reached the maximum allowed of ["" + maxWarningHeaderSize + ""] bytes set in [""+ HttpTransportSettings.SETTING_HTTP_MAX_WARNING_HEADER_SIZE.getKey()+ ""]!""",o,o,o
Elasticsearch,66,org.elasticsearch.discovery.azure.classic.AzureSeedHostsProvider.getSeedAddresses,157,debug,java.lang.String(name)Azure discovery service has been disabled. Returning empty list of nodes.,o,o,o
Elasticsearch,70,org.elasticsearch.discovery.azure.classic.AzureSeedHostsProvider.getSeedAddresses,173,trace,java.lang.String(name)exception while finding ip-java.io.IOException(name)e,o,o,x
Elasticsearch,98,org.elasticsearch.discovery.ec2.AwsEc2SeedHostsProvider.fetchDynamicNodes,169,debug,java.lang.String(name)using [{}] as the instance address-java.lang.String(name)address,o,o,x
Elasticsearch,93,org.elasticsearch.discovery.ec2.AwsEc2SeedHostsProvider.fetchDynamicNodes,116,debug,java.lang.String(name)Full exception:-com.amazonaws.AmazonClientException(name)e,o,x,o
Elasticsearch,139,org.elasticsearch.discovery.gce.GceSeedHostsProvider.getSeedAddresses,255,debug,java.lang.String(name)using transport addresses {}-java.util.List<TransportAddress>(name)cachedDynamicHosts,o,o,x
Elasticsearch,128,org.elasticsearch.discovery.gce.GceSeedHostsProvider.getSeedAddresses,161,trace,java.lang.String(name)no tags for this instance but we asked for tags. {} won't be part of the cluster.-java.lang.String(name)name,o,o,x
Elasticsearch,130,org.elasticsearch.discovery.gce.GceSeedHostsProvider.getSeedAddresses,182,trace,"java.lang.String(name)filtering out instance {} based tags {}, not part of {}-java.lang.String(name)name-java.util.List<String>(name)tags-java.lang.Object(name)instance.getTags() == null || instance.getTags().getItems() == null ? """" : instance.getTags()",o,o,x
Elasticsearch,135,org.elasticsearch.discovery.gce.GceSeedHostsProvider.getSeedAddresses,239,trace,"java.lang.String(name)adding {}, type {}, address {}, transport_address {}, status {}-java.lang.String(name)name-java.lang.String(name)type-java.lang.String(name)ip_private-org.elasticsearch.common.transport.TransportAddress(name)transportAddress-java.lang.String(name)status",o,x,x
Elasticsearch,137,org.elasticsearch.discovery.gce.GceSeedHostsProvider.getSeedAddresses,251,warn,java.lang.String(name)exception caught during discovery-java.lang.Exception(name)e,o,o,x
Elasticsearch,740,org.elasticsearch.discovery.PeerFinder.handleWakeUp,263,trace,java.lang.String(name)not active,o,x,x
Elasticsearch,843,org.elasticsearch.discovery.zen.ElectMasterService.logMinimumMasterNodesWarningIfNecessary,166,warn,"java.lang.String(name)value for setting ""{}"" is too low. This can result in data loss! Please set it to at least a quorum of master-eligible nodes (current value: [{}], total number of master-eligible nodes used for publishing in this round: [{}])-java.lang.String(name)ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey()-int(name)minimumMasterNodes()-int(name)newState.getNodes().getMasterNodes().size()",o,o,x
Elasticsearch,845,org.elasticsearch.discovery.zen.MasterFaultDetection.restart,114,debug,"java.lang.String(name)[master] restarting fault detection against master [{}], reason [{}]-org.elasticsearch.cluster.node.DiscoveryNode(name)masterNode-java.lang.String(name)reason",o,o,o
Elasticsearch,852,org.elasticsearch.discovery.zen.MasterFaultDetection.run,270,debug,"java.lang.String(name)[master] pinging a master {} but we do not exists on it, act as if its master failure-org.elasticsearch.cluster.node.DiscoveryNode(name)masterNode",o,o,x
Elasticsearch,846,org.elasticsearch.discovery.zen.MasterFaultDetection.stop,138,debug,"java.lang.String(name)[master] stopping fault detection against master [{}], reason [{}]-org.elasticsearch.cluster.node.DiscoveryNode(name)masterNode-java.lang.String(name)reason",o,o,o
Elasticsearch,776,org.elasticsearch.discovery.zen.PublishClusterStateAction.innerPublish,219,warn,"java.lang.String(name)timed out waiting for all nodes to process published state [{}] (timeout [{}], pending nodes: {})-long(name)clusterState.version()-org.elasticsearch.common.unit.TimeValue(name)publishTimeout-DiscoveryNode[](name)pendingNodes",o,o,x
Elasticsearch,816,org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState,753,debug,"java.lang.String(name)received a cluster state that has a lower version than the current one, ignoring (received {}, current {})-long(name)newClusterState.version()-long(name)currentState.version()",o,o,x
Elasticsearch,828,org.elasticsearch.discovery.zen.ZenDiscovery.validateIncomingState,1013,warn,java.lang.String(name)received cluster state from [{}] which is also master but with a different cluster name [{}]-org.elasticsearch.cluster.node.DiscoveryNode(name)incomingState.nodes().getMasterNode()-org.elasticsearch.cluster.ClusterName(name)incomingClusterName,o,o,x
Elasticsearch,907,org.elasticsearch.gateway.DanglingIndicesState.cleanupAllocatedDangledIndices,103,warn,"java.lang.String(name)[{}] can not be imported as a dangling index, as there is already another index with the same name but a different uuid. local index will be ignored (but not deleted)-org.elasticsearch.index.Index(name)index",o,o,o
Elasticsearch,910,org.elasticsearch.gateway.DanglingIndicesState.findNewDanglingIndices,141,warn,"java.lang.String(name)[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the index tombstones. This situation is likely caused by copying over the data directory for an index that was previously deleted.-org.elasticsearch.index.Index(name)indexMetaData.getIndex()",o,o,o
Elasticsearch,909,org.elasticsearch.gateway.DanglingIndicesState.findNewDanglingIndices,138,warn,"java.lang.String(name)[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata-org.elasticsearch.index.Index(name)indexMetaData.getIndex()",o,o,o
Elasticsearch,885,org.elasticsearch.gateway.GatewayMetaState.applyClusterState,210,warn,java.lang.String(name)Exception occurred when storing new meta data-org.elasticsearch.gateway.WriteStateException(name)e,o,o,o
Elasticsearch,886,org.elasticsearch.gateway.GatewayMetaState.setCurrentTerm,230,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""Failed to set current term to {}"",currentTerm)-org.elasticsearch.gateway.WriteStateException(name)e",o,o,o
Elasticsearch,887,org.elasticsearch.gateway.GatewayMetaState.setLastAcceptedState,248,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""Failed to set last accepted state with version {}"",clusterState.version())-org.elasticsearch.gateway.WriteStateException(name)e",o,o,o
Elasticsearch,917,org.elasticsearch.gateway.LocalAllocateDangledIndices.messageReceived,124,warn,java.lang.String(name)ignoring dangled index [{}] on node [{}] due to an existing alias with the same name-org.elasticsearch.index.Index(name)indexMetaData.getIndex()-org.elasticsearch.cluster.node.DiscoveryNode(name)request.fromNode,o,o,x
Elasticsearch,962,org.elasticsearch.http.AbstractHttpServerTransport.bindServer,157,info,java.lang.String(name){}-org.elasticsearch.common.transport.BoundTransportAddress(name)boundAddress,x,x,x
Elasticsearch,1008,org.elasticsearch.index.engine.CombinedDeletionPolicy.updateRetentionPolicy,116,debug,"java.lang.String(name)Safe commit [{}], last commit [{}]-java.lang.String(name)commitDescription(safeCommit)-java.lang.String(name)commitDescription(lastCommit)",o,o,o
Elasticsearch,996,org.elasticsearch.index.MergePolicyConfig.MergePolicyConfig,176,warn,"java.lang.String(name)[{}] is set to false, this should only be used in tests and can cause serious problems in production environments-java.lang.String(name)index.merge.enabled",o,o,o
Elasticsearch,1073,org.elasticsearch.index.reindex.ClientScrollableHitSource.doStart,72,debug,"java.lang.String(name)executing initial scroll against {}{}-java.lang.Object(name)isEmpty(firstSearchRequest.indices()) ? ""all indices"" : firstSearchRequest.indices()-java.lang.Object(name)isEmpty(firstSearchRequest.types()) ? """" : firstSearchRequest.types()",x,o,x
Elasticsearch,1082,org.elasticsearch.index.seqno.ReplicationTracker.updateGlobalCheckpointForShard,977,trace,java.lang.String(name)updated local knowledge for [{}] on the primary of the global checkpoint from [{}] to [{}]-java.lang.String(name)allocationId-long(name)previousGlobalCheckpoint-long(name)globalCheckpoint,o,o,x
Elasticsearch,1144,org.elasticsearch.index.shard.ElasticsearchMergePolicy.findForcedMerges,97,debug,java.lang.String(name)Adding segment {} to be upgraded-java.lang.String(name)info.info.name,o,o,o
Elasticsearch,1113,org.elasticsearch.index.shard.IndexShard.recoverLocallyUpToGlobalCheckpoint,1415,trace,java.lang.String(name)skip local recovery as the index was closed or not allowed to write; safe commit {} global checkpoint {}-org.elasticsearch.index.seqno.CommitInfo(name)safeCommit.get()-long(name)globalCheckpoint,o,o,x
Elasticsearch,1092,org.elasticsearch.index.shard.ShardPath.loadShardPath,141,warn,java.lang.String(name){} found shard on path: [{}] with a different index UUID - this shard seems to be leftover from a different index with the same name. Remove the leftover shard in order to reuse the path with the current index-org.elasticsearch.index.shard.ShardId(name)shardId-java.nio.file.Path(name)path,o,o,o
Elasticsearch,1154,org.elasticsearch.index.store.Store.readMetadataSnapshot,456,info,java.lang.String(name)Failed to open / find files while reading metadata snapshot-java.io.IOException(name)ex,o,o,o
Elasticsearch,1216,org.elasticsearch.indices.cluster.IndicesClusterStateService.updateShard,650,trace,"java.lang.String(name){} master marked shard as initializing, but shard has state [{}], resending shard started to {}-org.elasticsearch.index.shard.ShardId(name)shardRouting.shardId()-org.elasticsearch.index.shard.IndexShardState(name)state-org.elasticsearch.cluster.node.DiscoveryNode(name)nodes.getMasterNode()",o,o,x
Elasticsearch,1223,org.elasticsearch.indices.flush.SyncedFlushService.getInflightOpsCount,317,trace,"java.lang.String(name){} failed to resolve node for primary shard {}, skipping sync-org.elasticsearch.index.shard.ShardId(name)shardId-org.elasticsearch.cluster.routing.ShardRouting(name)primaryShard",o,o,o
Elasticsearch,1182,org.elasticsearch.indices.IndicesService.run,1201,warn,java.lang.String(name)Exception during periodic field data cache cleanup:-java.lang.Exception(name)e,o,o,o
Elasticsearch,1184,org.elasticsearch.indices.IndicesService.run,1211,warn,java.lang.String(name)Exception during periodic request cache cleanup:-java.lang.Exception(name)e,o,o,o
Elasticsearch,1259,org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStartRecoveryRequest,341,warn,"java.lang.String(name)error while listing local files, recovering as if there are none-java.io.IOException(name)e",o,o,x
Elasticsearch,1275,org.elasticsearch.indices.recovery.RecoverySourceHandler.handleErrorOnSendFiles,945,warn,java.lang.String(name){} Corrupted file detected {} checksum mismatch-int(name)shardId-org.elasticsearch.index.store.StoreFileMetaData(name)md,o,o,o
Elasticsearch,1279,org.elasticsearch.indices.store.IndicesStore.deleteShardIfExistElseWhere,221,trace,java.lang.String(name){} sending shard active check to {}-org.elasticsearch.index.shard.ShardId(name)request.v2().shardId-org.elasticsearch.cluster.node.DiscoveryNode(name)request.v1(),o,o,o
Elasticsearch,1288,org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData,170,debug,java.lang.String(name){} loaded store meta data (took [{}])-org.elasticsearch.index.shard.ShardId(name)shardId-org.elasticsearch.common.unit.TimeValue(name)took,o,o,o
Elasticsearch,1289,org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData,172,trace,java.lang.String(name){} didn't find any store meta data to load (took [{}])-org.elasticsearch.index.shard.ShardId(name)shardId-org.elasticsearch.common.unit.TimeValue(name)took,o,o,o
Elasticsearch,1582,org.elasticsearch.license.LicenseService.clusterChanged,414,debug,java.lang.String(name)previous [{}]-org.elasticsearch.license.LicensesMetaData(name)prevLicensesMetaData,o,x,x
Elasticsearch,1583,org.elasticsearch.license.LicenseService.clusterChanged,415,debug,java.lang.String(name)current [{}]-org.elasticsearch.license.LicensesMetaData(name)currentLicensesMetaData,o,x,x
Elasticsearch,1580,org.elasticsearch.license.LicenseService.logExpirationWarning,137,warn,"java.lang.String(name){}-java.lang.CharSequence(name)buildExpirationMessage(expirationMillis,expired)",x,x,x
Elasticsearch,1590,org.elasticsearch.license.OperationModeFileWatcher.onChange,101,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.io.IOException(name)e,x,x,x
Elasticsearch,1591,org.elasticsearch.license.OperationModeFileWatcher.onChange,111,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.IllegalArgumentException(name)e,x,x,x
Elasticsearch,1577,org.elasticsearch.license.StartupSelfGeneratedLicenseTask.extendBasic,108,info,java.lang.String(name)Existing basic license has an expiration. Basic licenses no longer expire.Regenerating license. Old license: {} New license: {}-org.elasticsearch.license.License(name)license-org.elasticsearch.license.License(name)newLicenseMetadata.getLicense(),o,o,o
Elasticsearch,1576,org.elasticsearch.license.StartupSelfGeneratedLicenseTask.onFailure,100,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.Exception(name)e,x,x,x
Elasticsearch,1312,org.elasticsearch.monitor.os.OsProbe.getCgroup,515,debug,java.lang.String(name)no [cpuacct] data found in cgroup stats,o,o,x
Elasticsearch,1313,org.elasticsearch.monitor.os.OsProbe.getCgroup,522,debug,java.lang.String(name)no [cpu] data found in cgroup stats,o,o,x
Elasticsearch,1314,org.elasticsearch.monitor.os.OsProbe.getCgroup,531,debug,java.lang.String(name)no [memory] data found in cgroup stats,o,o,x
Elasticsearch,1305,org.elasticsearch.monitor.os.OsProbe.getFreePhysicalMemorySize,94,warn,java.lang.String(name)OS reported a negative free memory value [{}]-long(name)freeMem,o,o,o
Elasticsearch,1308,org.elasticsearch.monitor.os.OsProbe.getTotalPhysicalMemorySize,115,warn,java.lang.String(name)OS reported a negative total memory value [{}]-long(name)totalMem,o,o,o
Elasticsearch,1331,org.elasticsearch.node.Node.close,826,info,java.lang.String(name)closing ...,o,x,x
Elasticsearch,1332,org.elasticsearch.node.Node.close,882,info,java.lang.String(name)closed,x,x,x
Elasticsearch,1324,org.elasticsearch.node.Node.Node,610,info,java.lang.String(name)initialized,x,x,x
Elasticsearch,1325,org.elasticsearch.node.Node.start,670,info,java.lang.String(name)starting ...,o,x,x
Elasticsearch,1328,org.elasticsearch.node.Node.start,766,info,java.lang.String(name)started,x,x,x
Elasticsearch,1329,org.elasticsearch.node.Node.stop,782,info,java.lang.String(name)stopping ...,o,x,x
Elasticsearch,1330,org.elasticsearch.node.Node.stop,806,info,java.lang.String(name)stopped,x,x,x
Elasticsearch,1346,org.elasticsearch.persistent.PersistentTasksClusterService.completePersistentTask,157,warn,"java.lang.String(name)The task [{}] with id [{}] was found but it has a different allocation id [{}], status is not updated-java.lang.String(name)PersistentTasksCustomMetaData.getTaskWithId(currentState,id).getTaskName()-java.lang.String(name)id-long(name)allocationId",o,o,o
Elasticsearch,144,org.elasticsearch.repositories.azure.AzureBlobContainer.blobExists,62,trace,java.lang.String(name)blobExists({})-java.lang.String(name)blobName,x,x,x
Elasticsearch,148,org.elasticsearch.repositories.azure.AzureBlobContainer.deleteBlob,115,trace,java.lang.String(name)deleteBlob({})-java.lang.String(name)blobName,x,x,x
Elasticsearch,151,org.elasticsearch.repositories.azure.AzureBlobContainer.listBlobs,180,trace,java.lang.String(name)listBlobs(),x,x,x
Elasticsearch,149,org.elasticsearch.repositories.azure.AzureBlobContainer.listBlobsByPrefix,168,trace,java.lang.String(name)listBlobsByPrefix({})-java.lang.String(name)prefix,x,x,x
Elasticsearch,146,org.elasticsearch.repositories.azure.AzureBlobContainer.readBlob,73,trace,java.lang.String(name)readBlob({})-java.lang.String(name)blobName,x,x,x
Elasticsearch,1390,org.elasticsearch.repositories.blobstore.BlobStoreRepository.cleanupStaleIndices,562,debug,java.lang.String(name)[{}] Found stale index [{}]. Cleaning it up-java.lang.String(name)metadata.name()-java.lang.String(name)indexSnId,o,o,o
Elasticsearch,1388,org.elasticsearch.repositories.blobstore.BlobStoreRepository.cleanupStaleRootFiles,538,info,java.lang.String(name)[{}] Found stale root level blobs {}. Cleaning them up-java.lang.String(name)metadata.name()-java.util.List<String>(name)blobsToDelete,o,o,o
Elasticsearch,1399,org.elasticsearch.repositories.blobstore.BlobStoreRepository.snapshotShard,956,debug,"java.lang.String(name)[{}] [{}] Aborted on the file [{}], exiting-org.elasticsearch.index.shard.ShardId(name)shardId-org.elasticsearch.snapshots.SnapshotId(name)snapshotId-java.lang.String(name)fileName",o,o,o
Elasticsearch,1383,org.elasticsearch.repositories.blobstore.FileRestoreContext.restore,202,warn,java.lang.String(name)[{}] [{}] failed to delete file [{}] during snapshot cleanup-org.elasticsearch.index.shard.ShardId(name)shardId-org.elasticsearch.snapshots.SnapshotId(name)snapshotId-java.lang.String(name)storeFile,o,o,o
Elasticsearch,1407,org.elasticsearch.repositories.fs.FsRepository.FsRepository,95,warn,"java.lang.String(name)The specified location [{}] should start with a repository path specified by the path.repo setting, but the path.repo setting was not set on this node-java.lang.String(name)location",o,o,o
Elasticsearch,156,org.elasticsearch.repositories.hdfs.HdfsRepository.login,166,warn,"java.lang.String(name)Hadoop authentication method is set to [SIMPLE], but a Kerberos principal is specified. Continuing with [KERBEROS] authentication.",o,o,o
Elasticsearch,162,org.elasticsearch.repositories.s3.S3Service.buildCredentials,192,debug,java.lang.String(name)Using instance profile credentials,o,o,o
Elasticsearch,163,org.elasticsearch.repositories.s3.S3Service.buildCredentials,195,debug,java.lang.String(name)Using basic key/secret credentials,o,o,o
Elasticsearch,1410,org.elasticsearch.rest.BytesRestResponse.build,131,debug,org.apache.logging.log4j.util.Supplier<>(name)messageSupplier-java.lang.Exception(name)e,x,x,x
Elasticsearch,1411,org.elasticsearch.rest.BytesRestResponse.build,133,warn,org.apache.logging.log4j.util.Supplier<>(name)messageSupplier-java.lang.Exception(name)e,x,x,x
Elasticsearch,1421,org.elasticsearch.search.fetch.FetchPhase.execute,94,trace,java.lang.String(name){}-org.elasticsearch.search.SearchContextSourcePrinter(name)new SearchContextSourcePrinter(context),x,x,x
Elasticsearch,1422,org.elasticsearch.search.query.QueryPhase.execute,105,trace,java.lang.String(name){}-org.elasticsearch.search.SearchContextSourcePrinter(name)new SearchContextSourcePrinter(searchContext),x,x,x
Elasticsearch,1503,org.elasticsearch.snapshots.mockstore.MockRepository.blockExecutionAndMaybeWait,290,info,java.lang.String(name)[{}] blocking I/O operation for file [{}] at path [{}]-java.lang.String(name)metadata.name()-java.lang.String(name)blobName-org.elasticsearch.common.blobstore.BlobPath(name)path(),o,o,o
Elasticsearch,1440,org.elasticsearch.snapshots.SnapshotsService.beginSnapshot,483,warn,java.lang.String(name)[{}] failed to create snapshot - no longer a master-org.elasticsearch.snapshots.SnapshotId(name)snapshot.snapshot().getSnapshotId(),o,o,o
Elasticsearch,1460,org.elasticsearch.tasks.TaskManager.setBan,313,trace,java.lang.String(name)setting ban for the parent task {} {}-org.elasticsearch.tasks.TaskId(name)parentTaskId-java.lang.String(name)reason,x,o,x
Elasticsearch,1490,org.elasticsearch.transport.RemoteClusterConnection.collectRemoteNodes,454,debug,java.lang.String(name)[{}] opening connection to seed node: [{}] proxy address: [{}]-java.lang.String(name)clusterAlias-org.elasticsearch.cluster.node.DiscoveryNode(name)seedNode-java.lang.String(name)proxyAddress,o,o,o
Elasticsearch,1486,org.elasticsearch.transport.TransportLogger.logInboundMessage,43,trace,java.lang.String(name)logMessage,x,x,x
Elasticsearch,1488,org.elasticsearch.transport.TransportLogger.logOutboundMessage,59,trace,java.lang.String(name)logMessage,x,x,x
Elasticsearch,1479,org.elasticsearch.transport.TransportService.checkForTimeout,965,trace,java.lang.String(name)[{}] received response but can't resolve it to a request-long(name)requestId,o,o,o
Elasticsearch,1477,org.elasticsearch.transport.TransportService.checkForTimeout,949,warn,"java.lang.String(name)Received response for a request that has timed out, sent [{}ms] ago, timed out [{}ms] ago, action [{}], node [{}], id [{}]-long(name)time - timeoutInfoHolder.sentTime()-long(name)time - timeoutInfoHolder.timeoutTime()-java.lang.String(name)timeoutInfoHolder.action()-org.elasticsearch.cluster.node.DiscoveryNode(name)timeoutInfoHolder.node()-long(name)requestId",o,o,o
Elasticsearch,1469,org.elasticsearch.transport.TransportService.doStart,232,info,java.lang.String(name){}-org.elasticsearch.common.transport.BoundTransportAddress(name)transport.boundAddress(),x,x,x
Elasticsearch,1533,org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator.checkAutoFollowPattern,498,warn,java.lang.String(name)message,x,x,x
Elasticsearch,1534,org.elasticsearch.xpack.ccr.action.ShardChangesAction.asyncShardOperation,385,trace,java.lang.String(name){} waiting for global checkpoint advancement from [{}] to [{}]-org.elasticsearch.index.shard.ShardId(name)shardId-long(name)seqNoStats.getGlobalCheckpoint()-long(name)request.getFromSeqNo(),o,o,o
Elasticsearch,1535,org.elasticsearch.xpack.ccr.action.ShardChangesAction.globalCheckpointAdvanced,413,trace,java.lang.String(name){} global checkpoint advanced to [{}] after waiting for [{}]-org.elasticsearch.index.shard.ShardId(name)shardId-long(name)globalCheckpoint-long(name)request.getFromSeqNo(),o,o,o
Elasticsearch,1545,org.elasticsearch.xpack.ccr.action.ShardFollowNodeTask.hasWriteBudget,267,trace,java.lang.String(name){} maximum number of concurrent writes have been reached [{}]-org.elasticsearch.index.shard.ShardId(name)params.getFollowShardId()-int(name)numOutstandingWrites,o,o,o
Elasticsearch,1546,org.elasticsearch.xpack.ccr.action.ShardFollowNodeTask.innerHandleReadResponse,378,trace,"java.lang.String(name){} received [{}] ops, still missing [{}/{}], continuing to read...-org.elasticsearch.index.shard.ShardId(name)params.getFollowShardId()-int(name)response.getOperations().length-long(name)newFromSeqNo-long(name)maxRequiredSeqNo",o,o,o
Elasticsearch,1519,org.elasticsearch.xpack.ccr.action.ShardFollowTasksExecutor.createTask,473,warn,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""{} background management of retention lease [{}] failed while following"",params.getFollowShardId(),retentionLeaseId)-java.lang.Throwable(name)cause",o,o,o
Elasticsearch,1555,org.elasticsearch.xpack.ccr.action.TransportUnfollowAction.masterOperation,121,trace,java.lang.String(name)[{}] removed retention lease [{}] on all leader primary shards-org.elasticsearch.index.Index(name)indexMetaData.getIndex()-java.lang.String(name)retentionLeaseId,o,o,o
Elasticsearch,1558,org.elasticsearch.xpack.ccr.action.TransportUnfollowAction.masterOperation,187,trace,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""{} retention lease [{}] not found on {} while unfollowing"",followerShardId,retentionLeaseId,leaderShardId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,1596,org.elasticsearch.xpack.core.common.notifications.AbstractAuditor.onIndexFailure,63,debug,java.lang.String(name)Failed to write audit message-java.lang.Exception(name)exception,o,o,o
Elasticsearch,1602,org.elasticsearch.xpack.core.ilm.UpdateRolloverLifecycleDateStep.performAction,44,trace,"java.lang.String(name)indexMetaData.getIndex() + "" has lifecycle complete set, skipping "" + UpdateRolloverLifecycleDateStep.NAME",o,x,x
Elasticsearch,1620,org.elasticsearch.xpack.core.ilm.WaitForRolloverReadyStep.evaluateCondition,74,trace,"java.lang.String(name)indexMetaData.getIndex() + "" has lifecycle complete set, skipping "" + WaitForRolloverReadyStep.NAME",o,x,x
Elasticsearch,1623,org.elasticsearch.xpack.core.indexing.AsyncTwoPhaseIndexer.maybeTriggerAsyncJob,146,debug,"java.lang.String(name)""Schedule was triggered for job ["" + getJobId() + ""] but job is stopped. Ignoring trigger.""",o,o,o
Elasticsearch,1626,org.elasticsearch.xpack.core.indexing.AsyncTwoPhaseIndexer.maybeTriggerAsyncJob,171,debug,"java.lang.String(name)""Could not move from STARTED to INDEXING state because current state is ["" + state.get() + ""]""",o,o,o
Elasticsearch,1632,org.elasticsearch.xpack.core.ml.datafeed.AggProvider.fromXContent,53,warn,java.lang.String(name)Datafeed aggregations are not parsable-java.lang.Exception(name)ex,o,o,o
Elasticsearch,1641,org.elasticsearch.xpack.core.ml.job.persistence.ElasticsearchMappings.addDocMappingIfMissing,1242,trace,java.lang.String(name)Mappings are up to date.,o,o,o
Elasticsearch,1642,org.elasticsearch.xpack.core.ml.utils.QueryProvider.fromXContent,56,warn,java.lang.String(name)failureMessage-java.lang.Exception(name)ex,x,x,x
Elasticsearch,1650,org.elasticsearch.xpack.core.security.authz.permission.ApplicationPermission.grants,84,trace,"java.lang.String(name)Permission [{}] {} grant [{} , {}]-org.elasticsearch.xpack.core.security.authz.permission.ApplicationPermission(name)this-java.lang.Object(name)matched ? ""does"" : ""does not""-org.elasticsearch.xpack.core.security.authz.privilege.ApplicationPrivilege(name)other-java.lang.String(name)resource",o,o,o
Elasticsearch,1655,org.elasticsearch.xpack.core.security.transport.SecurityTransportExceptionHandler.accept,39,debug,java.lang.String(name)connection {} closed during handshake-org.elasticsearch.transport.TcpChannel(name)channel,o,o,x
Elasticsearch,1653,org.elasticsearch.xpack.core.security.transport.SecurityTransportExceptionHandler.accept,33,warn,"java.lang.String(name)received plaintext traffic on an encrypted channel, closing connection {}-org.elasticsearch.transport.TcpChannel(name)channel",o,o,x
Elasticsearch,1657,org.elasticsearch.xpack.core.security.transport.SecurityTransportExceptionHandler.accept,46,warn,"java.lang.String(name)client did not trust this server's certificate, closing connection {}-org.elasticsearch.transport.TcpChannel(name)channel",o,o,x
Elasticsearch,1658,org.elasticsearch.xpack.core.slm.history.SnapshotHistoryStore.putAsync,58,trace,java.lang.String(name)not recording snapshot history item because [{}] is [false]: [{}]-java.lang.String(name)SLM_HISTORY_INDEX_ENABLED_SETTING.getKey()-org.elasticsearch.xpack.core.slm.history.SnapshotHistoryItem(name)item,o,o,x
Elasticsearch,1663,org.elasticsearch.xpack.core.ssl.RestrictedTrustManager.verifyTrust,101,info,java.lang.String(name)Rejecting certificate [{}] [{}] with common-names [{}]-java.security.Principal(name)certificate.getSubjectDN()-java.lang.String(name)certificate.getSerialNumber().toString(16)-java.util.Set<String>(name)names,o,o,o
Elasticsearch,1674,org.elasticsearch.xpack.core.ssl.SSLService.getSSLConfiguration,717,warn,java.lang.String(name)Cannot find SSL configuration for context {}. Known contexts are: {}-java.lang.String(name)contextName-java.lang.String(name)Strings.collectionToCommaDelimitedString(sslConfigurations.keySet()),o,o,x
Elasticsearch,1672,org.elasticsearch.xpack.core.ssl.SSLService.supportedCiphers,366,error,"java.lang.String(name)unsupported ciphers [{}] were requested but cannot be used in this JVM, however there are supported ciphers that will be used [{}]. If you are trying to use ciphers with a key length greater than 128 bits on an Oracle JVM, you will need to install the unlimited strength JCE policy files.-java.util.List<String>(name)unsupportedCiphers-java.util.List<String>(name)supportedCiphersList",o,o,x
Elasticsearch,1682,org.elasticsearch.xpack.core.template.IndexTemplateRegistry.addIndexLifecyclePoliciesIfMissing,195,trace,"java.lang.String(name)not adding lifecycle policy [{}] for [{}], because it already exists-java.lang.String(name)policy.getName()-java.lang.String(name)getOrigin()",o,o,x
Elasticsearch,1680,org.elasticsearch.xpack.core.template.IndexTemplateRegistry.addTemplatesIfMissing,142,trace,"java.lang.String(name)not adding index template [{}] for [{}], because it already exists-java.lang.String(name)templateName-java.lang.String(name)getOrigin()",o,o,x
Elasticsearch,1678,org.elasticsearch.xpack.core.template.IndexTemplateRegistry.onPutPolicyFailure,101,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""error adding lifecycle policy [{}] for [{}]"",policy.getName(),getOrigin())-java.lang.Exception(name)e",o,o,x
Elasticsearch,1683,org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute,136,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.RuntimeException(name)e,x,x,x
Elasticsearch,1685,org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute,156,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.Exception(name)e,x,x,x
Elasticsearch,1686,org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute,167,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.Exception(name)e,x,x,x
Elasticsearch,1687,org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute,226,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.Exception(name)e,x,x,x
Elasticsearch,1689,org.elasticsearch.xpack.core.watcher.transform.chain.ExecutableChainTransform.execute,48,error,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.lang.Exception(name)e,x,x,x
Elasticsearch,1695,org.elasticsearch.xpack.dataframe.checkpoint.TimeBasedCheckpointProvider.sourceHasChanged,65,trace,java.lang.String(name)query for changes based on time: {}-org.elasticsearch.search.builder.SearchSourceBuilder(name)sourceBuilder,o,o,x
Elasticsearch,1696,org.elasticsearch.xpack.dataframe.persistence.DataFrameTransformsConfigManager.parseTransformLenientlyFromSource,536,error,"java.lang.String(name)DataFrameMessages.getMessage(DataFrameMessages.FAILED_TO_PARSE_TRANSFORM_CONFIGURATION,transformId)-java.lang.Exception(name)e",x,x,x
Elasticsearch,1724,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.doSaveState,872,debug,java.lang.String(name)[{}] attempted to save state and stats while failed.-java.lang.String(name)transformId,o,o,o
Elasticsearch,1725,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.doSaveState,906,info,"java.lang.String(name)[{}] data frame transform finished indexing all data, initiating stop.-java.lang.String(name)transformConfig.getId()",o,o,o
Elasticsearch,1717,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.markAsFailed,444,info,java.lang.String(name)[{}] encountered a failure but indexer is STOPPED; reason [{}].-java.lang.String(name)getTransformId()-java.lang.String(name)reason,o,o,o
Elasticsearch,1715,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.markAsFailed,429,warn,java.lang.String(name)[{}] is already failed but encountered new failure; reason [{}].-java.lang.String(name)getTransformId()-java.lang.String(name)reason,o,o,o
Elasticsearch,1721,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.maybeTriggerAsyncJob,808,debug,java.lang.String(name)[{}] indexer for transform has state [{}]. Ignoring trigger.-java.lang.String(name)getJobId()-org.elasticsearch.xpack.core.indexing.IndexerState(name)indexerState,o,o,o
Elasticsearch,1718,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.onCancelled,479,info,"java.lang.String(name)[{}] received cancellation request for data frame transform, state: [{}].-java.lang.String(name)getTransformId()-org.elasticsearch.xpack.core.dataframe.transforms.DataFrameTransformTaskState(name)taskState.get()",o,o,o
Elasticsearch,1727,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.onFailure,970,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] data frame transform encountered an unexpected internal exception: "",transformId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,1712,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.triggered,374,debug,java.lang.String(name)[{}] indexer for transform has state [{}]. Ignoring trigger.-java.lang.String(name)getTransformId()-org.elasticsearch.xpack.core.indexing.IndexerState(name)indexerState,o,o,o
Elasticsearch,1713,org.elasticsearch.xpack.dataframe.transforms.DataFrameTransformTask.triggered,378,debug,"java.lang.String(name)[{}] data frame indexer schedule has triggered, state: [{}].-java.lang.String(name)event.getJobName()-org.elasticsearch.xpack.core.indexing.IndexerState(name)indexerState",o,o,o
Elasticsearch,1781,org.elasticsearch.xpack.ilm.action.TransportPutLifecycleAction.masterOperation,93,info,java.lang.String(name)adding index lifecycle policy [{}]-java.lang.String(name)request.getPolicy().getName(),o,o,x
Elasticsearch,1782,org.elasticsearch.xpack.ilm.action.TransportPutLifecycleAction.masterOperation,95,info,java.lang.String(name)updating index lifecycle policy [{}]-java.lang.String(name)request.getPolicy().getName(),o,o,x
Elasticsearch,1756,org.elasticsearch.xpack.ilm.IndexLifecycleRunner.maybeRunAsyncAction,168,trace,java.lang.String(name)[{}] maybe running async action step ({}) with current step {}-java.lang.String(name)index-java.lang.String(name)currentStep.getClass().getSimpleName()-org.elasticsearch.xpack.core.ilm.StepKey(name)currentStep.getKey(),o,o,o
Elasticsearch,1750,org.elasticsearch.xpack.ilm.IndexLifecycleRunner.runPeriodicStep,119,debug,"java.lang.String(name)policy [{}] for index [{}] on an error step, skipping execution-java.lang.String(name)policy-java.lang.String(name)index",o,o,x
Elasticsearch,1751,org.elasticsearch.xpack.ilm.IndexLifecycleRunner.runPeriodicStep,123,trace,java.lang.String(name)[{}] maybe running periodic step ({}) with current step {}-java.lang.String(name)index-java.lang.String(name)currentStep.getClass().getSimpleName()-org.elasticsearch.xpack.core.ilm.StepKey(name)currentStep.getKey(),o,o,o
Elasticsearch,1754,org.elasticsearch.xpack.ilm.IndexLifecycleRunner.runPeriodicStep,151,trace,java.lang.String(name)[{}] ignoring non periodic step execution from step transition [{}]-java.lang.String(name)index-org.elasticsearch.xpack.core.ilm.StepKey(name)currentStep.getKey(),o,o,o
Elasticsearch,1765,org.elasticsearch.xpack.ilm.IndexLifecycleRunner.runPolicyAfterStateChange,236,trace,java.lang.String(name)[{}] ignoring step execution from cluster state change event [{}]-java.lang.String(name)index-org.elasticsearch.xpack.core.ilm.StepKey(name)currentStep.getKey(),o,o,o
Elasticsearch,1773,org.elasticsearch.xpack.ilm.OperationModeUpdateTask.onFailure,83,error,"java.lang.String(name)""unable to update lifecycle metadata with new mode ["" + mode + ""]""-java.lang.Exception(name)e",o,o,x
Elasticsearch,1771,org.elasticsearch.xpack.ilm.PolicyStepsRegistry.update,134,trace,"java.lang.String(name)updating cached steps for [{}] policy, new steps: {}-java.lang.String(name)policyMetadata.getName()-java.util.Set<StepKey>(name)stepMapForPolicy.keySet()",o,o,x
Elasticsearch,1851,org.elasticsearch.xpack.ml.action.TransportDeleteExpiredDataAction.deleteExpiredData,82,info,java.lang.String(name)Completed deletion of expired data,o,o,o
Elasticsearch,1850,org.elasticsearch.xpack.ml.action.TransportDeleteExpiredDataAction.doExecute,52,info,java.lang.String(name)Deleting expired data,o,o,o
Elasticsearch,1819,org.elasticsearch.xpack.ml.action.TransportFinalizeJobExecutionAction.masterOperation,67,debug,java.lang.String(name)finalizing jobs [{}]-java.lang.String(name)jobIdString,o,o,x
Elasticsearch,1843,org.elasticsearch.xpack.ml.action.TransportGetDatafeedsStatsAction.masterOperation,65,debug,java.lang.String(name)Get stats for datafeed '{}'-java.lang.String(name)request.getDatafeedId(),o,o,o
Elasticsearch,1820,org.elasticsearch.xpack.ml.action.TransportGetJobsStatsAction.doExecute,72,debug,java.lang.String(name)Get stats for job [{}]-java.lang.String(name)request.getJobId(),o,o,o
Elasticsearch,1845,org.elasticsearch.xpack.ml.action.TransportKillProcessAction.doExecute,57,debug,java.lang.String(name)[{}] Cannot kill the process because job is not open-java.lang.String(name)request.getJobId(),o,o,o
Elasticsearch,1829,org.elasticsearch.xpack.ml.action.TransportOpenJobAction.makeCurrentlyBeingUpgradedException,590,warn,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,x,x,x
Elasticsearch,1828,org.elasticsearch.xpack.ml.action.TransportOpenJobAction.makeNoSuitableNodesException,582,warn,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,x,x,x
Elasticsearch,1847,org.elasticsearch.xpack.ml.action.TransportSetUpgradeModeAction.isolateDatafeeds,291,info,"java.lang.String(name)""Isolating datafeeds: "" + datafeedsToIsolate.toString()",o,o,o
Elasticsearch,1867,org.elasticsearch.xpack.ml.datafeed.DatafeedJob.run,390,debug,"java.lang.String(name)[{}] Complete iterating data extractor [{}], [{}], [{}], [{}], [{}]-java.lang.String(name)jobId-java.lang.RuntimeException(name)error-long(name)recordCount-java.lang.Long(name)lastEndTimeMs-boolean(name)isRunning()-boolean(name)dataExtractor.isCancelled()",o,o,o
Elasticsearch,1865,org.elasticsearch.xpack.ml.datafeed.DatafeedJob.run,360,trace,java.lang.String(name)[{}] Processed another {} records-java.lang.String(name)jobId-long(name)counts.getProcessedRecordCount(),o,o,o
Elasticsearch,1857,org.elasticsearch.xpack.ml.datafeed.DatafeedJob.runLookBack,134,info,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,x,x,x
Elasticsearch,1889,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.closeJob,413,debug,java.lang.String(name)[{}] No need to auto-close job as job state is [{}]-java.lang.String(name)getJobId()-org.elasticsearch.xpack.core.ml.job.config.JobState(name)jobState,o,o,o
Elasticsearch,1891,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.closeJob,453,debug,java.lang.String(name)[{}] {}-java.lang.String(name)getJobId()-java.lang.String(name)e.getMessage(),o,x,x
Elasticsearch,1895,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.clusterChanged,520,warn,java.lang.String(name)Datafeed [{}] is stopping because job [{}] state is [{}]-java.lang.String(name)datafeedTask.getDatafeedId()-java.lang.String(name)getJobId(datafeedTask)-org.elasticsearch.xpack.core.ml.job.config.JobState(name)jobState,o,o,o
Elasticsearch,1879,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.innerRun,192,warn,java.lang.String(name)[{}] {}-java.lang.String(name)holder.datafeedJob.getJobId()-java.lang.String(name)lookbackNoDataMsg,o,x,x
Elasticsearch,1894,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.runWhenJobIsOpened,479,info,java.lang.String(name)Datafeed [{}] is waiting for job [{}] to be opened-java.lang.String(name)datafeedTask.getDatafeedId()-java.lang.String(name)getJobId(datafeedTask),o,o,o
Elasticsearch,1887,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.stop,353,info,"java.lang.String(name)[{}] datafeed [{}] for job [{}] has been stopped{}-java.lang.String(name)source-java.lang.String(name)datafeedId-java.lang.String(name)datafeedJob.getJobId()-java.lang.Object(name)acquired ? """" : "", but there may be pending tasks as the timeout ["" + timeout.getStringRep() + ""] expired""",o,o,o
Elasticsearch,1888,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.stop,363,info,java.lang.String(name)[{}] datafeed [{}] for job [{}] was already stopped-java.lang.String(name)source-java.lang.String(name)datafeedId-java.lang.String(name)datafeedJob.getJobId(),o,o,o
Elasticsearch,1877,org.elasticsearch.xpack.ml.datafeed.DatafeedManager.stopAllDatafeedsOnThisNode,121,info,"java.lang.String(name)Closing [{}] datafeeds, because [{}]-int(name)numDatafeeds-java.lang.String(name)reason",o,o,o
Elasticsearch,1899,org.elasticsearch.xpack.ml.datafeed.extractor.aggregation.AbstractAggregationDataExtractor.search,113,debug,java.lang.String(name)[{}] Search response was obtained-java.lang.String(name)context.jobId,o,o,o
Elasticsearch,1910,org.elasticsearch.xpack.ml.datafeed.extractor.scroll.ScrollDataExtractor.continueScroll,201,debug,java.lang.String(name)[{}] Search response was obtained-java.lang.String(name)context.jobId,o,o,o
Elasticsearch,1906,org.elasticsearch.xpack.ml.datafeed.extractor.scroll.ScrollDataExtractor.initScroll,114,debug,java.lang.String(name)[{}] Search response was obtained-java.lang.String(name)context.jobId,o,o,o
Elasticsearch,1916,org.elasticsearch.xpack.ml.dataframe.extractor.DataFrameDataExtractor.cancel,85,debug,java.lang.String(name)[{}] Data extractor was cancelled-java.lang.String(name)context.jobId,o,o,o
Elasticsearch,1918,org.elasticsearch.xpack.ml.dataframe.extractor.DataFrameDataExtractor.tryRequestWithSearchResponse,110,debug,java.lang.String(name)[{}] Search response was obtained-java.lang.String(name)context.jobId,o,o,o
Elasticsearch,1911,org.elasticsearch.xpack.ml.dataframe.extractor.ExtractedFieldsDetector.hasCompatibleType,139,debug,java.lang.String(name)[{}] incompatible field [{}] because it is missing from mappings-java.lang.String(name)config.getId()-java.lang.String(name)field,o,o,o
Elasticsearch,1924,org.elasticsearch.xpack.ml.dataframe.process.AnalyticsProcessManager.processData,114,debug,java.lang.String(name)Removed process context for task [{}]; [{}] processes still running-java.lang.String(name)config.getId()-int(name)processContextByAllocation.size(),o,o,o
Elasticsearch,1926,org.elasticsearch.xpack.ml.dataframe.process.AnalyticsProcessManager.processData,122,error,java.lang.String(name)[{}] Marking task failed; {}-java.lang.String(name)config.getId()-java.lang.String(name)processContext.getFailureReason(),o,o,o
Elasticsearch,1923,org.elasticsearch.xpack.ml.dataframe.process.AnalyticsProcessManager.processData,106,info,java.lang.String(name)[{}] Result processor has completed-java.lang.String(name)config.getId(),o,o,o
Elasticsearch,1932,org.elasticsearch.xpack.ml.dataframe.process.AnalyticsProcessManager.stop,268,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] Failed to kill process"",id)-java.io.IOException(name)e",o,o,o
Elasticsearch,1942,org.elasticsearch.xpack.ml.dataframe.process.DataFrameRowsJoiner.close,129,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] Failed to join results"",analyticsId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,1941,org.elasticsearch.xpack.ml.dataframe.process.DataFrameRowsJoiner.processRowResults,70,error,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] Failed to join results "",analyticsId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,1964,org.elasticsearch.xpack.ml.job.JobNodeSelector.calculateCurrentLoadForNode,255,debug,java.lang.String(name)Falling back to allocating job [{}] by job counts because the memory requirement for job [{}] was not available-java.lang.String(name)jobId-java.lang.String(name)params.getJobId(),o,o,o
Elasticsearch,1965,org.elasticsearch.xpack.ml.job.JobNodeSelector.calculateCurrentLoadForNode,258,debug,"java.lang.String(name)""adding "" + jobMemoryRequirement",x,x,x
Elasticsearch,1960,org.elasticsearch.xpack.ml.job.JobNodeSelector.selectNode,192,debug,java.lang.String(name)Falling back to allocating job [{}] by job counts because its memory requirement was not available-java.lang.String(name)jobId,o,o,o
Elasticsearch,1992,org.elasticsearch.xpack.ml.job.persistence.JobDataCountsPersister.persistDataCounts,71,warn,org.apache.logging.log4j.util.Supplier<>(name)(Supplier<?>)null-java.io.IOException(name)ioe,x,x,x
Elasticsearch,1988,org.elasticsearch.xpack.ml.job.persistence.JobResultsPersister.persistDatafeedTimingStats,346,trace,java.lang.String(name)[{}] Persisting datafeed timing stats-java.lang.String(name)jobId,o,o,o
Elasticsearch,1971,org.elasticsearch.xpack.ml.job.persistence.StateStreamer.restoreStateToStream,72,trace,java.lang.String(name)ES API CALL: get ID {} from index {}-java.lang.String(name)stateDocId-java.lang.String(name)indexName,o,o,o
Elasticsearch,2018,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.closeAllJobsOnThisNode,146,info,"java.lang.String(name)Closing [{}] jobs, because [{}]-int(name)numJobs-java.lang.String(name)reason",o,o,o
Elasticsearch,2035,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.closeJob,605,debug,java.lang.String(name)Cannot close job [{}] as it has been marked as dying-java.lang.String(name)jobId,o,o,o
Elasticsearch,2037,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.closeJob,614,info,"java.lang.String(name)Closing job [{}], because [{}]-java.lang.String(name)jobId-java.lang.String(name)reason",o,o,o
Elasticsearch,2036,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.closeJob,612,info,java.lang.String(name)Closing job [{}]-java.lang.String(name)jobId,o,o,o
Elasticsearch,2029,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.create,492,warn,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,o,x,x
Elasticsearch,2030,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.create,497,warn,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,o,x,x
Elasticsearch,2028,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.createProcessAndSetRunning,458,debug,java.lang.String(name)Cannot open job [{}] when its state is [{}]-java.lang.String(name)job.getId()-java.lang.String(name)processContext.getState().getClass().getName(),o,o,o
Elasticsearch,2022,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.flushJob,248,debug,java.lang.String(name)Flushing job {}-java.lang.String(name)jobTask.getJobId(),o,o,o
Elasticsearch,2024,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.forecastJob,277,debug,java.lang.String(name)Forecasting job {}-java.lang.String(name)jobId,o,o,o
Elasticsearch,2032,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.notifyLoadingSnapshot,561,info,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,o,x,x
Elasticsearch,2027,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.openJob,376,info,java.lang.String(name)Opening job [{}]-java.lang.String(name)jobId,o,o,o
Elasticsearch,2043,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectProcessManager.setJobState,713,warn,java.lang.String(name)Error while delegating response-java.io.IOException(name)e1,o,o,o
Elasticsearch,2015,org.elasticsearch.xpack.ml.job.process.autodetect.AutodetectWorkerExecutorService.start,96,error,java.lang.String(name)error handling job operation-java.lang.Exception(name)e,o,o,x
Elasticsearch,2048,org.elasticsearch.xpack.ml.job.process.autodetect.output.AutodetectResultProcessor.process,141,warn,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] Error persisting autodetect results"",jobId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,2058,org.elasticsearch.xpack.ml.job.process.autodetect.output.AutodetectResultProcessor.processResult,281,debug,java.lang.String(name)[{}] Quantiles queued for renormalization-java.lang.String(name)jobId,o,o,o
Elasticsearch,2056,org.elasticsearch.xpack.ml.job.process.autodetect.output.AutodetectResultProcessor.processResult,243,trace,java.lang.String(name)Received Forecast Stats [{}]-java.lang.String(name)forecastRequestStats.getId(),o,o,o
Elasticsearch,2054,org.elasticsearch.xpack.ml.job.process.autodetect.output.AutodetectResultProcessor.readResults,186,warn,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] Error processing autodetect result"",jobId)-java.lang.Exception(name)e",o,o,o
Elasticsearch,2065,org.elasticsearch.xpack.ml.job.process.autodetect.output.AutodetectStateProcessor.persist,101,trace,java.lang.String(name)[{}] Persisting job state document-java.lang.String(name)jobId,o,o,o
Elasticsearch,2005,org.elasticsearch.xpack.ml.job.process.autodetect.ProcessContext.kill,128,info,"java.lang.String(name)Killing job [{}]{}, because [{}]-java.lang.String(name)jobId-java.lang.String(name)extraInfo-java.lang.String(name)reason",o,o,o
Elasticsearch,2066,org.elasticsearch.xpack.ml.job.process.autodetect.writer.FieldConfigWriter.write,75,debug,"java.lang.String(name)""FieldConfig:\n"" + contents.toString()",x,x,x
Elasticsearch,2087,org.elasticsearch.xpack.ml.job.process.normalizer.Normalizer.mergeRecursively,153,error,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,o,x,x
Elasticsearch,2093,org.elasticsearch.xpack.ml.job.retention.ExpiredResultsRemover.auditResultsWereDeleted,106,debug,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,o,x,x
Elasticsearch,1951,org.elasticsearch.xpack.ml.job.UpdateJobProcessNotifier.executeProcessUpdates,142,debug,java.lang.String(name)Remote job [{}] not updated as it is no longer open-java.lang.String(name)update.getJobId(),o,o,o
Elasticsearch,1950,org.elasticsearch.xpack.ml.job.UpdateJobProcessNotifier.executeProcessUpdates,139,debug,java.lang.String(name)Remote job [{}] not updated as it has been deleted-java.lang.String(name)update.getJobId(),o,o,o
Elasticsearch,1948,org.elasticsearch.xpack.ml.job.UpdateJobProcessNotifier.executeProcessUpdates,126,info,java.lang.String(name)Successfully updated remote job [{}]-java.lang.String(name)update.getJobId(),o,o,o
Elasticsearch,1813,org.elasticsearch.xpack.ml.MlAssignmentNotifier.auditChangesToMlTasks,106,warn,java.lang.String(name)[{}] {}-java.lang.String(name)jobId-java.lang.String(name)msg,x,x,x
Elasticsearch,1799,org.elasticsearch.xpack.ml.MlConfigMigrator.removeFromClusterState,269,info,java.lang.String(name)ml job configurations migrated: {}-java.util.List<String>(name)removedConfigs.get().removedJobIds,o,o,x
Elasticsearch,1800,org.elasticsearch.xpack.ml.MlConfigMigrator.removeFromClusterState,272,info,java.lang.String(name)ml datafeed configurations migrated: {}-java.util.List<String>(name)removedConfigs.get().removedDatafeedIds,o,o,x
Elasticsearch,1795,org.elasticsearch.xpack.ml.MlDailyMaintenanceService.start,83,debug,java.lang.String(name)Starting ML daily maintenance service,o,o,o
Elasticsearch,1796,org.elasticsearch.xpack.ml.MlDailyMaintenanceService.stop,88,debug,java.lang.String(name)Stopping ML daily maintenance service,o,o,o
Elasticsearch,1798,org.elasticsearch.xpack.ml.MlDailyMaintenanceService.triggerTasks,116,info,java.lang.String(name)triggering scheduled [ML] maintenance tasks,o,o,x
Elasticsearch,2112,org.elasticsearch.xpack.ml.process.NativeController.killProcess,146,debug,"java.lang.String(name)""Killing process with PID: "" + pid",o,o,o
Elasticsearch,2110,org.elasticsearch.xpack.ml.process.NativeController.startProcess,119,debug,"java.lang.String(name)""Starting process with command: "" + command",o,o,o
Elasticsearch,2191,org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter.doClose,160,trace,java.lang.String(name)stopped,x,x,x
Elasticsearch,2193,org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter.resolveBulk,198,debug,java.lang.String(name)started,x,x,x
Elasticsearch,2129,org.elasticsearch.xpack.monitoring.MonitoringService.doClose,173,debug,java.lang.String(name)monitoring service is closing,o,o,x
Elasticsearch,2131,org.elasticsearch.xpack.monitoring.MonitoringService.doRun,217,debug,java.lang.String(name)monitoring execution is skipped,o,o,x
Elasticsearch,2124,org.elasticsearch.xpack.monitoring.MonitoringService.doStart,151,debug,java.lang.String(name)monitoring service is starting,o,o,x
Elasticsearch,2127,org.elasticsearch.xpack.monitoring.MonitoringService.doStop,165,debug,java.lang.String(name)monitoring service is stopping,o,o,x
Elasticsearch,2244,org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply,76,error,"java.lang.String(name)blocking [{}] operation due to expired license. Cluster health, cluster stats and indices stats operations are blocked on license expiration. All data operations (read and write) continue to work. If you have a new license, please update it. Otherwise, please reach out to your support contact.-java.lang.String(name)action",o,o,x
Elasticsearch,2304,org.elasticsearch.xpack.security.authc.AuthenticationService.extractToken,341,warn,java.lang.String(name)An exception occurred while attempting to find authentication credentials-java.lang.Exception(name)e,o,o,x
Elasticsearch,2328,org.elasticsearch.xpack.security.authc.file.FileUserPasswdStore.parseFile,123,trace,java.lang.String(name)reading users file [{}]...-java.nio.file.Path(name)path.toAbsolutePath(),o,o,x
Elasticsearch,2618,org.elasticsearch.xpack.security.authc.kerberos.SimpleKdcLdapServer.SimpleKdcLdapServer,99,info,java.lang.String(name)SimpleKdcLdapServer started.,o,o,x
Elasticsearch,2620,org.elasticsearch.xpack.security.authc.kerberos.SimpleKdcLdapServer.stop,246,info,java.lang.String(name)SimpleKdcServer stoppped.,o,o,x
Elasticsearch,2349,org.elasticsearch.xpack.security.authc.ldap.PoolingSessionFactory.createConnectionPool,164,warn,"org.apache.logging.log4j.message.ParameterizedMessage(name)new ParameterizedMessage(""[{}] and [{}} have not been specified or are not valid distinguished names,"" + ""so connection health checking is disabled"",RealmSettings.getFullSettingKey(config,PoolingSessionFactorySettings.BIND_DN),RealmSettings.getFullSettingKey(config,PoolingSessionFactorySettings.HEALTH_CHECK_DN))",o,o,o
Elasticsearch,2400,org.elasticsearch.xpack.security.authc.saml.SamlAuthenticator.authenticateResponse,129,debug,java.lang.String(name)The Attribute Statements of SAML Response with ID {} contained no attributes and the SAML Assertion Subject didnot contain a SAML NameID. Please verify that the Identity Provider configuration with regards to attribute release is correct. -java.lang.String(name)response.getID(),x,o,x
Elasticsearch,2412,org.elasticsearch.xpack.security.authc.saml.SamlRequestHandler.samlSignatureException,184,warn,java.lang.String(name)The XML Signature of this SAML message cannot be validated. Please verify that the saml realm uses the correct SAMLmetadata file/URL for this Identity Provider,o,o,o
Elasticsearch,2425,org.elasticsearch.xpack.security.authc.support.DnRoleMapper.parseFile,145,error,"java.lang.String(name)message.getFormattedMessage() + "" skipping...""-com.unboundid.ldap.sdk.LDAPException(name)e",x,x,x
Elasticsearch,2295,org.elasticsearch.xpack.security.authc.TokenService.checkClientCanRefresh,1151,warn,java.lang.String(name)Token was originally created by [{}] but [{}] attempted to refresh it-java.lang.String(name)refreshToken.getAssociatedUser()-java.lang.String(name)clientAuthentication.getUser().principal(),o,o,o
Elasticsearch,2275,org.elasticsearch.xpack.security.authc.TokenService.getUserTokenFromId,418,warn,java.lang.String(name)failed to get access token [{}] because index [{}] is not available-java.lang.String(name)userTokenId-java.lang.String(name)tokensIndex.aliasName(),o,o,x
Elasticsearch,2292,org.elasticsearch.xpack.security.authc.TokenService.innerRefresh,956,warn,java.lang.String(name)could not encrypt access token and refresh token string-java.security.GeneralSecurityException(name)e,o,o,x
Elasticsearch,2280,org.elasticsearch.xpack.security.authc.TokenService.invalidateAccessToken,565,trace,java.lang.String(name)No access token provided,o,o,o
Elasticsearch,2281,org.elasticsearch.xpack.security.authc.TokenService.invalidateRefreshToken,584,trace,java.lang.String(name)No refresh token provided,o,o,o
Elasticsearch,2439,org.elasticsearch.xpack.security.authz.AuthorizationService.denialException,580,debug,java.lang.String(name)action [{}] is unauthorized for API key id [{}] of user [{}]-java.lang.String(name)action-java.lang.String(name)apiKeyId-java.lang.String(name)authUser.principal(),o,o,x
Elasticsearch,2468,org.elasticsearch.xpack.security.authz.store.FileRolesStore.checkDescriptor,304,error,java.lang.String(name)invalid role definition [{}] in roles file [{}]. document and field level security is not enabled. set [{}] to [true] in the configuration file. skipping role...-java.lang.String(name)roleName-java.nio.file.Path(name)path.toAbsolutePath()-java.lang.String(name)XPackSettings.DLS_FLS_ENABLED.getKey(),o,o,x
Elasticsearch,2490,org.elasticsearch.xpack.security.rest.action.oidc.RestOpenIdConnectAuthenticateAction.innerPrepareRequest,53,trace,"java.lang.String(name)""OIDC Authenticate: "" + authenticateRequest",o,x,o
Elasticsearch,2510,org.elasticsearch.xpack.security.transport.SecurityHttpExceptionHandler.accept,57,warn,"java.lang.String(name)http client did not trust this server's certificate, closing connection {}-org.elasticsearch.http.HttpChannel(name)channel",o,o,x
Elasticsearch,1791,org.elasticsearch.xpack.slm.action.TransportPutSnapshotLifecycleAction.masterOperation,96,info,java.lang.String(name)adding new snapshot lifecycle [{}]-java.lang.String(name)id,o,o,x
Elasticsearch,1792,org.elasticsearch.xpack.slm.action.TransportPutSnapshotLifecycleAction.masterOperation,109,info,java.lang.String(name)adding new snapshot lifecycle [{}]-java.lang.String(name)id,o,o,x
Elasticsearch,1793,org.elasticsearch.xpack.slm.action.TransportPutSnapshotLifecycleAction.masterOperation,111,info,java.lang.String(name)updating existing snapshot lifecycle [{}]-java.lang.String(name)id,o,o,x
Elasticsearch,1784,org.elasticsearch.xpack.slm.SnapshotLifecycleService.cancelSnapshotJobs,191,trace,java.lang.String(name)cancelling all snapshot lifecycle jobs,o,o,x
Elasticsearch,2516,org.elasticsearch.xpack.sql.analysis.analyzer.Analyzer.rule,372,trace,java.lang.String(name)Attempting to resolve {}-java.lang.String(name)plan.nodeString(),o,o,o
Elasticsearch,2522,org.elasticsearch.xpack.sql.parser.SqlParser.createExpression,81,debug,java.lang.String(name)Parsing as expression: {}-java.lang.String(name)expression,o,o,o
Elasticsearch,2521,org.elasticsearch.xpack.sql.parser.SqlParser.createStatement,64,debug,java.lang.String(name)Parsing as statement: {}-java.lang.String(name)sql,o,o,o
Elasticsearch,2524,org.elasticsearch.xpack.sql.parser.SqlParser.invokeParser,126,info,"java.lang.String(name)""Parse tree {} "" + tree.toStringTree()",o,x,o
Elasticsearch,2523,org.elasticsearch.xpack.sql.parser.SqlParser.invokeParser,117,info,"java.lang.String(name)format(Locale.ROOT,"" %-15s '%s'"",symbolicName == null ? literalName : symbolicName,t.getText())",x,x,x
Elasticsearch,2530,org.elasticsearch.xpack.sql.qa.cli.EmbeddedCli.EmbeddedCli,129,info,java.lang.String(name)out: {}-java.lang.String(name)security.keystorePassword,o,x,x
Elasticsearch,2533,org.elasticsearch.xpack.sql.qa.cli.EmbeddedCli.readLine,293,info,java.lang.String(name)in : {}-java.lang.String(name)line,o,x,x
Elasticsearch,2536,org.elasticsearch.xpack.sql.qa.jdbc.DataLoader.main,38,info,java.lang.String(name)Data loaded,o,x,o
Elasticsearch,2529,org.elasticsearch.xpack.sql.rule.RuleExecutor.executeWithInfo,192,debug,"java.lang.String(name)Tree transformation took {} {}-org.elasticsearch.common.unit.TimeValue(name)TimeValue.timeValueMillis(totalDuration)-java.lang.String(name)NodeUtils.diffString(plan,currentPlan)",x,o,o
Elasticsearch,2528,org.elasticsearch.xpack.sql.rule.RuleExecutor.executeWithInfo,186,trace,"java.lang.String(name)Batch {} applied took {} {}-java.lang.String(name)batch.name-org.elasticsearch.common.unit.TimeValue(name)TimeValue.timeValueMillis(batchDuration)-java.lang.String(name)NodeUtils.diffString(before,after)",x,o,o
Elasticsearch,2581,org.elasticsearch.xpack.watcher.execution.ExecutionService.execute,351,debug,java.lang.String(name)finished [{}]/[{}]-java.lang.String(name)watchId-org.elasticsearch.xpack.core.watcher.execution.Wid(name)ctx.id(),o,x,x
Elasticsearch,2588,org.elasticsearch.xpack.watcher.execution.ExecutionService.executeTriggeredWatches,561,debug,java.lang.String(name)triggered execution of [{}] watches-int(name)counter,o,o,x
Elasticsearch,2584,org.elasticsearch.xpack.watcher.execution.ExecutionService.forcePutHistory,459,debug,java.lang.String(name)indexed watch history record [{}]-java.lang.String(name)watchRecord.id().value(),o,o,x
Elasticsearch,2585,org.elasticsearch.xpack.watcher.execution.ExecutionService.forcePutHistory,470,debug,java.lang.String(name)overwrote watch history record [{}]-java.lang.String(name)watchRecord.id().value(),o,o,x
Elasticsearch,2583,org.elasticsearch.xpack.watcher.execution.ExecutionService.logWatchRecord,407,warn,java.lang.String(name)failed to execute watch [{}]-java.lang.String(name)ctx.id().watchId(),o,o,x
Elasticsearch,2596,org.elasticsearch.xpack.watcher.input.search.ExecutableSearchInput.doExecute,85,debug,"java.lang.String(name)[{}] hit [{}]-org.elasticsearch.xpack.core.watcher.execution.Wid(name)ctx.id()-java.util.Map<String,Object>(name)hit.getSourceAsMap()",o,x,o
Elasticsearch,2612,org.elasticsearch.xpack.watcher.notification.email.support.EmailServer.EmailServer,62,error,java.lang.String(name)Unexpected failure-java.lang.Exception(name)e,o,x,o
Elasticsearch,2615,org.elasticsearch.xpack.watcher.trigger.ScheduleTriggerEngineMock.trigger,98,debug,java.lang.String(name)firing watch [{}] at [{}]-java.lang.String(name)jobName-java.time.ZonedDateTime(name)now,o,o,x
Elasticsearch,2614,org.elasticsearch.xpack.watcher.trigger.ScheduleTriggerEngineMock.trigger,92,info,java.lang.String(name)not executing watch [{}] on this scheduler because it is paused-java.lang.String(name)jobName,o,o,x
Elasticsearch,2611,org.elasticsearch.xpack.watcher.watch.WatchParser.parse,116,trace,java.lang.String(name)parsing watch [{}] -java.lang.String(name)source.utf8ToString(),o,x,x
Elasticsearch,2562,org.elasticsearch.xpack.watcher.WatcherIndexingListener.clusterChanged,212,error,java.lang.String(name)error loading watches index: [{}]-java.lang.String(name)e.getMessage(),o,o,x
Elasticsearch,2557,org.elasticsearch.xpack.watcher.WatcherService.loadWatches,371,debug,java.lang.String(name)Loaded [{}] watches for execution-int(name)watches.size(),o,o,o
Elasticsearch,2552,org.elasticsearch.xpack.watcher.WatcherService.reloadInner,229,debug,"java.lang.String(name)watch service has not been reloaded for state [{}], another reload for state [{}] in progress-long(name)state.getVersion()-long(name)processedClusterStateVersion.get()",o,o,o
Elasticsearch,2554,org.elasticsearch.xpack.watcher.WatcherService.reloadInner,253,debug,"java.lang.String(name)watch service has not been reloaded for state [{}], another reload for state [{}] in progress-long(name)state.getVersion()-long(name)processedClusterStateVersion.get()",o,o,o
Elasticsearch,2549,org.elasticsearch.xpack.watcher.WatcherService.stop,155,info,"java.lang.String(name)stopping watch service, reason [{}]-java.lang.String(name)reason",o,o,x
Elasticsearch,2548,org.elasticsearch.xpack.watcher.WatcherService.validate,142,debug,java.lang.String(name)error validating to start watcher-java.lang.IllegalStateException(name)e,o,o,x
Flink,2197,org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression,938,trace,java.lang.String(name)After validation: {}-org.apache.calcite.sql.SqlNode(name)outermostNode,o,x,o
Flink,505,org.apache.flink.addons.hbase.HBaseLookupFunction.close,143,info,java.lang.String(name)end close.,o,x,x
Flink,501,org.apache.flink.addons.hbase.HBaseLookupFunction.open,119,info,java.lang.String(name)end open.,o,x,x
Flink,512,org.apache.flink.addons.hbase.HBaseUpsertSinkFunction.open,156,info,java.lang.String(name)end open.,o,x,x
Flink,369,org.apache.flink.api.common.io.BinaryInputFormat.createInputSplits,134,warn,"java.lang.String(name)String.format(""With the given block size %d, the files %s cannot be split into %d blocks. Filling up with empty splits..."",blockSize,Arrays.toString(getFilePaths()),minNumSplits)",o,o,o
Flink,373,org.apache.flink.api.common.state.StateDescriptor.initializeSerializerUnlessSet,307,debug,java.lang.String(name)Someone else beat us at initializing the serializer.,o,x,x
Flink,470,org.apache.flink.api.java.hadoop.mapred.HadoopInputFormatBase.getStatistics,133,warn,"java.lang.String(name)""Could not determine statistics due to an io error: "" + ioex.getMessage()",o,o,o
Flink,384,org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo,1909,info,"java.lang.String(name)""The default constructor of "" + clazz + "" is not Public so it cannot be used as a POJO type ""+ ""and must be processed as GenericType. Please read the Flink documentation ""+ ""on \""Data Types & Serialization\"" for details of the effect on performance.""",o,o,o
Flink,379,org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo,1847,info,"java.lang.String(name)""No fields were detected for "" + clazz + "" so it cannot be used as a POJO type ""+ ""and must be processed as GenericType. Please read the Flink documentation ""+ ""on \""Data Types & Serialization\"" for details of the effect on performance.""",o,o,o
Flink,383,org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo,1902,info,"java.lang.String(name)clazz + "" is missing a default constructor so it cannot be used as a POJO type "" + ""and must be processed as GenericType. Please read the Flink documentation ""+ ""on \""Data Types & Serialization\"" for details of the effect on performance.""",o,o,x
Flink,385,org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject,2026,warn,"java.lang.String(name)""Cannot extract type of Row field, because of Row field["" + i + ""] is null. ""+ ""Should define RowTypeInfo explicitly.""",o,o,o
Flink,21,org.apache.flink.client.cli.CliFrontend.handleParametrizationException,852,error,java.lang.String(name)Program has not been parametrized properly.-org.apache.flink.client.program.ProgramParametrizationException(name)e,o,o,o
Flink,8,org.apache.flink.client.cli.CliFrontend.runProgram,267,debug,java.lang.String(name)User parallelism is set to {}-int(name)userParallelism,o,o,o
Flink,7,org.apache.flink.client.cli.CliFrontend.runProgram,264,debug,java.lang.String(name){}-org.apache.flink.runtime.jobgraph.SavepointRestoreSettings(name)runOptions.getSavepointRestoreSettings(),x,x,x
Flink,34,org.apache.flink.client.program.rest.RestClusterClient.close,210,error,java.lang.String(name)Error while closing the Cluster Client-java.lang.Exception(name)e,o,o,o
Flink,708,org.apache.flink.client.python.PythonDriver.main,55,error,java.lang.String(name)Could not parse command line arguments {}.-String[](name)args-java.lang.Exception(name)e,o,o,o
Flink,411,org.apache.flink.configuration.Configuration.convertToDouble,942,warn,java.lang.String(name)Configuration cannot evaluate value {} as a double value-java.lang.Object(name)o,o,o,o
Flink,410,org.apache.flink.configuration.Configuration.convertToFloat,924,warn,java.lang.String(name)Configuration cannot evaluate value {} as a float value-java.lang.Object(name)o,o,o,o
Flink,407,org.apache.flink.configuration.Configuration.convertToInt,871,warn,java.lang.String(name)Configuration cannot evaluate value {} as an integer number-java.lang.Object(name)o,o,o,o
Flink,1997,org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.cleanUpPathQuietly,248,warn,"java.lang.String(name)""Failed to clean up path "" + path-java.io.IOException(name)ex",o,o,o
Flink,1990,org.apache.flink.contrib.streaming.state.RocksDBMapState.clear,262,warn,java.lang.String(name)Error while cleaning the state.-java.lang.Exception(name)e,o,o,o
Flink,1980,org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configureOptionsFactory,550,info,java.lang.String(name)Using application-defined options factory: {}.-org.apache.flink.contrib.streaming.state.OptionsFactory(name)originalOptionsFactory,o,o,o
Flink,1981,org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configureOptionsFactory,559,info,java.lang.String(name)Using default options factory: {}.-org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory(name)optionsFactory,o,o,o
Flink,1979,org.apache.flink.contrib.streaming.state.RocksDBStateBackend.lazyInitializeForJob,418,error,java.lang.String(name)msg,x,x,x
Flink,2000,org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.snapshotMetaData,234,trace,java.lang.String(name)Taking incremental snapshot for checkpoint {}. Snapshot is based on last completed checkpoint {} assuming the following (shared) files as base: {}.-long(name)checkpointId-long(name)lastCompletedCheckpoint-java.util.Set<StateHandleID>(name)baseSstFiles,o,o,o
Flink,424,org.apache.flink.core.fs.FileSystem.loadHadoopFsFactory,1068,info,java.lang.String(name)Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.,o,o,o
Flink,441,org.apache.flink.docs.rest.RestAPIDocGenerator.createMessageHtmlEntry,243,error,java.lang.String(name)Failed to generate message schema for class {}.-java.lang.Class<>(name)messageClass-org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonProcessingException(name)e,o,o,o
Flink,442,org.apache.flink.docs.rest.RestAPIDocGenerator.createMessageHtmlEntry,255,error,java.lang.String(name)Failed to write message schema for class {}.-java.lang.String(name)messageClass.getCanonicalName()-org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonProcessingException(name)e,o,o,o
Flink,624,org.apache.flink.dropwizard.ScheduledDropwizardReporter.notifyOfAddedMetric,155,warn,java.lang.String(name)Cannot add metric of type {}. This indicates that the reporter does not support this metric type.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,446,org.apache.flink.examples.java.distcp.DistCp.main,148,info,java.lang.String(name)== COUNTERS ==,x,x,x
Flink,447,org.apache.flink.examples.java.distcp.DistCp.main,150,info,"java.lang.String(name)e.getKey() + "": "" + e.getValue()",x,x,x
Flink,705,org.apache.flink.formats.parquet.utils.ParquetRecordReader.readNextRecord,259,debug,java.lang.String(name)read value: {}-T(name)readRecord,o,x,x
Flink,2166,org.apache.flink.fs.openstackhadoop.SwiftFileSystemFactory.create,65,debug,java.lang.String(name)Creating swift file system (backed by a Hadoop native swift file system),o,o,o
Flink,2169,org.apache.flink.fs.openstackhadoop.SwiftFileSystemFactory.create,98,warn,java.lang.String(name)The factory has not been configured prior to loading the Swift native file system. Using Hadoop configuration from the classpath.,o,o,o
Flink,1960,org.apache.flink.fs.s3.common.writer.S3Committer.commitAfterRecovery,97,warn,java.lang.String(name)message,x,x,x
Flink,556,org.apache.flink.mesos.entrypoint.MesosEntrypointUtils.createTmParameters,111,info,"java.lang.String(name)TaskManagers will be started with container size {} MB, JVM heap size {} MB, JVM direct memory limit {} MB, {} cpus, {} gpus, disk space {} MB-long(name)taskManagerParameters.containeredParameters().taskManagerTotalMemoryMB()-long(name)taskManagerParameters.containeredParameters().taskManagerHeapSizeMB()-long(name)taskManagerParameters.containeredParameters().taskManagerDirectMemoryLimitMB()-double(name)taskManagerParameters.cpus()-int(name)taskManagerParameters.gpus()-int(name)taskManagerParameters.disk()",o,o,o
Flink,565,org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.internalDeregisterApplication,410,info,java.lang.String(name)Shutting down and unregistering as a Mesos framework.,o,o,o
Flink,567,org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.startNewWorker,448,info,"java.lang.String(name)Scheduling Mesos task {} with ({} MB, {} cpus).-java.lang.String(name)launchable.taskID().getValue()-double(name)launchable.taskRequest().getMemory()-double(name)launchable.taskRequest().getCPUs()",o,o,o
Flink,568,org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.stopWorker,466,info,java.lang.String(name)Stopping worker {}.-org.apache.flink.runtime.clusterframework.types.ResourceID(name)workerNode.getResourceID(),o,o,o
Flink,574,org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.taskTerminated,665,info,"java.lang.String(name)Worker {} failed with status: {}, reason: {}, message: {}.-org.apache.flink.runtime.clusterframework.types.ResourceID(name)id-org.apache.mesos.TaskState(name)status.getState()-org.apache.mesos.Reason(name)status.getReason()-java.lang.String(name)status.getMessage()",o,o,o
Flink,583,org.apache.flink.mesos.util.MesosArtifactServer.MesosArtifactServer,169,info,"java.lang.String(name)Mesos Artifact Server Base URL: {}, listening at {}:{}-java.net.URL(name)baseURL-java.lang.String(name)address-int(name)port",o,o,o
Flink,598,org.apache.flink.mesos.util.MesosConfiguration.logMesosConfig,145,info,"java.lang.String(name) ID: {}-java.lang.Object(name)info.hasId() ? info.getId().getValue() : ""(none)""",o,x,o
Flink,599,org.apache.flink.mesos.util.MesosConfiguration.logMesosConfig,146,info,"java.lang.String(name) Name: {}-java.lang.Object(name)info.hasName() ? info.getName() : ""(none)""",o,x,o
Flink,601,org.apache.flink.mesos.util.MesosConfiguration.logMesosConfig,148,info,"java.lang.String(name) Role: {}-java.lang.Object(name)info.hasRole() ? info.getRole() : ""(none)""",o,x,o
Flink,608,org.apache.flink.mesos.util.MesosConfiguration.logMesosConfig,161,info,java.lang.String(name)--------------------------------------------------------------------------------,x,x,x
Flink,588,org.apache.flink.mesos.util.MesosResourceAllocation.takeScalar,88,debug,java.lang.String(name)Allocating {} {}-double(name)amount-java.lang.String(name)resourceName,x,x,o
Flink,612,org.apache.flink.metrics.datadog.DatadogHttpReporter.notifyOfAddedMetric,84,warn,java.lang.String(name)Cannot add unknown metric type {}. This indicates that the reporter does not support this metric type.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,613,org.apache.flink.metrics.datadog.DatadogHttpReporter.notifyOfRemovedMetric,100,warn,java.lang.String(name)Cannot remove unknown metric type {}. This indicates that the reporter does not support this metric type.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,616,org.apache.flink.metrics.datadog.DatadogHttpReporter.report,136,info,java.lang.String(name)The metric {} will not be reported because only number types are supported by this reporter.-java.lang.String(name)g.getMetric(),o,o,o
Flink,636,org.apache.flink.metrics.jmx.JMXReporter.notifyOfAddedMetric,186,error,java.lang.String(name)Cannot add unknown metric type: {}. This indicates that the metric type is not supported by this reporter.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,643,org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.createCollector,149,warn,java.lang.String(name)Cannot create collector for unknown metric type: {}. This indicates that the metric type is not supported by this reporter.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,648,org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.gaugeFrom,239,debug,"java.lang.String(name)Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.-org.apache.flink.metrics.Gauge(name)gauge-java.lang.String(name)value.getClass().getName()",o,o,o
Flink,645,org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.removeMetric,181,warn,java.lang.String(name)Cannot remove unknown metric type: {}. This indicates that the metric type is not supported by this reporter.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,609,org.apache.flink.metrics.reporter.AbstractReporter.notifyOfAddedMetric,60,warn,java.lang.String(name)Cannot add unknown metric type {}. This indicates that the reporter does not support this metric type.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,610,org.apache.flink.metrics.reporter.AbstractReporter.notifyOfRemovedMetric,78,warn,java.lang.String(name)Cannot remove unknown metric type {}. This indicates that the reporter does not support this metric type.-java.lang.String(name)metric.getClass().getName(),o,o,o
Flink,677,org.apache.flink.optimizer.traversals.GraphCreatingVisitor.preVisit,254,warn,java.lang.String(name)The parallelism of nested dataflows (such as step functions in iterations) is currently fixed to the parallelism of the surrounding operator (the iteration).,o,o,o
Flink,723,org.apache.flink.queryablestate.network.AbstractServerBase.attemptToBind,212,debug,java.lang.String(name)Attempting to start {} on port {}.-java.lang.String(name)serverName-int(name)port,o,o,o
Flink,730,org.apache.flink.queryablestate.network.AbstractServerHandler.operationComplete,303,debug,java.lang.String(name)Request {} was successfully answered after {} ms.-REQ(name)request-long(name)durationMillis,o,o,o
Flink,739,org.apache.flink.runtime.accumulators.AccumulatorRegistry.getSnapshot,59,warn,java.lang.String(name)Failed to serialize accumulators for task.-java.lang.Throwable(name)e,o,o,o
Flink,758,org.apache.flink.runtime.blob.BlobServer.close,315,debug,java.lang.String(name)Error while waiting for this thread to die.-java.lang.InterruptedException(name)ie,o,o,x
Flink,768,org.apache.flink.runtime.blob.BlobServer.moveTempFileToStore,717,debug,"java.lang.String(name)Trying to find a unique key for BLOB of job {} (retry {}, last tried {})-org.apache.flink.api.common.JobID(name)jobId-int(name)attempt-java.lang.String(name)storageFile.getAbsolutePath()",o,o,o
Flink,763,org.apache.flink.runtime.blob.BlobServer.putBuffer,586,warn,java.lang.String(name)Could not delete the staging file {} for job {}.-java.io.File(name)incomingFile-org.apache.flink.api.common.JobID(name)jobId,o,o,o
Flink,784,org.apache.flink.runtime.blob.BlobServerConnection.run,126,debug,java.lang.String(name)Socket connection closed-java.net.SocketException(name)e,o,o,o
Flink,798,org.apache.flink.runtime.blob.BlobUtils.closeSilently,388,debug,java.lang.String(name)Exception while closing BLOB server connection socket.-java.lang.Throwable(name)t,o,o,o
Flink,800,org.apache.flink.runtime.blob.BlobUtils.moveTempFileToStore,447,warn,java.lang.String(name)File upload for an existing file with key {} for job {}. This may indicate a duplicate upload or a hash collision. Ignoring newest upload.-org.apache.flink.runtime.blob.BlobKey(name)blobKey-org.apache.flink.api.common.JobID(name)jobId,o,o,o
Flink,801,org.apache.flink.runtime.blob.BlobUtils.moveTempFileToStore,454,warn,java.lang.String(name)Could not delete the storage file {}.-java.io.File(name)storageFile,o,o,o
Flink,826,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint,883,warn,java.lang.String(name)Could not properly discard completed checkpoint {}.-long(name)completedCheckpoint.getCheckpointID()-java.lang.Throwable(name)t,o,o,o
Flink,836,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.discardSubtaskState,1400,warn,java.lang.String(name)Could not properly discard state object of checkpoint {} belonging to task {} of job {}.-long(name)checkpointId-org.apache.flink.runtime.executiongraph.ExecutionAttemptID(name)executionAttemptID-org.apache.flink.api.common.JobID(name)jobId-java.lang.Throwable(name)t2,o,o,o
Flink,817,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage,732,debug,java.lang.String(name)Received another decline message for now expired checkpoint attempt {} from task {} of job {} at {} : {}-long(name)checkpointId-org.apache.flink.runtime.executiongraph.ExecutionAttemptID(name)message.getTaskExecutionId()-org.apache.flink.api.common.JobID(name)job-java.lang.String(name)taskManagerLocationInfo-java.lang.String(name)reason,o,o,o
Flink,818,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage,736,debug,java.lang.String(name)Received decline message for unknown (too old?) checkpoint attempt {} from task {} of job {} at {} : {}-long(name)checkpointId-org.apache.flink.runtime.executiongraph.ExecutionAttemptID(name)message.getTaskExecutionId()-org.apache.flink.api.common.JobID(name)job-java.lang.String(name)taskManagerLocationInfo-java.lang.String(name)reason,o,o,o
Flink,834,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.run,1326,error,java.lang.String(name)Exception while triggering checkpoint for job {}.-org.apache.flink.api.common.JobID(name)job-java.lang.Exception(name)e,o,o,o
Flink,808,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint,502,info,java.lang.String(name)Checkpoint triggering task {} of job {} is not being executed at the moment. Aborting checkpoint.-java.lang.String(name)tasksToTrigger[i].getTaskNameWithSubtaskIndex()-org.apache.flink.api.common.JobID(name)job,o,o,o
Flink,810,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint,527,info,java.lang.String(name)Checkpoint acknowledging task {} of job {} is not being executed at the moment. Aborting checkpoint.-java.lang.String(name)ev.getTaskNameWithSubtaskIndex()-org.apache.flink.api.common.JobID(name)job,o,o,o
Flink,815,org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint,673,warn,java.lang.String(name)Cannot dispose failed checkpoint storage location {}-org.apache.flink.runtime.state.CheckpointStorageLocation(name)checkpointStorageLocation-java.lang.Throwable(name)t2,o,o,o
Flink,845,org.apache.flink.runtime.checkpoint.Checkpoints.loadAndValidateCheckpoint,209,info,java.lang.String(name)Skipping empty savepoint state for operator {}.-org.apache.flink.runtime.jobgraph.OperatorID(name)operatorState.getOperatorID(),o,o,o
Flink,837,org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.getLatestCheckpoint,72,info,"java.lang.String(name)Found a completed checkpoint before the latest savepoint, will use it to recover!",o,o,o
Flink,874,org.apache.flink.runtime.checkpoint.hooks.MasterHooks.restoreMasterHooks,252,debug,java.lang.String(name)Found state to restore for hook '{}'-java.lang.String(name)name,o,o,o
Flink,869,org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.shutdown,97,info,java.lang.String(name)Shutting down.,o,x,o
Flink,861,org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.pathToCheckpointId,326,warn,java.lang.String(name)Could not parse checkpoint id from {}. This indicates that the checkpoint id to path conversion has changed.-java.lang.String(name)path,o,o,o
Flink,854,org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.recover,163,info,java.lang.String(name)Trying to fetch {} checkpoints from storage.-int(name)numberOfInitialCheckpoints,o,o,o
Flink,859,org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.shutdown,265,info,java.lang.String(name)Shutting down,o,x,o
Flink,860,org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.shutdown,276,info,java.lang.String(name)Suspending,o,x,o
Flink,876,org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem,244,info,java.lang.String(name)Trying to start actor system at {}-java.lang.String(name)hostPortUrl,o,o,o
Flink,880,org.apache.flink.runtime.clusterframework.BootstrapTools.updateTmpDirectoriesInConfiguration,530,info,java.lang.String(name)Setting directories for temporary files to: {}-java.lang.String(name)defaultDirs,o,o,o
Flink,898,org.apache.flink.runtime.dispatcher.Dispatcher.jobNotFinished,803,info,java.lang.String(name)Job {} was not finished by JobManager.-org.apache.flink.api.common.JobID(name)jobId,o,o,o
Flink,896,org.apache.flink.runtime.dispatcher.Dispatcher.jobReachedGloballyTerminalState,768,info,java.lang.String(name)Job {} reached globally terminal state {}.-org.apache.flink.api.common.JobID(name)archivedExecutionGraph.getJobID()-org.apache.flink.runtime.jobgraph.JobStatus(name)archivedExecutionGraph.getState(),o,o,o
Flink,893,org.apache.flink.runtime.dispatcher.Dispatcher.terminateJobManagerRunners,696,info,java.lang.String(name)Stopping all currently running jobs of dispatcher {}.-java.lang.String(name)getAddress(),o,o,o
Flink,904,org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.initializeHandlers,113,info,java.lang.String(name)Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.,o,o,o
Flink,911,org.apache.flink.runtime.entrypoint.ClusterEntrypoint.onFatalError,374,error,java.lang.String(name)Fatal error occurred in the cluster entrypoint.-java.lang.Throwable(name)exception,o,o,o
Flink,906,org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main,56,error,java.lang.String(name)Could not parse command line arguments {}.-String[](name)args-org.apache.flink.runtime.entrypoint.FlinkParseException(name)e,o,o,o
Flink,957,org.apache.flink.runtime.executiongraph.ExecutionGraph.allVerticesInTerminalState,1417,warn,java.lang.String(name)Job has entered globally terminal state without waiting for all job vertices to reach final state.,o,o,o
Flink,946,org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph,891,debug,java.lang.String(name)Attaching {} topologically sorted vertices to existing job graph with {} vertices and {} intermediate results.-int(name)topologiallySorted.size()-int(name)tasks.size()-int(name)intermediateResults.size(),o,o,o
Flink,958,org.apache.flink.runtime.executiongraph.ExecutionGraph.tryRestartOrFail,1444,debug,java.lang.String(name)Try to restart or fail the job {} ({}) if no longer possible.-java.lang.String(name)getJobName()-org.apache.flink.api.common.JobID(name)getJobID()-java.lang.Throwable(name)failureCause,o,o,o
Flink,964,org.apache.flink.runtime.executiongraph.ExecutionGraph.updateAccumulators,1693,debug,java.lang.String(name)Received accumulator result for unknown execution {}.-org.apache.flink.runtime.executiongraph.ExecutionAttemptID(name)execID,o,o,o
Flink,924,org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph,222,info,java.lang.String(name)Successfully ran initialization on master in {} ms.-long(name)(System.nanoTime() - initMasterStart) / 1_000_000,o,o,o
Flink,927,org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph,257,warn,java.lang.String(name)The setting for '{} : {}' is invalid. Using default value of {}-java.lang.String(name)CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.key()-int(name)maxNumberOfCheckpointsToRetain-java.lang.Integer(name)CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.defaultValue(),o,o,o
Flink,972,org.apache.flink.runtime.executiongraph.failover.FailoverRegion.FailoverRegion,86,debug,java.lang.String(name)Created failover region {} with vertices: {}-org.apache.flink.util.AbstractID(name)id-java.util.List<ExecutionVertex>(name)connectedExecutions,o,o,o
Flink,987,org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy.buildFailoverRegions,102,debug,java.lang.String(name)Creating a failover region with {} vertices.-int(name)regionVertices.size(),o,o,o
Flink,983,org.apache.flink.runtime.executiongraph.failover.RestartPipelinedRegionStrategy.makeAllOneRegion,206,warn,java.lang.String(name)Cannot decompose ExecutionGraph into individual failover regions due to use of Co-Location constraints (iterations). Job will fail over as one holistic unit.,o,o,o
Flink,991,org.apache.flink.runtime.executiongraph.restart.RestartStrategyFactory.createRestartStrategyFactory,150,warn,java.lang.String(name)Could not find restart strategy class {}.-java.lang.String(name)restartStrategyName,o,o,o
Flink,479,org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create,110,debug,java.lang.String(name)Instantiating for file system scheme {} Hadoop File System {}-java.lang.String(name)scheme-java.lang.String(name)fsClass.getName(),o,o,o
Flink,1010,org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.run,516,warn,java.lang.String(name)Error notifying leader listener about new leader-java.lang.Throwable(name)t,o,o,o
Flink,1017,org.apache.flink.runtime.history.FsJobArchivist.archiveJob,92,info,java.lang.String(name)Job {} has been archived at {}.-org.apache.flink.api.common.JobID(name)jobId-org.apache.flink.core.fs.Path(name)path,o,o,o
Flink,1020,org.apache.flink.runtime.io.disk.iomanager.IOManager.deleteChannel,101,warn,java.lang.String(name)IOManager failed to delete temporary file {}-java.lang.String(name)channel.getPath(),o,o,o
Flink,1043,org.apache.flink.runtime.io.network.api.writer.RecordWriter.notifyFlusherException,294,error,java.lang.String(name)An exception happened while flushing the outputs-java.lang.Throwable(name)t,o,o,o
Flink,1045,org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.NetworkBufferPool,140,info,"java.lang.String(name)Allocated {} MB for network buffer pool (number of memory segments: {}, bytes per segment: {}).-long(name)allocatedMb-int(name)availableMemorySegments.size()-int(name)segmentSize",o,o,o
Flink,1038,org.apache.flink.runtime.io.network.NettyShuffleEnvironment.close,312,warn,java.lang.String(name)Cannot shut down the network connection manager.-java.lang.Throwable(name)t,o,o,o
Flink,1040,org.apache.flink.runtime.io.network.NettyShuffleEnvironment.close,321,warn,java.lang.String(name)Cannot shut down the result partition manager.-java.lang.Throwable(name)t,o,o,o
Flink,1042,org.apache.flink.runtime.io.network.NettyShuffleEnvironment.close,340,warn,java.lang.String(name)Cannot close the file channel manager properly.-java.lang.Throwable(name)t,o,o,o
Flink,1034,org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start,283,info,java.lang.String(name)Starting the network environment and its components.,o,o,o
Flink,1032,org.apache.flink.runtime.io.network.TaskEventDispatcher.registerPartition,59,debug,java.lang.String(name)registering {}-org.apache.flink.runtime.io.network.partition.ResultPartitionID(name)partitionId,o,o,x
Flink,1033,org.apache.flink.runtime.io.network.TaskEventDispatcher.unregisterPartition,78,debug,java.lang.String(name)unregistering {}-org.apache.flink.runtime.io.network.partition.ResultPartitionID(name)partitionId,o,o,x
Flink,1099,org.apache.flink.runtime.iterative.task.AbstractIterativeTask.closeLocalStrategiesAndCaches,177,error,java.lang.String(name)Error while shutting down an iterative operator.-java.lang.Throwable(name)t,o,o,o
Flink,1113,org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.putJobGraph,215,debug,java.lang.String(name)Adding job graph {} to {}{}.-org.apache.flink.api.common.JobID(name)jobGraph.getJobID()-java.lang.String(name)zooKeeperFullBasePath-java.lang.String(name)path,x,o,o
Flink,1111,org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.recoverJobGraph,163,debug,java.lang.String(name)Recovering job graph {} from {}{}.-org.apache.flink.api.common.JobID(name)jobId-java.lang.String(name)zooKeeperFullBasePath-java.lang.String(name)path,x,o,o
Flink,1118,org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.releaseJobGraph,281,debug,java.lang.String(name)Releasing locks of job graph {} from {}{}.-org.apache.flink.api.common.JobID(name)jobId-java.lang.String(name)zooKeeperFullBasePath-java.lang.String(name)path,x,o,o
Flink,1116,org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.removeJobGraph,261,debug,java.lang.String(name)Removing job graph {} from {}{}.-org.apache.flink.api.common.JobID(name)jobId-java.lang.String(name)zooKeeperFullBasePath-java.lang.String(name)path,x,o,o
Flink,1158,org.apache.flink.runtime.jobmaster.JobManagerRunner.unregisterJobFromHighAvailability,270,error,java.lang.String(name)Could not un-register from high-availability services job {} ({}).Other JobManager's may attempt to recover it and re-execute it.-java.lang.String(name)jobGraph.getName()-org.apache.flink.api.common.JobID(name)jobGraph.getJobID()-java.lang.Throwable(name)t,o,o,o
Flink,1135,org.apache.flink.runtime.jobmaster.JobMaster.onStop,335,info,java.lang.String(name)Stopping the JobMaster for job {}({}).-java.lang.String(name)jobGraph.getName()-org.apache.flink.api.common.JobID(name)jobGraph.getJobID(),o,o,o
Flink,1139,org.apache.flink.runtime.jobmaster.JobMaster.requestKvStateLocation,461,info,java.lang.String(name)Error while request key-value state location-java.lang.Exception(name)e,o,o,x
Flink,1136,org.apache.flink.runtime.jobmaster.JobMaster.requestNextInputSplit,394,warn,java.lang.String(name)Error while requesting next input split-java.io.IOException(name)e,o,o,o
Flink,1143,org.apache.flink.runtime.jobmaster.JobMaster.requestOperatorBackPressureStats,657,info,java.lang.String(name)Error while requesting operator back pressure stats-org.apache.flink.util.FlinkException(name)e,o,o,o
Flink,1148,org.apache.flink.runtime.jobmaster.JobMaster.suspendExecution,760,debug,java.lang.String(name)Job has already been suspended or shutdown.,o,o,o
Flink,1190,org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.releaseSharedSlot,593,debug,java.lang.String(name)Could not find slot [{}] in slot sharing group {}. Ignoring release slot request.-org.apache.flink.runtime.jobmaster.SlotRequestId(name)slotRequestId-org.apache.flink.runtime.instance.SlotSharingGroupId(name)slotSharingGroupId,o,o,o
Flink,1172,org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestSlotFromResourceManager,319,info,java.lang.String(name)Requesting new slot [{}] and profile {} from resource manager.-org.apache.flink.runtime.jobmaster.SlotRequestId(name)pendingRequest.getSlotRequestId()-org.apache.flink.runtime.clusterframework.types.ResourceProfile(name)pendingRequest.getResourceProfile(),o,o,o
Flink,1169,org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.checkOversubscriptionAndReleaseChildren,607,debug,"java.lang.String(name)Not all requests are fulfilled due to over-allocated, number of requests is {}, number of evicted requests is {}, underlying allocated is {}, fulfilled is {}, evicted requests is {},-int(name)children.size()-int(name)childrenToEvict.size()-org.apache.flink.runtime.clusterframework.types.ResourceProfile(name)slotContext.getResourceProfile()-org.apache.flink.runtime.clusterframework.types.ResourceProfile(name)requiredResources-java.util.ArrayList<TaskSlot>(name)childrenToEvict",o,o,o
Flink,1198,org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.isLeader,230,debug,java.lang.String(name)Ignoring the grant leadership notification since the service has already been stopped.,o,o,o
Flink,1206,org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.writeLeaderInformation,331,debug,"java.lang.String(name)Write leader information: Leader={}, session ID={}.-java.lang.String(name)leaderContender.getAddress()-java.util.UUID(name)leaderSessionID",o,o,o
Flink,1215,org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService.nodeChanged,169,debug,"java.lang.String(name)New leader information: Leader={}, session ID={}.-java.lang.String(name)leaderAddress-java.util.UUID(name)leaderSessionID",o,o,o
Flink,1222,org.apache.flink.runtime.memory.MemoryManager.MemoryManager,181,debug,"java.lang.String(name)Initialized MemoryManager with total memory size {}, number of slots {}, page size {}, memory type {}, pre allocate memory {} and number of non allocated pages {}.-long(name)memorySize-int(name)numberOfSlots-int(name)pageSize-org.apache.flink.core.memory.MemoryType(name)memoryType-boolean(name)preAllocateMemory-int(name)numNonAllocatedPages",o,o,o
Flink,1259,org.apache.flink.runtime.metrics.dump.MetricQueryService.logDumpSizeWouldExceedLimit,192,info,java.lang.String(name){} will not be reported as the metric dump would exceed the maximum size of {} bytes.-java.lang.String(name)Some metrics-long(name)messageSizeLimit,o,o,o
Flink,1262,org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.addMetric,404,warn,"java.lang.String(name)""Name collision: Group already contains a Metric with the name '"" + name + ""'. Metric will not be reported.""+ Arrays.toString(scopeComponents)",o,o,o
Flink,1232,org.apache.flink.runtime.metrics.MetricRegistryImpl.getDelimiter,229,warn,"java.lang.String(name)Delimiter for reporter index {} not found, returning global delimiter.-int(name)reporterIndex",o,o,o
Flink,1228,org.apache.flink.runtime.metrics.MetricRegistryImpl.MetricRegistryImpl,146,info,java.lang.String(name)Reporting metrics for reporter {} of type {}.-java.lang.String(name)namedReporter-java.lang.String(name)className,o,o,o
Flink,1246,org.apache.flink.runtime.metrics.ReporterSetup.loadReporterFactories,189,warn,java.lang.String(name)Error while loading reporter factory.-java.lang.Throwable(name)e,o,o,o
Flink,1249,org.apache.flink.runtime.metrics.ReporterSetup.loadViaReflection,247,info,java.lang.String(name)The reporter configuration of {} is out-dated (but still supported). Please configure a factory class instead: '{}{}.{}: {}' to ensure that the configuration continues to work with future versions.-java.lang.String(name)reporterName-java.lang.String(name)metrics.reporter.-java.lang.String(name)reporterName-java.lang.String(name)factory.class-java.lang.String(name)alternativeFactoryClassName,x,o,o
Flink,1270,org.apache.flink.runtime.metrics.util.MetricUtils.getValue,242,warn,java.lang.String(name)Could not read attribute {}.-java.lang.String(name)attributeName-javax.management.JMException(name)e,o,o,o
Flink,1274,org.apache.flink.runtime.minicluster.MiniCluster.start,267,info,java.lang.String(name)Starting RPC Service(s),o,o,o
Flink,1289,org.apache.flink.runtime.net.ConnectionUtils.findAddressUsingStrategy,261,debug,java.lang.String(name)Target address {} and local address {} share prefix - trying to connect.-java.net.InetSocketAddress(name)targetAddress-java.net.InetAddress(name)interfaceAddress,o,o,o
Flink,1290,org.apache.flink.runtime.net.ConnectionUtils.findAddressUsingStrategy,272,debug,java.lang.String(name)Trying to connect to {} from local address {} with timeout {}-java.net.InetSocketAddress(name)targetAddress-java.net.InetAddress(name)interfaceAddress-int(name)strategy.getTimeout(),o,o,o
Flink,1287,org.apache.flink.runtime.net.ConnectionUtils.findAddressUsingStrategy,228,warn,java.lang.String(name)Could not resolve local hostname to an IP address: {}-java.lang.String(name)uhe.getMessage(),o,o,o
Flink,1283,org.apache.flink.runtime.net.ConnectionUtils.findConnectingAddress,150,debug,java.lang.String(name)Could not connect. Waiting for {} msecs before next attempt-long(name)toWait,o,o,o
Flink,1296,org.apache.flink.runtime.net.ConnectionUtils.findConnectingAddress,409,info,java.lang.String(name)Trying to connect to address {}-java.net.InetSocketAddress(name)targetAddress,o,o,o
Flink,1299,org.apache.flink.runtime.net.RedirectingSslHandler.channelRead,96,trace,"java.lang.String(name)Received non-SSL request, redirecting to {}{}-java.lang.String(name)redirectAddress-java.lang.String(name)path",x,o,o
Flink,1312,org.apache.flink.runtime.operators.BatchTask.closeLocalStrategiesAndCaches,576,error,"java.lang.String(name)""Error closing local strategy for input "" + i-java.lang.Throwable(name)t",o,o,o
Flink,1313,org.apache.flink.runtime.operators.BatchTask.closeLocalStrategiesAndCaches,587,error,"java.lang.String(name)""Error closing temp barrier for input "" + i-java.lang.Throwable(name)t",o,o,o
Flink,1300,org.apache.flink.runtime.operators.BatchTask.invoke,245,debug,"java.lang.String(name)formatLogString(""Start registering input and output."")",o,o,o
Flink,1365,org.apache.flink.runtime.operators.DataSinkTask.invoke,143,debug,"java.lang.String(name)getLogString(""Rich Sink detected. Initializing runtime context."")",o,o,o
Flink,1330,org.apache.flink.runtime.operators.DataSourceTask.invoke,142,debug,"java.lang.String(name)getLogString(""Rich Source detected. Initializing runtime context."")",o,o,o
Flink,1331,org.apache.flink.runtime.operators.DataSourceTask.invoke,144,debug,"java.lang.String(name)getLogString(""Rich Source detected. Opening the InputFormat."")",o,o,o
Flink,1334,org.apache.flink.runtime.operators.DataSourceTask.invoke,175,debug,"java.lang.String(name)getLogString(""Starting to read input from split "" + split.toString())",o,o,o
Flink,1336,org.apache.flink.runtime.operators.DataSourceTask.invoke,243,debug,"java.lang.String(name)getLogString(""Rich Source detected. Closing the InputFormat."")",o,o,o
Flink,1351,org.apache.flink.runtime.operators.GroupReduceDriver.run,112,debug,"java.lang.String(name)this.taskContext.formatLogString(""GroupReducer preprocessing done. Running GroupReducer code."")",o,o,o
Flink,1382,org.apache.flink.runtime.operators.hash.CompactingHashTable.close,283,debug,java.lang.String(name)Closing hash table and releasing resources.,o,o,o
Flink,1385,org.apache.flink.runtime.operators.hash.MutableHashTable.initBloomFilter,826,debug,"java.lang.String(name)String.format(""Create BloomFilter with average input entries per bucket[%d], bytes size[%d], false positive probability[%f]."",avgNumRecordsPerBucket,byteSize,fpp)",o,o,o
Flink,1396,org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.go,211,error,java.lang.String(name)Sorting thread was interrupted (without being shut down) while grabbing a buffer. Retrying to grab buffer...,o,o,o
Flink,1447,org.apache.flink.runtime.operators.sort.LargeRecordHandler.close,319,error,java.lang.String(name)Cannot close the large records reader.-java.lang.Throwable(name)t,o,o,o
Flink,1417,org.apache.flink.runtime.operators.sort.UnilateralSortMerger.go,980,debug,java.lang.String(name)Large record did not fit into a fresh sort buffer. Putting into large record store.,o,o,o
Flink,1452,org.apache.flink.runtime.query.QueryableStateUtils.createKvStateClientProxy,82,debug,"java.lang.String(name)msg + "" Cause: "" + e.getMessage()",o,x,o
Flink,1453,org.apache.flink.runtime.query.QueryableStateUtils.createKvStateClientProxy,84,info,"java.lang.String(name)Could not load Queryable State Client Proxy. Probable reason: flink-queryable-state-runtime is not in the classpath. To enable Queryable State, please move the flink-queryable-state-runtime jar from the opt to the lib folder.",o,o,o
Flink,1460,org.apache.flink.runtime.registration.RetryingRegistration.register,204,info,java.lang.String(name)Registration at {} attempt {} (timeout={}ms)-java.lang.String(name)targetName-int(name)attempt-long(name)timeoutMillis,o,o,o
Flink,1491,org.apache.flink.runtime.resourcemanager.JobLeaderIdService.notifyLeaderAddress,265,debug,java.lang.String(name)Found a new job leader {}@{}.-java.util.UUID(name)leaderSessionId-java.lang.String(name)leaderAddress,o,o,o
Flink,1478,org.apache.flink.runtime.resourcemanager.ResourceManager.closeTaskManagerConnection,805,info,java.lang.String(name)Closing TaskExecutor connection {} because: {}-org.apache.flink.runtime.clusterframework.types.ResourceID(name)resourceID-java.lang.String(name)cause.getMessage(),o,o,o
Flink,1468,org.apache.flink.runtime.resourcemanager.ResourceManager.deregisterApplication,504,warn,java.lang.String(name)Could not properly shutdown the application.-org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException(name)e,o,o,o
Flink,1481,org.apache.flink.runtime.resourcemanager.ResourceManager.jobLeaderLostLeadership,838,debug,"java.lang.String(name)Discarding job leader lost leadership, because a new job leader was found for job {}. -org.apache.flink.api.common.JobID(name)jobId",o,o,o
Flink,1471,org.apache.flink.runtime.resourcemanager.ResourceManager.registerJobMasterInternal,633,debug,java.lang.String(name)Job manager {}@{} was already registered.-org.apache.flink.runtime.jobmaster.JobMasterId(name)jobMasterGateway.getFencingToken()-java.lang.String(name)jobManagerAddress,o,o,o
Flink,1473,org.apache.flink.runtime.resourcemanager.ResourceManager.registerTaskExecutorInternal,695,debug,java.lang.String(name)Replacing old registration of TaskExecutor {}.-org.apache.flink.runtime.clusterframework.types.ResourceID(name)taskExecutorResourceId,o,o,o
Flink,1483,org.apache.flink.runtime.resourcemanager.ResourceManager.releaseResource,860,debug,java.lang.String(name)Worker {} could not be stopped.-org.apache.flink.runtime.clusterframework.types.ResourceID(name)worker.getResourceID(),o,o,o
Flink,1494,org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.getSlotRequestTimeout,98,warn,java.lang.String(name)Config key {} is deprecated; use {} instead.-org.apache.flink.configuration.ConfigOption<Long>(name)ResourceManagerOptions.SLOT_REQUEST_TIMEOUT-org.apache.flink.configuration.ConfigOption<Long>(name)JobManagerOptions.SLOT_REQUEST_TIMEOUT,o,o,o
Flink,1506,org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.freeSlot,465,debug,"java.lang.String(name)Received request to free slot {} with expected allocation id {}, but actual allocation id {} differs. Ignoring the request.-org.apache.flink.runtime.clusterframework.types.SlotID(name)slotId-org.apache.flink.runtime.clusterframework.types.AllocationID(name)allocationId-org.apache.flink.runtime.clusterframework.types.AllocationID(name)slot.getAllocationId()",o,o,o
Flink,1507,org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.freeSlot,469,debug,java.lang.String(name)Slot {} has not been allocated.-org.apache.flink.runtime.clusterframework.types.AllocationID(name)allocationId,o,o,o
Flink,1514,org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.handleFailedSlotRequest,1027,debug,java.lang.String(name)There was not pending slot request with allocation id {}. Probably the request has been fulfilled or cancelled.-org.apache.flink.runtime.clusterframework.types.AllocationID(name)allocationId,o,o,o
Flink,1502,org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.unregisterTaskManager,395,debug,java.lang.String(name)Unregister TaskManager {} from the SlotManager.-org.apache.flink.runtime.instance.InstanceID(name)instanceId,o,o,o
Flink,1535,org.apache.flink.runtime.rest.FileUploadHandler.handleError,174,warn,java.lang.String(name)errorMessage-java.lang.Throwable(name)e,x,x,x
Flink,1557,org.apache.flink.runtime.rest.handler.AbstractHandler.handleException,200,error,java.lang.String(name)Exception occurred in REST handler: {}-java.lang.String(name)rhe.getMessage(),o,o,o
Flink,1554,org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader,154,error,java.lang.String(name)Could not create the handler request.-org.apache.flink.runtime.rest.handler.HandlerRequestException(name)hre,o,o,o
Flink,1564,org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImpl.apply,286,debug,java.lang.String(name)Failed to gather stack trace sample.-java.lang.Throwable(name)throwable,o,o,o
Flink,1562,org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImpl.triggerStackTraceSampleInternal,191,debug,"java.lang.String(name)""Triggering stack trace sample for tasks: "" + Arrays.toString(vertex.getTaskVertices())",o,o,o
Flink,1572,org.apache.flink.runtime.rest.handler.legacy.backpressure.StackTraceSampleCoordinator.collectStackTraces,271,debug,java.lang.String(name)Received late stack trace sample {} of task {}-int(name)sampleId-org.apache.flink.runtime.executiongraph.ExecutionAttemptID(name)executionId,o,o,o
Flink,1570,org.apache.flink.runtime.rest.handler.legacy.backpressure.StackTraceSampleCoordinator.shutDown,220,info,java.lang.String(name)Shutting down stack trace sample coordinator.,o,o,o
Flink,1574,org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.respondToRequest,171,debug,java.lang.String(name)Loading missing file from classloader: {}-java.lang.String(name)requestPath,o,o,o
Flink,1577,org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.respondToRequest,229,debug,"java.lang.String(name)""Responding 'NOT MODIFIED' for file '"" + file.getAbsolutePath() + '\''",o,o,o
Flink,1589,org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.removeBlob,190,debug,java.lang.String(name)Remove cached file for TaskExecutor {}.-org.apache.flink.runtime.clusterframework.types.ResourceID(name)removalNotification.getKey(),o,o,o
Flink,1591,org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse,83,error,java.lang.String(name)Internal server error. Could not map response to JSON.-java.io.IOException(name)ioe,o,o,o
Flink,1542,org.apache.flink.runtime.rest.RestClient.sendRequest,251,debug,java.lang.String(name)Sending request of class {} to {}:{}{}-java.lang.Class<>(name)request.getClass()-java.lang.String(name)targetAddress-int(name)targetPort-java.lang.String(name)targetUrl,x,o,o
Flink,1526,org.apache.flink.runtime.rest.RestServerEndpoint.checkAndCreateUploadDir,474,info,java.lang.String(name)Using directory {} for file uploads.-java.nio.file.Path(name)uploadDir,o,o,o
Flink,1527,org.apache.flink.runtime.rest.RestServerEndpoint.checkAndCreateUploadDir,476,info,java.lang.String(name)Created directory {} for file uploads.-java.nio.file.Path(name)uploadDir,o,o,o
Flink,1524,org.apache.flink.runtime.rest.RestServerEndpoint.createUploadDir,454,info,java.lang.String(name)Upload directory {} does not exist. -java.nio.file.Path(name)uploadDir,o,o,o
Flink,1598,org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage,157,info,java.lang.String(name)The rpc endpoint {} has not been started yet. Discarding message {} until processing is started.-java.lang.String(name)rpcEndpoint.getClass().getName()-java.lang.String(name)message.getClass().getName(),o,o,x
Flink,1602,org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation,256,error,java.lang.String(name)Could not deserialize rpc invocation message.-java.io.IOException(name)e,o,o,x
Flink,1597,org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage,91,debug,java.lang.String(name)Unknown message type: Ignoring message {} because it is neither of type {} nor {}.-java.lang.Object(name)message-java.lang.String(name)FencedMessage.class.getSimpleName()-java.lang.String(name)UnfencedMessage.class.getSimpleName(),o,o,o
Flink,1616,org.apache.flink.runtime.scheduler.LegacyScheduler.notifyKvStateRegistered,427,debug,java.lang.String(name)Key value state registered for job {} under name {}.-org.apache.flink.api.common.JobID(name)jobGraph.getJobID()-java.lang.String(name)registrationName,o,o,o
Flink,1617,org.apache.flink.runtime.scheduler.LegacyScheduler.notifyKvStateUnregistered,448,debug,java.lang.String(name)Key value state unregistered for job {} under name {}.-org.apache.flink.api.common.JobID(name)jobGraph.getJobID()-java.lang.String(name)registrationName,o,o,o
Flink,1628,org.apache.flink.runtime.security.SecurityUtils.uninstall,109,warn,java.lang.String(name)unable to uninstall a security module-org.apache.flink.runtime.security.modules.SecurityInstallException(name)e,o,o,x
Flink,1677,org.apache.flink.runtime.state.filesystem.FileBasedStateOutputStream.closeAndGetHandle,141,warn,java.lang.String(name)Could not delete the checkpoint stream file {}.-org.apache.flink.core.fs.Path(name)path-java.lang.Exception(name)deleteException,o,o,o
Flink,1679,org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.close,284,warn,java.lang.String(name)Cannot delete closed and discarded state stream for {}.-org.apache.flink.core.fs.Path(name)statePath-java.lang.Exception(name)e,o,o,o
Flink,1635,org.apache.flink.runtime.state.SharedStateRegistry.registerReference,110,trace,java.lang.String(name)Identified duplicate state registration under key {}. New state {} was determined to be an unnecessary copy of existing state {} and will be dropped.-org.apache.flink.runtime.state.SharedStateRegistryKey(name)registrationKey-org.apache.flink.runtime.state.StreamStateHandle(name)state-org.apache.flink.runtime.state.StreamStateHandle(name)entry.stateHandle,o,o,o
Flink,1638,org.apache.flink.runtime.state.SharedStateRegistry.scheduleAsyncDelete,197,trace,java.lang.String(name)Scheduled delete of state handle {}.-org.apache.flink.runtime.state.StreamStateHandle(name)streamStateHandle,o,o,o
Flink,1659,org.apache.flink.runtime.state.StateUtil.discardStateFuture,82,debug,"java.lang.String(name)Cancelled execution of snapshot future runnable. Cancellation produced the following exception, which is expected an can be ignored.-java.lang.Exception(name)ex",o,o,o
Flink,1663,org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.localStateStoreForSubtask,162,debug,java.lang.String(name)Registered new local state store with configuration {} for {} - {} - {} under allocation id {}.-org.apache.flink.runtime.state.LocalRecoveryConfig(name)localRecoveryConfig-org.apache.flink.api.common.JobID(name)jobId-org.apache.flink.runtime.jobgraph.JobVertexID(name)jobVertexID-int(name)subtaskIndex-org.apache.flink.runtime.clusterframework.types.AllocationID(name)allocationID,o,o,o
Flink,1645,org.apache.flink.runtime.state.TaskLocalStateStoreImpl.confirmCheckpoint,218,debug,java.lang.String(name)Received confirmation for checkpoint {} in subtask ({} - {} - {}). Starting to prune history.-long(name)confirmedCheckpointId-org.apache.flink.api.common.JobID(name)jobID-org.apache.flink.runtime.jobgraph.JobVertexID(name)jobVertexID-int(name)subtaskIndex,o,o,o
Flink,1647,org.apache.flink.runtime.state.TaskLocalStateStoreImpl.discardLocalStateForCheckpoint,290,debug,java.lang.String(name)Discarding local task state snapshot {} of checkpoint {} for subtask ({} - {} - {}).-org.apache.flink.runtime.checkpoint.TaskStateSnapshot(name)o-long(name)checkpointID-org.apache.flink.api.common.JobID(name)jobID-org.apache.flink.runtime.jobgraph.JobVertexID(name)jobVertexID-int(name)subtaskIndex,o,o,o
Flink,1644,org.apache.flink.runtime.state.TaskLocalStateStoreImpl.retrieveLocalState,202,debug,java.lang.String(name)Did not find registered local state for checkpoint {} in subtask ({} - {} - {})-long(name)checkpointID-org.apache.flink.api.common.JobID(name)jobID-org.apache.flink.runtime.jobgraph.JobVertexID(name)jobVertexID-int(name)subtaskIndex,o,o,o
Flink,1642,org.apache.flink.runtime.state.TaskLocalStateStoreImpl.retrieveLocalState,195,trace,java.lang.String(name)Found registered local state for checkpoint {} in subtask ({} - {} - {}) : {}-long(name)checkpointID-org.apache.flink.api.common.JobID(name)jobID-org.apache.flink.runtime.jobgraph.JobVertexID(name)jobVertexID-int(name)subtaskIndex-org.apache.flink.runtime.checkpoint.TaskStateSnapshot(name)snapshot,o,o,o
Flink,1688,org.apache.flink.runtime.taskexecutor.JobLeaderService.start,125,info,java.lang.String(name)Start job leader service.,o,o,o
Flink,1689,org.apache.flink.runtime.taskexecutor.JobLeaderService.stop,142,info,java.lang.String(name)Stop job leader service.,o,o,o
Flink,1719,org.apache.flink.runtime.taskexecutor.TaskExecutor.cancelTask,693,debug,java.lang.String(name)message,x,x,x
Flink,1724,org.apache.flink.runtime.taskexecutor.TaskExecutor.confirmCheckpoint,817,debug,java.lang.String(name)message,x,x,x
Flink,1753,org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal,1481,debug,java.lang.String(name)Free slot with allocation id {} because: {}-org.apache.flink.runtime.clusterframework.types.AllocationID(name)allocationId-java.lang.String(name)cause.getMessage(),o,o,o
Flink,1760,org.apache.flink.runtime.taskexecutor.TaskExecutor.notifyFatalError,1748,error,java.lang.String(name)message-java.lang.Throwable(name)cause,x,x,x
Flink,1729,org.apache.flink.runtime.taskexecutor.TaskExecutor.requestSlot,856,info,java.lang.String(name)message,x,x,x
Flink,1722,org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint,796,debug,java.lang.String(name)message,x,x,x
Flink,1773,org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main,281,info,java.lang.String(name)Maximum number of open file descriptors is {}.-long(name)maxOpenFileHandles,o,o,o
Flink,1708,org.apache.flink.runtime.taskexecutor.TaskManagerServices.createMemoryManager,364,info,"java.lang.String(name)Limiting managed memory to {} of the currently free heap space ({} MB), memory will be allocated lazily.-float(name)memoryFraction-long(name)relativeMemSize >> 20",o,o,o
Flink,1788,org.apache.flink.runtime.taskmanager.MemoryLogger.run,130,info,java.lang.String(name)getDirectMemoryStatsAsString(directBufferBean),x,x,x
Flink,1789,org.apache.flink.runtime.taskmanager.MemoryLogger.run,131,info,java.lang.String(name)getMemoryPoolStatsAsString(poolBeans),x,x,x
Flink,1806,org.apache.flink.runtime.taskmanager.Task.closeNetworkResources,906,error,java.lang.String(name)Failed to release input gate for task {}.-java.lang.String(name)taskNameWithSubtask-java.lang.Throwable(name)t,o,o,o
Flink,1802,org.apache.flink.runtime.taskmanager.Task.doRun,839,error,java.lang.String(name)message-java.lang.Throwable(name)t,x,x,x
Flink,1821,org.apache.flink.runtime.taskmanager.Task.run,1442,error,java.lang.String(name)Error in the task canceler for task {}.-java.lang.String(name)taskName-java.lang.Throwable(name)t,o,o,o
Flink,1824,org.apache.flink.runtime.taskmanager.Task.run,1570,error,java.lang.String(name)msg,x,x,x
Flink,1822,org.apache.flink.runtime.taskmanager.Task.run,1499,warn,"java.lang.String(name)Task '{}' did not react to cancelling signal for {} seconds, but is stuck in method: {}-java.lang.String(name)taskName-long(name)(interruptIntervalMillis / 1000)-java.lang.StringBuilder(name)bld",o,o,o
Flink,1828,org.apache.flink.runtime.taskmanager.TaskManagerLocation.getHostName,202,warn,"java.lang.String(name)No hostname could be resolved for the IP address {}, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.-java.lang.String(name)inetAddress.getHostAddress()",o,o,o
Flink,1861,org.apache.flink.runtime.util.EnvironmentInformation.logEnvironmentInfo,286,info,"java.lang.String(name)"" JVM: "" + jvmVersion",x,x,o
Flink,1857,org.apache.flink.runtime.util.EnvironmentInformation.logEnvironmentInfo,281,info,java.lang.String(name)--------------------------------------------------------------------------------,x,x,x
Flink,1868,org.apache.flink.runtime.util.EnvironmentInformation.logEnvironmentInfo,303,info,"java.lang.String(name)"" "" + s",x,x,x
Flink,1871,org.apache.flink.runtime.util.EnvironmentInformation.logEnvironmentInfo,313,info,"java.lang.String(name)"" "" + s",x,x,x
Flink,1838,org.apache.flink.runtime.util.Hardware.getSizeOfPhysicalMemoryForWindows,256,error,java.lang.String(name)Cannot determine the size of the physical memory for Windows host (using 'wmic memorychip')-java.lang.Throwable(name)t,o,o,o
Flink,1925,org.apache.flink.runtime.webmonitor.history.HistoryServer.start,217,info,java.lang.String(name)Starting history server.,o,o,o
Flink,1927,org.apache.flink.runtime.webmonitor.history.HistoryServer.stop,240,info,java.lang.String(name)Stopping history server.,o,o,o
Flink,1931,org.apache.flink.runtime.webmonitor.history.HistoryServer.stop,257,info,java.lang.String(name)Stopped history server.,o,o,o
Flink,1936,org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.run,206,error,java.lang.String(name)Failure while fetching/processing job archive for job {}.-java.lang.String(name)jobID-java.io.IOException(name)e,o,o,o
Flink,1943,org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.respondWithFile,161,debug,java.lang.String(name)Unable to load requested file {} from classloader-java.lang.String(name)requestPath,o,o,o
Flink,1942,org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.respondWithFile,158,error,java.lang.String(name)error while responding-java.lang.Throwable(name)t,o,x,x
Flink,1949,org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.getServerPort,143,error,java.lang.String(name)Cannot access local server port-java.lang.Exception(name)e,o,o,o
Flink,1899,org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.runFlinkZkQuorumPeer,104,info,"java.lang.String(name)""Configuration: "" + zkProps",x,x,o
Flink,2031,org.apache.flink.streaming.api.functions.sink.SocketClientSink.invoke,187,error,java.lang.String(name)Could not close socket from failed write attempt-java.io.IOException(name)ee,o,o,o
Flink,2026,org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.recoverAndCommitInternal,410,error,java.lang.String(name)Error while committing transaction {}. Transaction has been open for longer than the transaction timeout ({}).Commit will not be attempted again. Data loss might have occurred.-TXN(name)transactionHolder.handle-long(name)transactionTimeout-java.lang.Exception(name)e,o,o,o
Flink,2050,org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.listEligibleFiles,303,warn,java.lang.String(name)Path does not exist: {}-org.apache.flink.core.fs.Path(name)path,o,o,o
Flink,2067,org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.dispose,180,warn,java.lang.String(name)The reader is stuck in method: {}-java.lang.String(name)bld.toString(),o,o,o
Flink,2062,org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.run,98,info,"java.lang.String(name)File processed: {}, {}, {}-java.lang.String(name)filePath-long(name)offset-long(name)fileSize",x,o,o
Flink,2054,org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.initializeState,150,info,java.lang.String(name)Restoring state for the {}.-java.lang.String(name)getClass().getSimpleName(),o,o,o
Flink,2076,org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform,233,debug,"java.lang.String(name)""Transforming "" + transform",x,x,o
Flink,2088,org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState,424,info,java.lang.String(name)snapshotFailMessage-java.lang.Exception(name)snapshotException,x,x,x
Flink,2089,org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState,472,warn,java.lang.String(name)Could not close raw keyed operator state stream for {}. This might have prevented deleting some state data.-java.lang.String(name)getOperatorName()-java.lang.Exception(name)closeException,o,o,o
Flink,2091,org.apache.flink.streaming.api.operators.async.Emitter.run,82,debug,java.lang.String(name)Wait for next completed async stream element result.,o,o,o
Flink,2080,org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore,109,debug,java.lang.String(name)Creating {} with empty state.-java.lang.String(name)logDescription,o,o,o
Flink,87,org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.handlePendingInProgressFile,825,debug,java.lang.String(name)Truncating {} to valid length {}-org.apache.hadoop.fs.Path(name)partPath-long(name)validLength,o,o,o
Flink,88,org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.handlePendingInProgressFile,830,debug,java.lang.String(name)Trying to recover file lease {}-org.apache.hadoop.fs.Path(name)partPath,o,o,o
Flink,79,org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.reflectTruncate,661,error,java.lang.String(name)Could not delete truncate test file.-java.io.IOException(name)e,o,o,o
Flink,116,org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.startHasFailedKillEverything,157,error,java.lang.String(name)| ====================,x,x,x
Flink,115,org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.startHasFailedKillEverything,156,error,java.lang.String(name)|,x,x,x
Flink,124,org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.terminateAndDiscardAnyExistingContainers,226,info,java.lang.String(name)/===========================================,x,x,x
Flink,130,org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.terminateAndDiscardAnyExistingContainers,247,info,java.lang.String(name)\===========================================,x,x,x
Flink,102,org.apache.flink.streaming.connectors.gcp.pubsub.emulator.PubsubHelper.deleteTopic,101,info,java.lang.String(name)DeleteTopic {} first delete old subscriptions.-com.google.pubsub.v1.ProjectTopicName(name)topicName,o,o,o
Flink,100,org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSink.onSuccess,322,debug,java.lang.String(name)Successfully published message with id: {}-java.lang.String(name)result,o,o,o
Flink,243,org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.initializeState,886,info,java.lang.String(name)Consumer subtask {} has no restore state.-int(name)getRuntimeContext().getIndexOfThisSubtask(),o,o,o
Flink,249,org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.notifyCheckpointComplete,972,debug,java.lang.String(name)Consumer subtask {} has empty checkpoint state.-int(name)getRuntimeContext().getIndexOfThisSubtask(),o,o,o
Flink,236,org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.open,623,info,"java.lang.String(name)Consumer subtask {} will start reading the following {} partitions from the specified startup offsets {}: {}-int(name)getRuntimeContext().getIndexOfThisSubtask()-int(name)subscribedPartitionsToStartOffsets.size()-java.util.Map<KafkaTopicPartition,Long>(name)specificStartupOffsets-java.util.Set<KafkaTopicPartition>(name)subscribedPartitionsToStartOffsets.keySet()",o,o,o
Flink,158,org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.FlinkKafkaProducer,649,warn,java.lang.String(name)Overwriting the '{}' is not recommended-java.lang.String(name)key.serializer,o,o,o
Flink,159,org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.FlinkKafkaProducer,655,warn,java.lang.String(name)Overwriting the '{}' is not recommended-java.lang.String(name)value.serializer,o,o,o
Flink,167,org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.initProducer,1184,info,java.lang.String(name)Producer implementation does not support metrics,o,o,o
Flink,161,org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.open,743,error,"java.lang.String(name)""Error while sending record to Kafka: "" + e.getMessage()-java.lang.Exception(name)e",o,o,o
Flink,254,org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.open,247,warn,"java.lang.String(name)Flushing on checkpoint is enabled, but checkpointing is not enabled. Disabling flushing.",o,o,o
Flink,173,org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.resumeTransaction,196,info,java.lang.String(name)Attempting to resume transaction {} with producerId {} and epoch {}-java.lang.String(name)transactionalId-long(name)producerId-short(name)epoch,o,o,o
Flink,168,org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run,182,info,java.lang.String(name)Consumer implementation does not support metrics,o,o,o
Flink,214,org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.runFetchLoop,158,warn,java.lang.String(name)No group offset can be found for partition {} in Zookeeper; resetting starting offset to 'auto.offset.reset'-org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState<TopicAndPartition>(name)partition,o,o,o
Flink,208,org.apache.flink.streaming.connectors.kafka.internals.Kafka08PartitionDiscoverer.getAllTopics,141,debug,java.lang.String(name)Detailed trace-java.lang.Exception(name)e,o,x,o
Flink,205,org.apache.flink.streaming.connectors.kafka.internals.Kafka08PartitionDiscoverer.getAllTopics,116,info,java.lang.String(name)Trying to get topic metadata from broker {} in try {}/{}-java.lang.String(name)seedBrokerAddresses[currentContactSeedBrokerIndex]-int(name)retry-int(name)numRetries,o,o,o
Flink,194,org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.run,231,debug,java.lang.String(name)Full exception-java.lang.Throwable(name)cce,o,x,o
Flink,191,org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.run,190,info,java.lang.String(name)Consumer thread {} does not have any partitions assigned anymore. Stopping thread.-java.lang.String(name)getName(),o,o,o
Flink,195,org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.run,236,warn,java.lang.String(name)Unable to reach broker after {} retries. Returning all current partitions-int(name)reconnectLimit,o,o,o
Flink,224,org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandler.getOffsetFromZooKeeper,145,error,"java.lang.String(name)The offset in ZooKeeper for group '{}', topic '{}', partition {} is a malformed string: {}-java.lang.String(name)groupId-java.lang.String(name)topic-int(name)partition-java.lang.String(name)asString",o,o,o
Flink,269,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.run,312,info,"java.lang.String(name)Subtask {} is seeding the fetcher with new discovered shard {}, starting state set to the SENTINEL_EARLIEST_SEQUENCE_NUM-int(name)getRuntimeContext().getIndexOfThisSubtask()-java.lang.String(name)shard.toString()",o,o,o
Flink,270,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.run,328,info,"java.lang.String(name)Subtask {} will be seeded with initial shard {}, starting state set as sequence number {}-int(name)getRuntimeContext().getIndexOfThisSubtask()-java.lang.String(name)shard.toString()-org.apache.flink.streaming.connectors.kinesis.model.SequenceNumber(name)startingSeqNum.get()",o,o,o
Flink,260,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.close,299,info,java.lang.String(name)Closing producer,o,x,o
Flink,264,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.enforceQueueLimit,385,warn,"java.lang.String(name)Waiting for the queue length to drop below the limit takes unusually long, still not done after {} attempts.-int(name)attempt",o,o,o
Flink,258,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.open,250,info,java.lang.String(name)Started Kinesis producer instance for region '{}'-java.lang.String(name)producerConfig.getRegion(),o,o,o
Flink,257,org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.open,241,warn,java.lang.String(name)An exception occurred while processing a record-java.lang.Throwable(name)t,o,o,o
Flink,288,org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitWatermark,870,debug,java.lang.String(name)Evaluating watermark for subtask {} time {}-int(name)indexOfThisConsumerSubtask-long(name)getCurrentTimeMillis(),o,o,o
Flink,281,org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.runFetcher,519,info,java.lang.String(name)Subtask {} has no active shards to read on startup; marking the subtask as temporarily idle ...-int(name)indexOfThisConsumerSubtask,o,o,o
Flink,287,org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.updateState,803,info,java.lang.String(name)Subtask {} has reached the end of all currently subscribed shards; marking the subtask as temporarily idle ...-int(name)indexOfThisConsumerSubtask,o,o,o
Flink,299,org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.listShards,452,info,java.lang.String(name)The stream is currently not in active state. Reusing the older state for the time being,o,o,o
Flink,309,org.apache.flink.streaming.connectors.nifi.NiFiSource.run,105,debug,"java.lang.String(name)No data available to pull, waiting and will try again...",o,o,o
Flink,318,org.apache.flink.streaming.connectors.twitter.TwitterSource.close,168,info,java.lang.String(name)Closing source,o,x,o
Flink,323,org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEventIrcStream.onDisconnected,119,debug,java.lang.String(name)Disconnected.,o,x,o
Flink,324,org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEventIrcStream.onError,124,error,java.lang.String(name)Error: '{}'.-java.lang.String(name)msg,o,x,o
Flink,334,org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEventIrcStream.onQuit,178,debug,java.lang.String(name)Quit: {}.-java.lang.String(name)user.getNick(),o,x,o
Flink,322,org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEventIrcStream.onRegistered,114,debug,java.lang.String(name)Connected.,o,x,o
Flink,2129,org.apache.flink.streaming.runtime.tasks.StreamIterationHead.cleanup,111,info,java.lang.String(name)Iteration head {} removed feedback queue under {}-java.lang.String(name)getName()-java.lang.String(name)brokerID,o,o,o
Flink,2128,org.apache.flink.streaming.runtime.tasks.StreamIterationHead.init,93,info,java.lang.String(name)Iteration head {} added feedback queue under {}-java.lang.String(name)getName()-java.lang.String(name)brokerID,o,o,o
Flink,2135,org.apache.flink.streaming.runtime.tasks.StreamTask.invoke,437,debug,java.lang.String(name)Closed operators for task {}-java.lang.String(name)getName(),o,o,o
Flink,2141,org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier,735,info,java.lang.String(name)Operator {} was cancelled while performing checkpoint {}.-java.lang.String(name)getName()-long(name)checkpointMetaData.getCheckpointId(),o,o,o
Flink,140,org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf,152,info,java.lang.String(name)Setting hive conf dir as {}-java.lang.String(name)hiveConfDir,o,o,x
Flink,141,org.apache.flink.table.catalog.hive.HiveCatalog.open,182,info,java.lang.String(name)Connected to Hive metastore,o,o,o
Flink,1967,org.apache.flink.table.client.cli.CliClient.printException,603,warn,java.lang.String(name)message-java.lang.Throwable(name)t,x,x,x
Flink,1974,org.apache.flink.table.client.gateway.local.LocalExecutor.discoverDependencies,614,debug,java.lang.String(name)Using the following dependencies: {}-java.util.List<URL>(name)dependencies,o,o,o
Flink,1971,org.apache.flink.table.client.gateway.local.LocalExecutor.LocalExecutor,140,info,java.lang.String(name)Using default environment file: {}-java.net.URL(name)defaultEnv,o,o,o
Flink,1964,org.apache.flink.table.client.SqlClient.readSessionEnvironment,167,info,java.lang.String(name)Using session environment file: {}-java.net.URL(name)envUrl,o,o,o
Flink,2210,org.apache.flink.table.runtime.functions.SqlFunctionUtils.parseUrl,510,error,"java.lang.String(name)""Parse URL error: "" + urlStr-java.lang.Exception(name)e",o,x,o
Flink,2231,org.apache.flink.table.runtime.hashtable.LongHashPartition.reHash,401,info,java.lang.String(name)The rehash take {} ms for {} segments-long(name)(System.currentTimeMillis() - reHashStartTime)-int(name)numBuckets,o,o,o
Flink,2244,org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.growAndRehash,440,info,java.lang.String(name)The rehash take {} ms for {} segments-long(name)(System.currentTimeMillis() - reHashStartTime)-int(name)required,o,o,o
Flink,2261,org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.initHeapStates,225,warn,"java.lang.String(name)Failed to build sorted map from state, this may result in wrong result. The sort key is {}, partition key is {}, treeMap is {}. The expected inner rank is {}, but current size is {}.-org.apache.flink.table.dataformat.BaseRow(name)sortKey-org.apache.flink.table.dataformat.BaseRow(name)partitionKey-java.util.TreeMap<Integer,BaseRow>(name)treeMap-java.lang.Integer(name)innerRank-int(name)size",o,o,o
Flink,2281,org.apache.flink.table.runtime.operators.sort.BinaryExternalSorter.getIterator,599,debug,java.lang.String(name)Sending done.,o,x,o
Flink,2277,org.apache.flink.table.runtime.operators.sort.BinaryExternalSorter.releaseSortMemory,457,info,java.lang.String(name)error.-java.lang.Throwable(name)ignored,x,x,x
Flink,438,org.apache.flink.util.NetUtils.createSocketFromPorts,404,debug,java.lang.String(name)Trying to open socket on port {}-int(name)port,o,o,o
Flink,440,org.apache.flink.util.NetUtils.createSocketFromPorts,411,info,"java.lang.String(name)Unable to allocate on port {}, due to error: {}-int(name)port-java.lang.String(name)e.getMessage()",o,o,o
Flink,434,org.apache.flink.util.ShutdownHookUtil.addShutdownHookThread,75,error,java.lang.String(name)Cannot register shutdown hook that cleanly terminates {}.-java.lang.String(name)serviceName-java.lang.Throwable(name)t,o,o,o
Flink,2319,org.apache.flink.walkthrough.common.sink.LoggerOutputFormat.writeRecord,45,info,java.lang.String(name)record,x,x,x
Flink,2386,org.apache.flink.yarn.AbstractYarnClusterDescriptor.checkYarnQueues,642,warn,"java.lang.String(name)""Error while getting queue information from YARN: "" + e.getMessage()",o,o,o
Flink,2375,org.apache.flink.yarn.AbstractYarnClusterDescriptor.retrieve,354,info,java.lang.String(name)Found Web Interface {}:{} of application '{}'.-java.lang.String(name)host-int(name)port-org.apache.hadoop.yarn.api.records.ApplicationId(name)applicationId,o,o,o
Flink,2373,org.apache.flink.yarn.AbstractYarnClusterDescriptor.retrieve,337,warn,java.lang.String(name)Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.,o,o,o
Flink,2397,org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster,1022,debug,java.lang.String(name)Application State: {}-org.apache.hadoop.yarn.api.records.YarnApplicationState(name)appState,o,o,o
Flink,2401,org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster,1050,info,"java.lang.String(name)""The Flink YARN client has been started in detached mode. In order to stop "" + ""Flink on YARN, use the following command or a YARN web interface to stop "" + ""it:\nyarn application -kill "" + appId + ""\nPlease also note that the ""+ ""temporary files of the YARN session in the home directory will not be removed.""",o,o,o
Flink,2396,org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster,1011,info,java.lang.String(name)Waiting for the cluster to be allocated,o,o,o
Flink,2388,org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster,673,warn,"java.lang.String(name)""The file system scheme is '"" + fs.getScheme() + ""'. This indicates that the ""+ ""specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.""+ ""The Flink YARN client needs to store its files in a distributed file system""",o,o,o
Flink,2443,org.apache.flink.yarn.cli.FlinkYarnSessionCli.deleteYarnPropertiesFile,761,warn,java.lang.String(name)Couldn't delete Yarn properties file at {}-java.io.File(name)propertiesFile.getAbsoluteFile(),o,o,o
Flink,2444,org.apache.flink.yarn.cli.FlinkYarnSessionCli.deleteYarnPropertiesFile,765,warn,java.lang.String(name)Exception while deleting the JobManager address file-java.lang.Exception(name)e,o,o,o
Flink,2447,org.apache.flink.yarn.cli.FlinkYarnSessionCli.handleCliArgsException,947,error,java.lang.String(name)Could not parse the command line arguments.-org.apache.flink.client.cli.CliArgsException(name)e,o,o,o
Flink,2448,org.apache.flink.yarn.cli.FlinkYarnSessionCli.handleError,956,error,java.lang.String(name)Error while running the Flink Yarn session.-java.lang.Throwable(name)t,o,o,o
Flink,2445,org.apache.flink.yarn.cli.FlinkYarnSessionCli.logAndSysout,791,info,java.lang.String(name)message,x,x,x
Flink,2441,org.apache.flink.yarn.cli.FlinkYarnSessionCli.logApplicationReport,744,warn,"java.lang.String(name)""If log aggregation is activated in the Hadoop cluster, we recommend to retrieve "" + ""the full application log using this command:"" + System.lineSeparator() + ""\tyarn logs -applicationId ""+ appReport.getApplicationId()+ System.lineSeparator()+ ""(It sometimes takes a few seconds until the logs are aggregated)""",o,o,o
Flink,2433,org.apache.flink.yarn.cli.FlinkYarnSessionCli.run,637,info,java.lang.String(name)Could not properly terminate the Flink cluster.-org.apache.flink.util.FlinkException(name)fe,o,o,o
Flink,2438,org.apache.flink.yarn.cli.FlinkYarnSessionCli.tryRetrieveAndLogApplicationReport,728,info,java.lang.String(name)Could not log the final application report.-java.lang.Exception(name)e,o,o,o
Flink,2364,org.apache.flink.yarn.Utils.createTaskExecutorContext,512,debug,java.lang.String(name)Prepared local resource for modified yaml: {}-org.apache.hadoop.yarn.api.records.LocalResource(name)flinkConf,o,o,o
Flink,2353,org.apache.flink.yarn.Utils.obtainTokenForHBase,325,error,java.lang.String(name)No Kerberos security token for HBase available,o,o,o
Flink,2354,org.apache.flink.yarn.Utils.obtainTokenForHBase,330,info,java.lang.String(name)Added HBase Kerberos security token to credentials.,o,o,o
Flink,2328,org.apache.flink.yarn.YarnResourceManager.YarnResourceManager,173,warn,java.lang.String(name)The heartbeat interval of the Flink Application master ({}) is greater than YARN's expiry interval ({}). The application is likely to be killed by YARN.-int(name)yarnHeartbeatIntervalMS-long(name)yarnExpiryIntervalMS,o,o,o
Flink,460,org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream,2628,info,"java.lang.String(name)name + "" not found""",o,x,x
Flink,462,org.apache.hadoop.conf.Configuration.getConfResourceAsReader,2652,info,"java.lang.String(name)name + "" not found""",o,x,x
Flink,2186,org.apache.hadoop.conf.Configuration.loadResource,2564,debug,"java.lang.String(name)""parsing File "" + file",o,o,x
Flink,2189,org.apache.hadoop.conf.Configuration.loadResource,2658,fatal,"java.lang.String(name)""error parsing conf "" + name-org.w3c.dom.DOMException(name)e",o,o,x
Flink,2183,org.apache.hadoop.conf.Configuration.parse,2472,debug,"java.lang.String(name)""parsing URL "" + url",o,o,x
Flink,2174,org.apache.hadoop.conf.Configuration.warnOnceIfDeprecated,1147,info,java.lang.String(name)keyInfo.getWarningMessage(name),x,x,x
HBase,306,org.apache.hadoop.hbase.AuthUtil.loginFromKeytabAndReturnUser,147,error,"java.lang.String(name)Error while trying to login as user {} through {}, with message: {}.-java.lang.String(name)hbase.client.keytab.principal-java.lang.String(name)hbase.client.keytab.file-java.lang.String(name)ioe.getMessage()",o,o,o
HBase,1415,org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.addTable,57,debug,"java.lang.String(name)""Already archiving table: "" + table + "", ignoring it""",o,o,o
HBase,1423,org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.checkEnabledAndUpdate,168,debug,"java.lang.String(name)archiveHFileZNode + "" znode does exist, checking for tables to archive""",o,o,x
HBase,1386,org.apache.hadoop.hbase.backup.HFileArchiver.archiveFamilyByFamilyDir,266,debug,"java.lang.String(name)No files to dispose of in {}, family={}-java.lang.String(name)parent.getRegionNameAsString()-java.lang.String(name)Bytes.toString(family)",o,o,o
HBase,1390,org.apache.hadoop.hbase.backup.HFileArchiver.resolveAndArchive,396,trace,java.lang.String(name)Moving files to the archive directory {}-org.apache.hadoop.fs.Path(name)baseArchiveDir,o,o,o
HBase,4657,org.apache.hadoop.hbase.c.doRow,502,info,java.lang.String(name)NOTHING FOLLOWS,x,x,x
HBase,635,org.apache.hadoop.hbase.chaos.actions.ChangeSplitPolicyAction.perform,52,info,"java.lang.String(name)""Performing action: Change split policy of table "" + tableName",o,o,o
HBase,642,org.apache.hadoop.hbase.chaos.actions.MoveRegionsOfTableAction.perform,62,info,java.lang.String(name)Performing action: Move regions of table {}-org.apache.hadoop.hbase.TableName(name)tableName,o,o,o
HBase,643,org.apache.hadoop.hbase.chaos.actions.MoveRegionsOfTableAction.perform,65,info,java.lang.String(name)Table {} doesn't have regions to move-org.apache.hadoop.hbase.TableName(name)tableName,o,o,o
HBase,708,org.apache.hadoop.hbase.chaos.actions.RestartActionBaseAction.sleep,37,info,"java.lang.String(name)""Sleeping for:"" + sleepTime",o,o,o
HBase,655,org.apache.hadoop.hbase.chaos.actions.UnbalanceKillAndRebalanceAction.perform,84,info,java.lang.String(name)Not killing server because it holds hbase:meta.,o,o,o
HBase,92,org.apache.hadoop.hbase.client.AsyncConnectionConfiguration.AsyncConnectionConfiguration,134,warn,"java.lang.String(name)The {} setting: {} ms is less than the {} setting: {} ms, use the greater one instead-java.lang.String(name)hbase.client.pause.cqtbe-long(name)pauseForCQTBEMs-java.lang.String(name)hbase.client.pause-long(name)pauseMs",o,o,o
HBase,206,org.apache.hadoop.hbase.client.AsyncNonMetaRegionLocator.addToCache,255,trace,java.lang.String(name)Will not add {} to cache because the old value {} is newer than us or has the same server name. Maybe it is updated before we replace it-org.apache.hadoop.hbase.RegionLocations(name)locs-org.apache.hadoop.hbase.RegionLocations(name)oldLocs,o,o,o
HBase,109,org.apache.hadoop.hbase.client.AsyncRegionLocator.clearCache,158,debug,java.lang.String(name)Clear meta cache for {}-org.apache.hadoop.hbase.TableName(name)tableName,o,o,o
HBase,110,org.apache.hadoop.hbase.client.AsyncRegionLocator.clearCache,167,debug,java.lang.String(name)Clear meta cache for {}-org.apache.hadoop.hbase.ServerName(name)serverName,o,o,o
HBase,117,org.apache.hadoop.hbase.client.AsyncRegionLocatorHelper.updateCachedLocationOnError,83,debug,java.lang.String(name)Try removing {} from cache-org.apache.hadoop.hbase.HRegionLocation(name)loc,o,o,o
HBase,63,org.apache.hadoop.hbase.client.ConnectionImplementation.isTableAvailable,650,debug,java.lang.String(name)Table {} has not deployed region {}-org.apache.hadoop.hbase.TableName(name)tableName-java.lang.String(name)pair.getFirst().getEncodedName(),o,o,o
HBase,64,org.apache.hadoop.hbase.client.ConnectionImplementation.isTableAvailable,669,debug,java.lang.String(name)Table {} has {} regions not deployed-org.apache.hadoop.hbase.TableName(name)tableName-int(name)notDeployed,o,o,o
HBase,66,org.apache.hadoop.hbase.client.ConnectionImplementation.isTableAvailable,679,trace,java.lang.String(name)Table {} should be available-org.apache.hadoop.hbase.TableName(name)tableName,o,o,o
HBase,67,org.apache.hadoop.hbase.client.ConnectionImplementation.isTableAvailable,683,warn,java.lang.String(name)Table {} does not exist-org.apache.hadoop.hbase.TableName(name)tableName,o,o,o
HBase,155,org.apache.hadoop.hbase.client.HBaseAdmin.execProcedure,2875,debug,java.lang.String(name)Getting current status of procedure from master...,o,o,o
HBase,147,org.apache.hadoop.hbase.client.HBaseAdmin.snapshot,2601,debug,java.lang.String(name)Getting current status of snapshot from master...,o,o,o
HBase,25,org.apache.hadoop.hbase.client.HBaseHbck.bypassProcedure,153,error,"java.lang.String(name)pids.stream().map(null).collect(Collectors.joining("", ""))-java.lang.Throwable(name)t",x,x,x
HBase,27,org.apache.hadoop.hbase.client.HBaseHbck.runHbckChore,186,debug,java.lang.String(name)Failed to run HBCK chore-org.apache.hbase.thirdparty.com.google.protobuf.ServiceException(name)se,o,o,x
HBase,96,org.apache.hadoop.hbase.client.HTableMultiplexer.run,607,debug,"java.lang.String(name)""Caught some exceptions when flushing puts to region server "" + addr.getHostnamePort()-org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException(name)arf.getErrors()",o,o,o
HBase,118,org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.RawAsyncHBaseAdmin,344,warn,"java.lang.String(name)Configured value of pauseForCQTBENs is {} ms, which is less than the normal pause value {} ms, use the greater one instead-long(name)TimeUnit.NANOSECONDS.toMillis(builder.pauseForCQTBENs)-long(name)TimeUnit.NANOSECONDS.toMillis(builder.pauseNs)",o,o,o
HBase,1444,org.apache.hadoop.hbase.conf.ConfigurationManager.notifyAllObservers,125,error,"java.lang.String(name)""Encountered a throwable while notifying observers: "" + "" of type : "" + observer.getClass().getCanonicalName() + ""(""+ observer+ "")""-java.lang.Throwable(name)t",o,o,o
HBase,1443,org.apache.hadoop.hbase.conf.ConfigurationManager.notifyAllObservers,117,info,java.lang.String(name)Starting to notify all observers that config changed.,o,o,o
HBase,1446,org.apache.hadoop.hbase.constraint.Constraints.getConstraints,581,warn,"java.lang.String(name)""Corrupted configuration found for key:"" + key + "", skipping it.""",o,o,o
HBase,1483,org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.deleteNodeFailure,295,info,"java.lang.String(name)""Failed to delete node "" + path + "" and will retry soon.""",o,o,o
HBase,1479,org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.resubmitTask,185,trace,"java.lang.String(name)""Skipping the resubmit of "" + task.toString() + "" because the server ""+ task.cur_worker_name+ "" is not marked as dead, we waited for ""+ time+ "" while the timeout is ""+ timeout",o,o,o
HBase,1462,org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.attemptToOwnTask,371,warn,"java.lang.String(name)""Interrupted while trying to assert ownership of "" + task + "" ""+ StringUtils.stringifyException(e1)",o,o,o
HBase,1471,org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.endTask,557,info,"java.lang.String(name)""successfully transitioned task "" + task + "" to final state ""+ slt",o,o,x
HBase,1523,org.apache.hadoop.hbase.coprocessor.CoprocessorHost.loadSystemCoprocessors,151,warn,"java.lang.String(name)""Attempted duplicate loading of "" + className + ""; skipped""",o,o,o
HBase,1538,org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.start,108,debug,"java.lang.String(name)""Scheduling process timer to run in: "" + maxTime + "" ms""",o,o,o
HBase,1559,org.apache.hadoop.hbase.favored.FavoredNodeAssignmentHelper.placeSecondaryAndTertiaryWithRestrictions,379,warn,"java.lang.String(name)""Cannot place the favored nodes for region "" + regionInfo.getRegionNameAsString() + "" because ""+ e-java.lang.Exception(name)e",o,o,o
HBase,1567,org.apache.hadoop.hbase.fs.HFileSystem.addLocationsOrderInterceptor,312,warn,java.lang.String(name)Can't get the file system from the conf.-java.io.IOException(name)e,o,o,o
HBase,4651,org.apache.hadoop.hbase.HFilePerformanceEvaluation.runBenchmarks,165,info,java.lang.String(name)testSummary.toString(),x,x,x
HBase,1579,org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.unbuffer,302,trace,"java.lang.String(name)""Failed to find 'unbuffer' method in class "" + streamClass + "" . So there may be a TCP socket connection ""+ ""left open in CLOSE_WAIT state. For more details check ""+ ""https://issues.apache.org/jira/browse/HBASE-9393""",o,o,o
HBase,1679,org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.logStats,637,info,"java.lang.String(name)""failedBlockAdditions="" + cacheStats.getFailedInserts() + "", ""+ ""totalSize=""+ StringUtils.byteDesc(totalSize)+ "", ""+ ""freeSize=""+ StringUtils.byteDesc(freeSize)+ "", ""+ ""usedSize=""+ StringUtils.byteDesc(usedSize)+ "", ""+ ""cacheSize=""+ StringUtils.byteDesc(cacheSize)+ "", ""+ ""accesses=""+ cacheStats.getRequestCount()+ "", ""+ ""hits=""+ cacheStats.getHitCount()+ "", ""+ ""IOhitsPerSecond=""+ cacheStats.getIOHitsPerSecond()+ "", ""+ ""IOTimePerHit=""+ String.format(""%.2f"",cacheStats.getIOTimePerHit())+ "", ""+ ""hitRatio=""+ (cacheStats.getHitCount() == 0 ? ""0,"" : (StringUtils.formatPercent(cacheStats.getHitRatio(),2) + "", ""))+ ""cachingAccesses=""+ cacheStats.getRequestCachingCount()+ "", ""+ ""cachingHits=""+ cacheStats.getHitCachingCount()+ "", ""+ ""cachingHitsRatio=""+ (cacheStats.getHitCachingCount() == 0 ? ""0,"" : (StringUtils.formatPercent(cacheStats.getHitCachingRatio(),2) + "", ""))+ ""evictions=""+ cacheStats.getEvictionCount()+ "", ""+ ""evicted=""+ cacheStats.getEvictedCount()+ "", ""+ ""evictedPerRun=""+ cacheStats.evictedPerEviction()",x,x,o
HBase,1662,org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.FileIOEngine,72,debug,"java.lang.String(name)""File "" + filePath + "" already exists. Deleting!!""",o,o,x
HBase,1659,org.apache.hadoop.hbase.io.hfile.bucket.FileMmapEngine.FileMmapEngine,67,error,"java.lang.String(name)""Can't extend bucket cache file; insufficient space for "" + StringUtils.byteDesc(fileSize)-java.io.IOException(name)ioex",o,o,o
HBase,1598,org.apache.hadoop.hbase.io.hfile.HFile.create,335,trace,java.lang.String(name)Unable to set drop behind on {}-org.apache.hadoop.fs.Path(name)path-java.lang.UnsupportedOperationException(name)uoe,o,o,o
HBase,1582,org.apache.hadoop.hbase.io.HFileLink.create,421,error,"java.lang.String(name)""couldn't create the link="" + name + "" for ""+ dstFamilyPath-java.io.IOException(name)e",o,o,x
HBase,1700,org.apache.hadoop.hbase.io.util.MemorySizeUtil.safeGetHeapMemoryUsage,74,warn,"java.lang.String(name)Got an exception while attempting to read information about the JVM heap. Please submit this log information in a bug report and include your JVM settings, specifically the GC in use and any -XX options. Consider restarting the service.-java.lang.RuntimeException(name)exception",o,o,o
HBase,1710,org.apache.hadoop.hbase.ipc.CallRunner.run,134,warn,"java.lang.String(name)""Can not complete this request in time, drop it: "" + call",o,o,o
HBase,1753,org.apache.hadoop.hbase.ipc.RpcServer.initReconfigurable,351,warn,java.lang.String(name)****************************,x,x,x
HBase,1767,org.apache.hadoop.hbase.ipc.ServerRpcConnection.processConnectionHeader,519,warn,java.lang.String(name)Allowed fallback to SIMPLE auth for {} connecting from {}-org.apache.hadoop.security.UserGroupInformation(name)ugi-java.lang.String(name)getHostAddress(),o,o,o
HBase,1761,org.apache.hadoop.hbase.ipc.ServerRpcConnection.saslReadAndProcess,388,warn,"java.lang.String(name)RpcServer.AUTH_FAILED_FOR + clientIP + "":""+ saslServer.getAttemptingUser()",x,x,x
HBase,852,org.apache.hadoop.hbase.mapreduce.CopyTable.run,419,info,java.lang.String(name)command: ./bin/hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles {} {}-java.lang.String(name)this.bulkloadDir.toString()-java.lang.String(name)this.dstTableName,x,x,x
HBase,776,org.apache.hadoop.hbase.mapreduce.Import.createSubmittableJob,828,info,java.lang.String(name)Use Large Result!!,o,o,x
HBase,901,org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.cleanup,301,error,java.lang.String(name)fail to scan peer table in cleanup-java.lang.Exception(name)e,o,o,x
HBase,902,org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.cleanup,312,error,java.lang.String(name)fail to close source table in cleanup-java.io.IOException(name)e,o,o,x
HBase,903,org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.cleanup,319,error,java.lang.String(name)fail to close source connection in cleanup-java.lang.Exception(name)e,o,o,x
HBase,904,org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.cleanup,327,error,java.lang.String(name)fail to close replicated table in cleanup-java.lang.Exception(name)e,o,o,x
HBase,905,org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.cleanup,334,error,java.lang.String(name)fail to close replicated connection in cleanup-java.lang.Exception(name)e,o,o,x
HBase,754,org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJarsForClasses,926,warn,"java.lang.String(name)""Could not find jar for class "" + clazz + "" in order to ship it to the cluster.""",o,o,o
HBase,864,org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue,233,warn,"java.lang.String(name)We are restarting the first next() invocation, if your mapper has restarted a few other times like this then you should consider killing this job and investigate why it's taking so long.",o,o,o
HBase,1836,org.apache.hadoop.hbase.master.ActiveMasterManager.hasActiveMaster,253,info,"java.lang.String(name)""Received an unexpected KeeperException when checking "" + ""isActiveMaster : "" + ke",o,o,o
HBase,2221,org.apache.hadoop.hbase.master.assignment.AssignmentManager.checkOnlineRegionsReport,1123,warn,java.lang.String(name){} reported an unexpected OPEN on {}; time since last update={}ms-org.apache.hadoop.hbase.master.assignment.RegionStateNode(name)regionNode-org.apache.hadoop.hbase.ServerName(name)serverName-long(name)diff,x,o,o
HBase,2216,org.apache.hadoop.hbase.master.assignment.AssignmentManager.reportOnlineRegions,1063,trace,"java.lang.String(name)ReportOnlineRegions {} regionCount={}, metaLoaded={} {}-org.apache.hadoop.hbase.ServerName(name)serverName-int(name)regionNames.size()-boolean(name)isMetaLoaded()-java.util.List<String>(name)regionNames.stream().map(null).collect(Collectors.toList())",x,o,o
HBase,2210,org.apache.hadoop.hbase.master.assignment.AssignmentManager.reportRegionStateTransition,927,warn,"java.lang.String(name)The region server {} is already dead, skip reportRegionStateTransition call-org.apache.hadoop.hbase.ServerName(name)serverName",o,o,o
HBase,2214,org.apache.hadoop.hbase.master.assignment.AssignmentManager.updateRegionSplitTransition,1009,debug,"java.lang.String(name)""Split request from "" + serverName + "", parent=""+ parent+ "" splitKey=""+ Bytes.toStringBinary(splitKey)",o,o,o
HBase,2212,org.apache.hadoop.hbase.master.assignment.AssignmentManager.updateRegionTransition,966,info,java.lang.String(name)RegionServer {} {}-org.apache.hadoop.hbase.shaded.protobuf.generated.TransitionCode(name)state-java.lang.String(name)regionNode.getRegionInfo().getEncodedName(),x,o,o
HBase,2213,org.apache.hadoop.hbase.master.assignment.AssignmentManager.updateRegionTransition,968,warn,java.lang.String(name)No matching procedure found for {} transition to {}-org.apache.hadoop.hbase.master.assignment.RegionStateNode(name)regionNode-org.apache.hadoop.hbase.shaded.protobuf.generated.TransitionCode(name)state,o,o,o
HBase,2255,org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.acquireLock,410,debug,"java.lang.String(name)LockState.LOCK_EVENT_WAIT + "" "" + env.getProcedureScheduler().dumpLocks()",x,x,x
HBase,2263,org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase.execute,287,warn,"java.lang.String(name)Can not add remote operation {} for region {} to server {}, this usually because the server is alread dead, give up and mark the procedure as complete, the parent procedure will take care of this.-org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase(name)this-org.apache.hadoop.hbase.client.RegionInfo(name)region-org.apache.hadoop.hbase.ServerName(name)targetServer-org.apache.hadoop.hbase.procedure2.FailedRemoteDispatchException(name)e",o,o,o
HBase,2162,org.apache.hadoop.hbase.master.assignment.RegionStates.addToOfflineRegions,629,info,"java.lang.String(name)""Added to offline, CURRENTLY NEVER CLEARED!!! "" + regionNode",o,o,x
HBase,2169,org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.acquireLock,144,debug,"java.lang.String(name)LockState.LOCK_EVENT_WAIT + "" "" + env.getProcedureScheduler().dumpLocks()",x,x,x
HBase,2297,org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.postMasterStartupInitialize,1163,warn,"java.lang.String(name)Refreshing region HDFS Block dist failed with exception, ignoring-java.lang.Exception(name)e",o,o,o
HBase,2278,org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.getTableDescriptor,223,debug,java.lang.String(name)tableName={}-org.apache.hadoop.hbase.TableName(name)tableName-java.io.FileNotFoundException(name)fnfe,x,x,x
HBase,2285,org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.balanceCluster,281,debug,java.lang.String(name)strBalanceParam.toString(),x,x,x
HBase,2288,org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.balanceCluster,447,info,"java.lang.String(name)""Done. Calculated a load balance in "" + (endTime - startTime) + ""ms. ""+ ""Moving ""+ totalNumMoved+ "" regions off of ""+ serversOverloaded+ "" overloaded servers onto ""+ serversUnderloaded+ "" less loaded servers""",o,o,o
HBase,2287,org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.balanceCluster,443,warn,"java.lang.String(name)""Input "" + sb.toString()",o,x,o
HBase,2290,org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.balanceOverall,502,warn,"java.lang.String(name)""Encounter incorrect region numbers after calculating move plan during balanceOverall, "" + ""for this table, "" + serverload.getServerName() + "" originally has ""+ balanceInfo.getHriList().size()+ "" regions and ""+ balanceInfo.getNumRegionsAdded()+ "" regions have been added. Yet, max =""+ max+ "", min =""+ min+ "". Thus stop balance for this table""",o,o,o
HBase,2274,org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster,441,info,java.lang.String(name)Finished computing new load balance plan. Computation took {} to try {} different iterations. Found a solution that moves {} regions; Going from a computed cost of {} to a new cost of {}-java.time.Duration(name)java.time.Duration.ofMillis(endTime - startTime)-long(name)step-int(name)plans.size()-double(name)initCost-double(name)currentCost,o,o,o
HBase,2275,org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster,448,info,"java.lang.String(name)Could not find a better load balance plan. Tried {} different configurations in {}, and did not find anything with a computed cost less than {}-long(name)step-java.time.Duration(name)java.time.Duration.ofMillis(endTime - startTime)-double(name)initCost",o,o,o
HBase,2271,org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.needsBalance,304,trace,"java.lang.String(name)""Skipping load balancing because balanced cluster; "" + ""total cost is "" + total + "", sum multiplier is ""+ sumMultiplier+ "" min cost which need balance is ""+ minCostNeedBalance",o,o,o
HBase,1824,org.apache.hadoop.hbase.master.CatalogJanitor.metaTableConsistencyCheck,599,warn,java.lang.String(name)INCONSISTENCY: Row name is not equal to serialized info:regioninfo content; row={} {}; See if RegionInfo is referenced in another hbase:meta row? Delete?-java.lang.String(name)Bytes.toStringBinary(metaTableRow.getRow())-java.lang.String(name)ri.getRegionNameAsString(),x,o,x
HBase,2348,org.apache.hadoop.hbase.master.cleaner.CleanerChore.calculatePoolSize,127,debug,"java.lang.String(name)Computed {} threads for CleanerChore, using 1 instead-int(name)computedThreads",o,o,o
HBase,2347,org.apache.hadoop.hbase.master.cleaner.CleanerChore.calculatePoolSize,119,warn,"java.lang.String(name)Use full core processors to scan dir, size={}-int(name)size",o,o,o
HBase,2363,org.apache.hadoop.hbase.master.cleaner.CleanerChore.cleanup,347,warn,java.lang.String(name)Stopping-java.lang.Throwable(name)t,o,x,o
HBase,2355,org.apache.hadoop.hbase.master.cleaner.CleanerChore.runCleaner,221,info,java.lang.String(name)Failed to traverse and delete the dir: {}-org.apache.hadoop.fs.Path(name)oldFileDir-java.lang.Exception(name)e,o,o,o
HBase,2364,org.apache.hadoop.hbase.master.cleaner.CleanerChore.traverseAndDelete,427,debug,java.lang.String(name)Failed to traverse and delete the path: {}-org.apache.hadoop.fs.Path(name)dir-java.lang.Exception(name)e,o,o,o
HBase,2344,org.apache.hadoop.hbase.master.cleaner.DirScanPool.tryUpdatePoolSize,112,info,java.lang.String(name)Update chore's pool size from {} to {}-int(name)pool.getPoolSize()-int(name)size,o,o,o
HBase,2380,org.apache.hadoop.hbase.master.cleaner.LogCleaner.deleteFile,171,debug,java.lang.String(name)Attempting to delete old WAL file: {}-org.apache.hadoop.fs.FileStatus(name)oldWalFile,o,o,x
HBase,2383,org.apache.hadoop.hbase.master.cleaner.LogCleaner.deleteFile,187,debug,java.lang.String(name)Exiting,o,x,o
HBase,2385,org.apache.hadoop.hbase.master.cleaner.LogCleaner.getResult,220,warn,java.lang.String(name)Spend too much time [{}ms] to delete old WAL file: {}-long(name)waitIfNotFinished-org.apache.hadoop.fs.FileStatus(name)target,x,o,o
HBase,1958,org.apache.hadoop.hbase.master.DrainingServerTracker.add,97,info,"java.lang.String(name)""Draining RS node created, adding to list ["" + sn + ""]""",o,o,o
HBase,1959,org.apache.hadoop.hbase.master.DrainingServerTracker.nodeDeleted,115,info,"java.lang.String(name)""Draining RS node deleted, removing from list ["" + sn + ""]""",o,o,o
HBase,1869,org.apache.hadoop.hbase.master.HMaster.checkUnsupportedProcedure,827,error,"java.lang.String(name)Unsupported procedure type {} found, please rollback your master to the old version to finish them, and then try to upgrade again. The full procedure list: {}-java.lang.Class<>(name)clazz-java.util.List<Procedure<MasterProcedureEnv>>(name)procs",o,o,o
HBase,1877,org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization,1181,debug,"java.lang.String(name)""Balancer post startup initialization complete, took "" + ((System.currentTimeMillis() - start) / 1000) + "" seconds""",o,o,o
HBase,1879,org.apache.hadoop.hbase.master.HMaster.initMobCleaner,1280,info,"java.lang.String(name)""The period is "" + mobCompactionPeriod + "" seconds, MobCompactionChore is disabled""",o,o,o
HBase,1908,org.apache.hadoop.hbase.master.HMaster.move,1971,debug,"java.lang.String(name)""Skipping move of region "" + hri.getRegionNameAsString() + "" because region already assigned to the same server ""+ dest+ "".""",o,o,o
HBase,1895,org.apache.hadoop.hbase.master.HMaster.normalizeRegions,1793,debug,"java.lang.String(name)Master has not been initialized, don't run region normalizer.",o,o,o
HBase,1930,org.apache.hadoop.hbase.master.HMaster.shutdown,2738,error,java.lang.String(name)ZooKeeper exception trying to set cluster as down in ZK-org.apache.zookeeper.KeeperException(name)e,o,o,o
HBase,2000,org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster,238,info,java.lang.String(name)Won't bring the Master up as a shutdown is requested,o,o,o
HBase,2394,org.apache.hadoop.hbase.master.locking.LockProcedure.acquireLock,313,debug,"java.lang.String(name)""LOCKED "" + toString()",o,x,x
HBase,2393,org.apache.hadoop.hbase.master.locking.LockProcedure.execute,243,debug,"java.lang.String(name)(unlock.get() ? ""UNLOCKED "" : ""TIMED OUT "") + toString()",o,x,x
HBase,2390,org.apache.hadoop.hbase.master.locking.LockProcedure.updateHeartBeat,195,debug,"java.lang.String(name)""Heartbeat "" + toString()",o,x,o
HBase,2012,org.apache.hadoop.hbase.master.MasterFileSystem.bootstrap,411,error,java.lang.String(name)bootstrap-java.io.IOException(name)e,o,x,x
HBase,2145,org.apache.hadoop.hbase.master.MasterWalManager.getLogDirs,315,info,"java.lang.String(name)""Log dir for server "" + serverName + "" does not exist""",o,o,o
HBase,2526,org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.waitFor,217,debug,"java.lang.String(name)""waitFor "" + purpose",x,x,x
HBase,2525,org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.waitFor,215,trace,"java.lang.String(name)""waitFor "" + purpose",x,x,x
HBase,2430,org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.scheduleForRetry,241,warn,"java.lang.String(name)waiting a little before trying on the same server={}, try={}, can wait up to {}ms-org.apache.hadoop.hbase.ServerName(name)serverName-int(name)numberOfAttemptsSoFar-long(name)remainingTime",x,o,x
HBase,2436,org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.scheduleForRetry,282,warn,"java.lang.String(name)request to server {} failed due to {}, try={}, retrying...-org.apache.hadoop.hbase.ServerName(name)serverName-java.lang.String(name)e.toString()-int(name)numberOfAttemptsSoFar",o,o,x
HBase,2469,org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.cleanupSplitDir,250,warn,"java.lang.String(name)remove WAL directory of server {} failed, ignore...-org.apache.hadoop.hbase.ServerName(name)serverName-java.io.IOException(name)e",o,o,x
HBase,2422,org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure.execute,84,warn,"java.lang.String(name)Can not send remote operation {} to {}, this operation will be retried to send to another server-long(name)this.getProcId()-org.apache.hadoop.hbase.ServerName(name)targetServer",o,o,o
HBase,2127,org.apache.hadoop.hbase.master.RegionPlacementMaintainer.main,1096,error,"java.lang.String(name)""Cannot find the region "" + regionName + "" from the META""",o,o,o
HBase,2114,org.apache.hadoop.hbase.master.RegionPlacementMaintainer.updateAssignmentPlanToRegionServers,662,info,java.lang.String(name)Start to update the region servers with the new assignment plan,o,o,o
HBase,1848,org.apache.hadoop.hbase.master.RegionServerTracker.getServerInfo,97,warn,"java.lang.String(name)Invalid data for region server node {} on zookeeper, data length = {}-java.lang.String(name)name-int(name)data.length",o,o,o
HBase,2537,org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure.complete,117,info,java.lang.String(name)Refresh peer {} for {} on {} suceeded-java.lang.String(name)peerId-org.apache.hadoop.hbase.master.procedure.PeerOperationType(name)type-org.apache.hadoop.hbase.ServerName(name)targetServer,o,o,o
HBase,2536,org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure.complete,114,warn,java.lang.String(name)Refresh peer {} for {} on {} failed-java.lang.String(name)peerId-org.apache.hadoop.hbase.master.procedure.PeerOperationType(name)type-org.apache.hadoop.hbase.ServerName(name)targetServer-java.lang.Throwable(name)error,o,o,o
HBase,2533,org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure.postPeerModification,197,info,java.lang.String(name)Successfully updated peer config of {} to {}-java.lang.String(name)peerId-org.apache.hadoop.hbase.replication.ReplicationPeerConfig(name)peerConfig,o,o,o
HBase,2570,org.apache.hadoop.hbase.master.snapshot.SnapshotManager.checkSnapshotSupport,1115,error,"java.lang.String(name)""Snapshots from an earlier release were found under: "" + oldSnapshotDir",o,o,o
HBase,2574,org.apache.hadoop.hbase.master.snapshot.SnapshotManager.checkSnapshotSupport,1158,error,"java.lang.String(name)Snapshots are present, but cleaners are not enabled.",o,o,o
HBase,2559,org.apache.hadoop.hbase.master.snapshot.SnapshotManager.takeSnapshotInternal,661,debug,"java.lang.String(name)Table is disabled, running snapshot entirely on master.",o,o,o
HBase,2041,org.apache.hadoop.hbase.master.SplitLogManager.createTaskIfAbsent,428,error,org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)Logic error. Deleted task still present in tasks map,o,o,o
HBase,2029,org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed,283,warn,"java.lang.String(name)""Returning success without actually splitting and "" + ""deleting all the log files in path "" + logDir + "": ""+ Arrays.toString(files)-java.io.IOException(name)ioe",o,o,o
HBase,2034,org.apache.hadoop.hbase.master.SplitLogManager.waitForSplittingCompletion,336,warn,"java.lang.String(name)""No more task remaining, splitting "" + ""should have completed. Remaining tasks is "" + remainingTasks + "", active tasks in map ""+ actual",o,o,o
HBase,2095,org.apache.hadoop.hbase.master.SplitWALManager.splitWALs,99,error,java.lang.String(name)failed to create procedures for splitting logs of {}-org.apache.hadoop.hbase.ServerName(name)crashedServer-java.io.IOException(name)e,o,o,x
HBase,15,org.apache.hadoop.hbase.MetaTableAccessor.debugLogMutation,2185,debug,java.lang.String(name){} {}-java.lang.String(name)p.getClass().getSimpleName()-java.lang.String(name)p.toJSON(),x,x,x
HBase,10,org.apache.hadoop.hbase.MetaTableAccessor.deleteRegionInfos,1865,info,java.lang.String(name)Deleted {} regions from META-int(name)regionsInfo.size(),o,o,o
HBase,4637,org.apache.hadoop.hbase.MiniHBaseCluster.killDataNode,336,warn,java.lang.String(name)Aborting datanodes on mini cluster is not supported,o,o,x
HBase,4642,org.apache.hadoop.hbase.MiniHBaseCluster.killNameNode,361,warn,java.lang.String(name)Aborting namenodes on mini cluster is not supported,o,o,x
HBase,4630,org.apache.hadoop.hbase.MiniHBaseCluster.killRegionServer,281,info,"java.lang.String(name)""Killing "" + server.toString()",o,x,o
HBase,4636,org.apache.hadoop.hbase.MiniHBaseCluster.startDataNode,331,warn,java.lang.String(name)Starting datanodes on mini cluster is not supported,o,o,o
HBase,4641,org.apache.hadoop.hbase.MiniHBaseCluster.startNameNode,356,warn,java.lang.String(name)Starting namenodes on mini cluster is not supported,o,o,o
HBase,4638,org.apache.hadoop.hbase.MiniHBaseCluster.stopDataNode,341,warn,java.lang.String(name)Stopping datanodes on mini cluster is not supported,o,o,o
HBase,4643,org.apache.hadoop.hbase.MiniHBaseCluster.stopNameNode,366,warn,java.lang.String(name)Stopping namenodes on mini cluster is not supported,o,o,o
HBase,2650,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.closeMobFileWriter,855,error,"java.lang.String(name)""Failed to close the writer of the file "" + writer.getPath()-java.io.IOException(name)e",o,o,o
HBase,2646,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.closeStoreFileReaders,584,warn,"java.lang.String(name)""Failed to close the reader on store file "" + storeFile.getPath()-java.io.IOException(name)e",o,o,o
HBase,2648,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.compactDelFilesInBatch,788,error,"java.lang.String(name)""Failed to close the writer of the file "" + filePath-java.io.IOException(name)e",o,o,o
HBase,2643,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.compactMobFiles,498,error,"java.lang.String(name)""Failed to compact the partition "" + result.getKey()-java.lang.Exception(name)e",o,o,o
HBase,2644,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.compactMobFiles,510,error,java.lang.String(name)Failed to close the Table-java.io.IOException(name)e,o,o,o
HBase,2647,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.compactMobFilesInBatch,689,error,"java.lang.String(name)""Failed to archive the files "" + mobFilesToCompact-java.io.IOException(name)e",o,o,o
HBase,2652,org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.deletePath,912,error,"java.lang.String(name)""Failed to delete the file "" + path-java.io.IOException(name)e",o,o,o
HBase,2630,org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor.performCompaction,314,debug,"java.lang.String(name)Compaction progress: {} {}, rate={} KB/sec, throughputController is {}-java.lang.String(name)compactionName-org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress(name)progress-java.lang.String(name)rate-org.apache.hadoop.hbase.regionserver.throttle.ThroughputController(name)throughputController",x,o,o
HBase,4660,org.apache.hadoop.hbase.PerformanceEvaluationCommons.concurrentReads,84,info,"java.lang.String(name)""Test took "" + (System.currentTimeMillis() - now)",o,x,o
HBase,2715,org.apache.hadoop.hbase.procedure.ProcedureCoordinator.abortProcedure,223,debug,"java.lang.String(name)""abort procedure "" + procName-org.apache.hadoop.hbase.errorhandling.ForeignException(name)reason",o,o,x
HBase,2663,org.apache.hadoop.hbase.procedure.ProcedureMember.submitSubprocedure,125,warn,"java.lang.String(name)Submitted null subprocedure, nothing to run here.",o,o,o
HBase,4680,org.apache.hadoop.hbase.procedure.SimpleMasterProcedureManager.stop,52,info,"java.lang.String(name)""stop: "" + why",o,x,x
HBase,4677,org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager.abort,189,warn,"java.lang.String(name)""Aborting because: "" + why-java.lang.Throwable(name)e",o,x,o
HBase,4675,org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager.buildSubprocedure,112,info,"java.lang.String(name)""Building procedure: "" + name",o,o,o
HBase,4671,org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager.initialize,60,info,"java.lang.String(name)""Initialized: "" + rss.getServerName().toString()",o,x,o
HBase,4672,org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager.start,66,info,java.lang.String(name)Started.,o,x,o
HBase,4673,org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager.stop,71,info,"java.lang.String(name)""stop: "" + force",o,x,x
HBase,2695,org.apache.hadoop.hbase.procedure.ZKProcedureCoordinator.start,198,debug,"java.lang.String(name)""Node created: "" + path",o,o,o
HBase,2681,org.apache.hadoop.hbase.procedure.ZKProcedureUtil.logZKTree,248,debug,java.lang.String(name)prefix + root,x,x,x
HBase,2682,org.apache.hadoop.hbase.procedure.ZKProcedureUtil.logZKTree,265,debug,java.lang.String(name)prefix + child,x,x,x
HBase,975,org.apache.hadoop.hbase.procedure2.Procedure.doExecute,959,info,"java.lang.String(name){} bypassed, returning null to finish it-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)this",o,o,o
HBase,978,org.apache.hadoop.hbase.procedure2.Procedure.restoreLock,992,debug,"java.lang.String(name){} is already finished, skip acquiring lock.-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)this",o,o,o
HBase,979,org.apache.hadoop.hbase.procedure2.Procedure.restoreLock,997,debug,"java.lang.String(name){} is already bypassed, skip acquiring lock.-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)this",o,o,o
HBase,1022,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.bypassProcedure,899,debug,"java.lang.String(name)Procedure pid={} does not exist, skipping bypass-long(name)pid",o,o,o
HBase,1065,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execCompletionCleanup,1867,warn,"java.lang.String(name)Usually this should not happen, we will release the lock before if the procedure is finished, even if the holdLock is true, arrive here means we have some holes where we do not release the lock. And the releaseLock below may fail since the procedure may have already been deleted from the procedure store.",o,x,o
HBase,1059,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure,1689,trace,java.lang.String(name)Added to timeoutExecutor {}-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)procedure,o,x,o
HBase,1009,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.forceUpdateProcedure,285,debug,java.lang.String(name)Force update procedure {}-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)proc,o,o,o
HBase,1064,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.handleInterruptedException,1856,trace,java.lang.String(name)Interrupt during {}. suspend and retry it later.-org.apache.hadoop.hbase.procedure2.Procedure<TEnvironment>(name)proc-java.lang.InterruptedException(name)e,o,o,o
HBase,1038,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.sendProcedureLoadedNotification,1262,error,"java.lang.String(name)""Listener "" + listener + "" had an error: ""+ e.getMessage()-java.lang.Throwable(name)e",o,o,o
HBase,1019,org.apache.hadoop.hbase.procedure2.ProcedureExecutor.stop,624,info,java.lang.String(name)Stopping,o,x,o
HBase,1002,org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.awaitTermination,341,warn,"java.lang.String(name)""Waiting termination of thread "" + getName() + "", ""+ StringUtils.humanTimeDiff(EnvironmentEdgeManager.currentTime() - startTime)",o,o,o
HBase,1003,org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.awaitTermination,346,warn,"java.lang.String(name)getName() + "" join wait got interrupted""-java.lang.InterruptedException(name)e",o,o,x
HBase,987,org.apache.hadoop.hbase.procedure2.StateMachineProcedure.abort,232,debug,"java.lang.String(name)Abort requested for {}-org.apache.hadoop.hbase.procedure2.StateMachineProcedure<TEnvironment,TState>(name)this",o,o,o
HBase,986,org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute,193,trace,"java.lang.String(name){}-org.apache.hadoop.hbase.procedure2.StateMachineProcedure<TEnvironment,TState>(name)this",x,x,x
HBase,1145,org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.initOldLog,1372,warn,java.lang.String(name)Remove uninitialized log: {}-org.apache.hadoop.fs.FileStatus(name)logFile,o,o,o
HBase,1111,org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.insert,584,error,"org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)""Unable to serialize one of the procedure: "" + Arrays.toString(procs)-java.io.IOException(name)e",o,o,o
HBase,1101,org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.recoverLease,419,warn,java.lang.String(name)Someone else is active and deleted logs. retrying.-java.io.FileNotFoundException(name)e,o,o,o
HBase,1106,org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.tryCleanupLogsOnLoad,518,debug,"java.lang.String(name)""WALs cleanup on load is not enabled: "" + getActiveLogs()",o,o,o
HBase,1095,org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.WALProcedureStore,271,warn,java.lang.String(name)Failed create of {}-org.apache.hadoop.fs.Path(name)this.walArchiveDir,o,o,o
HBase,2883,org.apache.hadoop.hbase.quotas.MasterQuotaManager.switchRpcThrottle,374,info,java.lang.String(name){} switch rpc throttle from {} to {}-java.lang.String(name)masterServices.getClientIdAuditPrefix()-boolean(name)oldRpcThrottle-boolean(name)rpcThrottle,o,o,x
HBase,2895,org.apache.hadoop.hbase.quotas.policies.NoWritesCompactionsViolationPolicyEnforcement.disable,51,trace,java.lang.String(name)Compactions were already enabled upon disabling the policy,o,o,o
HBase,2894,org.apache.hadoop.hbase.quotas.policies.NoWritesCompactionsViolationPolicyEnforcement.enable,43,trace,java.lang.String(name)Compactions were already disabled upon enabling the policy,o,o,o
HBase,2832,org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager.switchRpcThrottle,102,warn,java.lang.String(name)Skip switch rpc throttle because previous value {} is the same as current value {}-boolean(name)rpcThrottleEnabled-boolean(name)enable,o,o,x
HBase,2837,org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager.start,74,info,"java.lang.String(name)Quota support disabled, not starting space quota manager.",o,o,o
HBase,2844,org.apache.hadoop.hbase.quotas.SnapshotQuotaObserverChore.chore,118,warn,"java.lang.String(name)Failed to compute the size of snapshots, will retry-java.io.IOException(name)e",o,o,o
HBase,3410,org.apache.hadoop.hbase.regionserver.BusyRegionSplitPolicy.shouldSplit,116,debug,"java.lang.String(name)""Going to split region "" + region.getRegionInfo().getRegionNameAsString() + "" because it's too busy. Blocked Request rate: ""+ blockedReqRate",o,o,o
HBase,3445,org.apache.hadoop.hbase.regionserver.ChunkCreator.onHeapMemoryTune,453,info,java.lang.String(name){} max count for chunks increased from {} to {}-java.lang.String(name)this.label-int(name)this.maxCount-int(name)newMaxCount,o,o,o
HBase,3446,org.apache.hadoop.hbase.regionserver.ChunkCreator.onHeapMemoryTune,459,info,java.lang.String(name){} max count for chunks decreased from {} to {}-java.lang.String(name)this.label-int(name)this.maxCount-int(name)newMaxCount,o,o,o
HBase,3444,org.apache.hadoop.hbase.regionserver.ChunkCreator.onHeapMemoryTune,443,warn,java.lang.String(name){} not tuning the chunk pool as it is offheap-java.lang.String(name)label,o,o,o
HBase,3539,org.apache.hadoop.hbase.regionserver.compactions.Compactor.getFileDetails,201,debug,"java.lang.String(name)Compacting {}, keycount={}, bloomtype={}, size={}, encoding={}, compression={}, seqNum={}{}-java.lang.Object(name)(file.getPath() == null ? null : file.getPath().getName())-long(name)keyCount-java.lang.String(name)r.getBloomFilterType().toString()-java.lang.String(name)TraditionalBinaryPrefix.long2String(r.length(),"""",1)-org.apache.hadoop.hbase.io.encoding.DataBlockEncoding(name)r.getHFileReader().getDataBlockEncoding()-org.apache.hadoop.hbase.io.compress.Algorithm(name)compactionCompression-long(name)seqNum-java.lang.Object(name)(allFiles ? "", earliestPutTs="" + earliestPutTs : """")",x,o,o
HBase,3540,org.apache.hadoop.hbase.regionserver.compactions.Compactor.performCompaction,453,debug,"java.lang.String(name)Compaction progress: {} {}, rate={} KB/sec, throughputController is {}-java.lang.String(name)compactionName-org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress(name)progress-java.lang.String(name)rate-org.apache.hadoop.hbase.regionserver.throttle.ThroughputController(name)throughputController",x,o,o
HBase,3504,org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy.shouldPerformMajorCompaction,165,debug,"java.lang.String(name)""Major compaction triggered on store "" + this + ""; because there are more than one file in some windows""",o,o,o
HBase,3104,org.apache.hadoop.hbase.regionserver.CompactSplit.doCompaction,651,debug,java.lang.String(name)Status {}-org.apache.hadoop.hbase.regionserver.CompactSplit(name)CompactSplit.this,o,x,o
HBase,3089,org.apache.hadoop.hbase.regionserver.CompactSplit.requestSplit,232,info,"java.lang.String(name)""Could not execute split for "" + r-java.util.concurrent.RejectedExecutionException(name)ree",o,o,o
HBase,3095,org.apache.hadoop.hbase.regionserver.CompactSplit.selectCompaction,404,debug,java.lang.String(name)reason,x,x,x
HBase,3098,org.apache.hadoop.hbase.regionserver.CompactSplit.shouldSplitRegion,465,warn,"java.lang.String(name)""Total number of regions is approaching the upper limit "" + regionSplitLimit + "". ""+ ""Please consider taking a look at http://hbase.apache.org/book.html#ops.regionmgt""",o,o,o
HBase,4692,org.apache.hadoop.hbase.regionserver.CreateRandomStoreFile.main,316,error,java.lang.String(name)ex.toString()-java.io.IOException(name)ex,x,x,x
HBase,4683,org.apache.hadoop.hbase.regionserver.CreateRandomStoreFile.run,129,error,java.lang.String(name)ex.toString()-org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException(name)ex,x,x,x
HBase,4699,org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.main,733,error,"java.lang.String(name)""The number of times to run each benchmark ("" + benchmarkNTimes + "") must be greater than the number of benchmark runs to exclude ""+ ""from statistics (""+ benchmarkNOmit+ "")""",o,o,o
HBase,4700,org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.main,739,info,"java.lang.String(name)""Running benchmark "" + benchmarkNTimes + "" times. ""+ ""Excluding the first ""+ benchmarkNOmit+ "" times from statistics.""",o,o,o
HBase,4694,org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.verifyCodecs,290,error,"java.lang.String(name)""There is bug in codec "" + it.toString() + ""\n on element ""+ j+ ""\n codecKv.getKeyLength() ""+ codecKv.getKeyLength()+ ""\n codecKv.getValueLength() ""+ codecKv.getValueLength()+ ""\n codecKv.getLength() ""+ codecKv.getLength()+ ""\n currentKv.getKeyLength() ""+ currentKv.getKeyLength()+ ""\n currentKv.getValueLength() ""+ currentKv.getValueLength()+ ""\n codecKv.getLength() ""+ currentKv.getLength()+ ""\n currentKV rowLength ""+ currentKv.getRowLength()+ "" familyName ""+ currentKv.getFamilyLength()+ "" qualifier ""+ currentKv.getQualifierLength()+ ""\n prefix ""+ prefix+ ""\n codecKv '""+ Bytes.toStringBinary(codecKv.getBuffer(),codecKv.getOffset(),prefix)+ ""' diff '""+ Bytes.toStringBinary(codecKv.getBuffer(),codecKv.getOffset() + prefix,codecKv.getLength() - prefix)+ ""'""+ ""\n currentKv '""+ Bytes.toStringBinary(currentKv.getBuffer(),currentKv.getOffset(),prefix)+ ""' diff '""+ Bytes.toStringBinary(currentKv.getBuffer(),currentKv.getOffset() + prefix,currentKv.getLength() - prefix)+ ""'""",x,x,x
HBase,4695,org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.verifyCodecs,320,info,java.lang.String(name)Verification was successful!,o,x,x
HBase,3270,org.apache.hadoop.hbase.regionserver.DefaultMemStore.main,204,info,"java.lang.String(name)""vmInputArguments="" + runtime.getInputArguments()",x,x,x
HBase,3379,org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy.getSplitPoint,72,warn,"java.lang.String(name)""Delimiter "" + Bytes.toString(delimiter) + "" not found for split key ""+ Bytes.toString(splitPoint)",o,o,o
HBase,3453,org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy.setFlushSizeLowerBounds,73,warn,"java.lang.String(name)Number format exception parsing {} for table {}: {}, {}; using region.getMemStoreFlushHeapSize/# of families ({}) and region.getMemStoreFlushOffHeapSize/# of families ({}) instead.-java.lang.String(name)hbase.hregion.percolumnfamilyflush.size.lower.bound-org.apache.hadoop.hbase.TableName(name)region.getTableDescriptor().getTableName()-java.lang.String(name)flushedSizeLowerBoundString-java.lang.NumberFormatException(name)nfe-long(name)flushSizeLowerBound",o,o,o
HBase,3576,org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler.process,116,info,"java.lang.String(name)Receiving OPEN for the region:{}, which we are trying to close, try again after {}ms-java.lang.String(name)regionName-long(name)backoff",x,o,o
HBase,3584,org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler.process,99,debug,"java.lang.String(name)Received CLOSE for a region {} which is not online, and we're not opening/closing.-java.lang.String(name)encodedName",o,o,o
HBase,3582,org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler.process,88,warn,"java.lang.String(name)Received CLOSE for the region: {}, which we are already trying to OPEN. try again after {}ms-java.lang.String(name)encodedName-long(name)backoff",x,o,o
HBase,2938,org.apache.hadoop.hbase.regionserver.HRegion.compact,2227,info,"java.lang.String(name)Starting compaction of {} in {}{}-org.apache.hadoop.hbase.regionserver.HStore(name)store-org.apache.hadoop.hbase.regionserver.HRegion(name)this-java.lang.Object(name)(compaction.getRequest().isOffPeak() ? "" as an off-peak compaction"" : """")",x,o,o
HBase,2972,org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsForPaths,4692,error,"java.lang.String(name)HConstants.HREGION_EDITS_REPLAY_SKIP_ERRORS + ""=true so continuing. Renamed "" + edits+ "" as ""+ p-java.io.IOException(name)e",x,x,o
HBase,3471,org.apache.hadoop.hbase.regionserver.HRegionFileSystem.createRegionOnFileSystem,991,warn,"java.lang.String(name)""Unable to create the region directory: "" + regionDir",o,o,o
HBase,3359,org.apache.hadoop.hbase.regionserver.HRegionServer.addToMovedRegions,3472,warn,"java.lang.String(name)""Not adding moved region record: "" + encodedName + "" to self.""",o,o,o
HBase,3352,org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion,3202,warn,"java.lang.String(name)""The opening for region "" + encodedName + "" was done before we could cancel it.""+ "" Doing a standard close now""",o,o,o
HBase,3335,org.apache.hadoop.hbase.regionserver.HRegionServer.createRegionServerStatusStub,2615,info,"java.lang.String(name)Master isn't available yet, retrying",o,o,o
HBase,3339,org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty,2682,debug,java.lang.String(name)Master is not running yet,o,o,o
HBase,3365,org.apache.hadoop.hbase.regionserver.HRegionServer.run,3864,warn,"java.lang.String(name)Aborting region server timed out, terminating forcibly and does not wait for any running shutdown hooks or finalizers to finish their work. Thread dump to stdout.",o,o,o
HBase,3310,org.apache.hadoop.hbase.regionserver.HRegionServer.setupWALAndReplication,1856,debug,java.lang.String(name)logDir={}-org.apache.hadoop.fs.Path(name)logDir,x,x,x
HBase,3316,org.apache.hadoop.hbase.regionserver.HRegionServer.stop,2205,warn,java.lang.String(name)The region server did not stop-java.io.IOException(name)ioe,o,o,o
HBase,3172,org.apache.hadoop.hbase.regionserver.HStore.closeAndArchiveCompactedFiles,2556,trace,java.lang.String(name)No compacted files to archive,o,o,o
HBase,3168,org.apache.hadoop.hbase.regionserver.HStore.getCompactPriority,2181,warn,java.lang.String(name)Compaction priority is USER despite there being no user compaction,o,o,o
HBase,3129,org.apache.hadoop.hbase.regionserver.HStore.HStore,272,trace,java.lang.String(name)Time to purge deletes set to {}ms in store {}-long(name)timeToPurgeDeletes-org.apache.hadoop.hbase.regionserver.HStore(name)this,x,o,o
HBase,3176,org.apache.hadoop.hbase.regionserver.HStore.removeCompactedfiles,2601,error,java.lang.String(name)Exception while trying to close the compacted store file {}-org.apache.hadoop.fs.Path(name)file.getPath()-java.lang.Exception(name)e,o,o,o
HBase,3237,org.apache.hadoop.hbase.regionserver.HStoreFile.open,464,error,java.lang.String(name)Error reading timestamp range data from meta -- proceeding without-java.lang.IllegalArgumentException(name)e,o,x,o
HBase,3261,org.apache.hadoop.hbase.regionserver.MemStoreFlusher.logMsg,775,info,"java.lang.String(name)Blocking updates: {} {} is >= blocking {}-java.lang.String(name)type-java.lang.String(name)TraditionalBinaryPrefix.long2String(val,"""",1)-java.lang.String(name)TraditionalBinaryPrefix.long2String(max,"""",1)",x,x,o
HBase,3252,org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run,382,info,"java.lang.String(name)getName() + "" exiting""",o,o,x
HBase,3202,org.apache.hadoop.hbase.regionserver.RSRpcServices.flushRegion,1706,info,"java.lang.String(name)""Flushing "" + region.getRegionInfo().getRegionNameAsString()",o,x,o
HBase,3194,org.apache.hadoop.hbase.regionserver.RSRpcServices.logBatchWarning,1192,warn,"java.lang.String(name)""Large batch operation detected (greater than "" + rowSizeWarnThreshold + "") (HBASE-18023).""+ "" Requested Number of Rows: ""+ sum+ "" Client: ""+ RpcServer.getRequestUserName().orElse(null)+ ""/""+ RpcServer.getRemoteAddress().orElse(null)+ "" first region in multi=""+ firstRegionName",o,o,o
HBase,3224,org.apache.hadoop.hbase.regionserver.RSRpcServices.scan,3318,debug,"java.lang.String(name)""Server shutting down and client tried to access missing scanner "" + scannerName",o,o,o
HBase,3218,org.apache.hadoop.hbase.regionserver.RSRpcServices.skipCellsForMutation,2786,error,java.lang.String(name)Error while skipping Cells in CellScanner for invalid Region Mutations-java.io.IOException(name)e,o,o,o
HBase,3440,org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.failedBulkLoad,436,warn,"java.lang.String(name)""Can't find previous permission for path="" + srcPath",o,o,o
HBase,3419,org.apache.hadoop.hbase.regionserver.ShutdownHook.suppressHdfsShutdownHook,219,error,org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)Couldn't access field 'clientFinalizer' in FileSystem!-java.lang.IllegalAccessException(name)iae,o,o,o
HBase,3118,org.apache.hadoop.hbase.regionserver.SplitWALCallable.call,89,warn,java.lang.String(name)failed to split WAL {}.-java.lang.String(name)walPath-java.io.IOException(name)e,o,o,x
HBase,3085,org.apache.hadoop.hbase.regionserver.StoreScanner.trySwitchToStreamRead,1042,debug,java.lang.String(name)Switch to stream read (scanned={} bytes) of {}-long(name)bytesRead-java.lang.String(name)this.store.getColumnFamilyName(),o,o,o
HBase,3054,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.debugDumpState,540,debug,java.lang.String(name)sb.toString(),x,x,x
HBase,3058,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.findExpiredFiles,1038,info,"java.lang.String(name)""Found an expired store file: "" + sf.getPath() + "" whose maxTimestamp is ""+ fileTs+ "", which is below ""+ maxTs",o,o,o
HBase,3049,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.loadUnclassifiedStoreFiles,395,debug,"java.lang.String(name)""Attempting to load "" + storeFiles.size() + "" store files.""",o,o,o
HBase,3050,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.loadUnclassifiedStoreFiles,408,error,"java.lang.String(name)""Unexpected metadata - start row ["" + Bytes.toString(startRow) + ""], end row [""+ Bytes.toString(endRow)+ ""] in file [""+ sf.getPath()+ ""], pushing to L0""",o,o,o
HBase,3051,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.loadUnclassifiedStoreFiles,439,warn,"java.lang.String(name)""Store file doesn't fit into the tentative stripes - expected to start at ["" + Bytes.toString(expectedStartRow) + ""], but starts at [""+ Bytes.toString(startRow)+ ""], to L0 it goes""",o,o,o
HBase,3056,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.processNewCandidateStripes,933,warn,"java.lang.String(name)""Stripes were created by a flush, but results of size "" + newSize + "" cannot be added because the stripes have changed""",o,o,o
HBase,3055,org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.processResults,840,warn,"java.lang.String(name)""The newly compacted file doesn't have stripes set: "" + sf.getPath()",o,o,o
HBase,3669,org.apache.hadoop.hbase.regionserver.wal.FSHLog.doShutdown,416,warn,"java.lang.String(name)""Timed out bringing down disruptor after "" + timeoutms + ""ms; forcing halt ""+ ""(It is a problem if this is NOT an ABORT! -- DATALOSS!!!!)""",o,o,x
HBase,3679,org.apache.hadoop.hbase.regionserver.wal.FSHLog.onEvent,1031,error,"java.lang.String(name)""UNEXPECTED!!! syncFutures.length="" + this.syncFutures.length-java.lang.Throwable(name)t",x,x,x
HBase,3672,org.apache.hadoop.hbase.regionserver.wal.FSHLog.run,590,warn,java.lang.String(name)UNEXPECTED-java.lang.Exception(name)e,x,x,x
HBase,3631,org.apache.hadoop.hbase.regionserver.wal.SequenceIdAccounting.abortCacheFlush,368,error,java.lang.String(name)errorStr,x,x,x
HBase,3721,org.apache.hadoop.hbase.replication.master.ReplicationPeerConfigUpgrader.copyTableCFs,128,warn,"java.lang.String(name)""NOTICE!! Update peerId failed, peerId="" + peerId-org.apache.zookeeper.KeeperException(name)e",o,o,x
HBase,3722,org.apache.hadoop.hbase.replication.master.ReplicationPeerConfigUpgrader.copyTableCFs,131,warn,"java.lang.String(name)""NOTICE!! Update peerId failed, peerId="" + peerId-java.lang.InterruptedException(name)e",o,o,x
HBase,3723,org.apache.hadoop.hbase.replication.master.ReplicationPeerConfigUpgrader.copyTableCFs,134,warn,"java.lang.String(name)""NOTICE!! Update peerId failed, peerId="" + peerId-java.io.IOException(name)e",o,o,x
HBase,3724,org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.sleepForRetries,202,trace,"java.lang.String(name){} {}, sleeping {} times {}-java.lang.String(name)logPeerId()-java.lang.String(name)msg-long(name)sleepForRetries-int(name)sleepMultiplier",x,o,o
HBase,3804,org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.doBulkLoad,177,warn,"java.lang.String(name)""Error occurred while replicating HFiles, retry attempt "" + count + "" with ""+ queue.size()+ "" files still remaining to replicate.""",o,o,o
HBase,3750,org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.sleepForRetries,443,trace,"java.lang.String(name){} {}, sleeping {} times {}-java.lang.String(name)logPeerId()-java.lang.String(name)msg-long(name)sleepForRetries-int(name)sleepMultiplier",x,o,o
HBase,3748,org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.tryThrottle,409,trace,java.lang.String(name){} To sleep {}ms for throttling control-java.lang.String(name)logPeerId()-long(name)sleepTicks,x,o,o
HBase,3827,org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.run,681,warn,java.lang.String(name)Interrupted while waiting before transferring a queue.,o,o,o
HBase,3799,org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader.countDistinctRowKeysAndHFiles,353,error,java.lang.String(name)Failed to deserialize bulk load entry from wal edit. Then its hfiles count will not be added into metric.,o,o,o
HBase,3764,org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.checkAllBytesParsed,231,warn,"java.lang.String(name)Processing end of WAL file '{}'. At position {}, which is too far away from reported file length {}. Restarting WAL reading (see HBASE-15983 for details). {}-org.apache.hadoop.fs.Path(name)currentPath-long(name)currentPositionOfReader-long(name)stat.getLen()-java.lang.String(name)getCurrentPathStat()",o,o,o
HBase,451,org.apache.hadoop.hbase.ResourceChecker.check,132,warn,"java.lang.String(name)ra.getName() + ""="" + cur+ "" is inferior to ""+ ra.getMin()",o,x,x
HBase,452,org.apache.hadoop.hbase.ResourceChecker.check,135,warn,"java.lang.String(name)ra.getName() + ""="" + cur+ "" is superior to ""+ ra.getMax()",o,x,x
HBase,1253,org.apache.hadoop.hbase.rest.client.Client.initialize,89,debug,"java.lang.String(name)""classpath "" + clspath",x,x,x
HBase,1186,org.apache.hadoop.hbase.rest.ScannerResource.update,115,error,"java.lang.String(name)""Exception occurred while processing "" + uriInfo.getAbsolutePath() + "" : ""-java.lang.Exception(name)e",o,o,o
HBase,1176,org.apache.hadoop.hbase.rest.TableResource.getScanResource,147,trace,"java.lang.String(name)""Query parameters : Table Name = > "" + this.table + "" Start Row => ""+ startRow+ "" End Row => ""+ endRow+ "" Columns => ""+ column+ "" Start Time => ""+ startTime+ "" End Time => ""+ endTime+ "" Cache Blocks => ""+ cacheBlocks+ "" Max Versions => ""+ maxVersions+ "" Batch Size => ""+ batchSize",x,x,o
HBase,1304,org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.balanceRSGroup,441,info,"java.lang.String(name)Creating partial plan for table {} : {}-org.apache.hadoop.hbase.TableName(name)tableMap.getKey()-java.util.Map<ServerName,List<RegionInfo>>(name)tableMap.getValue()",o,o,o
HBase,1294,org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.moveServerRegionsFromGroup,228,info,java.lang.String(name)Server {} has no more regions to move for RSGroup-java.lang.String(name)rs.getHostname(),o,o,o
HBase,1321,org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer.correctAssignments,388,debug,"java.lang.String(name)""Group not found for table "" + region.getTable() + "", using default""",o,o,o
HBase,1325,org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl.moveServers,213,debug,"java.lang.String(name)""Dropping "" + el + "" during move-to-default rsgroup because not online""",o,x,o
HBase,1326,org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl.removeServers,331,warn,"java.lang.String(name)""Server "" + el + "" does not belong to any rsgroup.""",o,o,o
HBase,3891,org.apache.hadoop.hbase.security.access.AuthManager.release,568,error,org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)msg,x,x,x
HBase,3957,org.apache.hadoop.hbase.security.token.ZKSecretWatcher.refreshNodes,156,error,"org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)""Failed reading new secret key for id '"" + keyId + ""' from zk""-java.io.IOException(name)ioe",o,o,x
HBase,3969,org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getVisibilityExpEvaluator,510,error,java.lang.String(name)t.toString()-java.lang.Throwable(name)t,x,x,x
HBase,4702,org.apache.hadoop.hbase.security.visibility.ExpAsStringVisibilityLabelServiceImpl.getVisibilityExpEvaluator,286,error,java.lang.String(name)t.toString()-java.lang.Throwable(name)t,x,x,x
HBase,3988,org.apache.hadoop.hbase.security.visibility.VisibilityController.addLabels,794,error,java.lang.String(name)User is not having required permissions to add labels-org.apache.hadoop.hbase.security.AccessDeniedException(name)e,o,o,o
HBase,3989,org.apache.hadoop.hbase.security.visibility.VisibilityController.addLabels,797,error,java.lang.String(name)e.toString()-java.io.IOException(name)e,x,x,x
HBase,3995,org.apache.hadoop.hbase.security.visibility.VisibilityController.clearAuths,973,error,java.lang.String(name)User is not having required permissions to clear authorization-org.apache.hadoop.hbase.security.AccessDeniedException(name)e,o,o,o
HBase,3990,org.apache.hadoop.hbase.security.visibility.VisibilityController.setAuths,849,error,java.lang.String(name)User is not having required permissions to set authorization-org.apache.hadoop.hbase.security.AccessDeniedException(name)e,o,o,o
HBase,3980,org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.replicate,94,error,"java.lang.String(name)""Exception while reading the visibility labels from the cell. The replication "" + ""would happen as per the existing format and not as "" + ""string type for the cell "" + cell + "".""-java.lang.Exception(name)ioe",o,o,o
HBase,3981,org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.refreshVisibilityLabelsCache,80,error,java.lang.String(name)Failed parsing data from labels table from zk-java.io.IOException(name)ioe,o,o,x
HBase,923,org.apache.hadoop.hbase.snapshot.ExportSnapshot.copyData,447,info,"java.lang.String(name)""copy completed for input="" + inputPath + "" output=""+ outputPath",o,o,x
HBase,920,org.apache.hadoop.hbase.snapshot.ExportSnapshot.preserveAttributes,376,warn,"java.lang.String(name)""Unable to set the permission for file="" + stat.getPath() + "": ""+ e.getMessage()",o,o,o
HBase,921,org.apache.hadoop.hbase.snapshot.ExportSnapshot.preserveAttributes,389,warn,"java.lang.String(name)""Unable to set the owner/group for file="" + stat.getPath() + "": ""+ e.getMessage()",o,o,o
HBase,919,org.apache.hadoop.hbase.snapshot.ExportSnapshot.preserveAttributes,365,warn,"java.lang.String(name)""Unable to get the status for file="" + path",o,o,o
HBase,4018,org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.completeSnapshot,396,debug,"java.lang.String(name)""Snapshot is done, just moving the snapshot from "" + workingDir + "" to ""+ finishedDir",o,o,o
HBase,4016,org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.validate,313,debug,"java.lang.String(name)""Creation time not specified, setting to:"" + time + "" (current time:""+ EnvironmentEdgeManager.currentTime()+ "").""",o,o,o
HBase,4851,org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer.run,299,error,java.lang.String(name)Error occurred during processing of message.-java.lang.Exception(name)x,o,o,o
HBase,4818,org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.atomicIncrement,826,warn,java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,x,x
HBase,4837,org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.checkAndPut,1266,warn,java.lang.String(name)e.getMessage()-java.lang.IllegalArgumentException(name)e,x,x,x
HBase,4817,org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs,799,warn,java.lang.String(name)e.getMessage()-java.lang.IllegalArgumentException(name)e,x,x,x
HBase,4822,org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.scannerGetList,852,warn,java.lang.String(name)message,x,x,x
HBase,4862,org.apache.hadoop.hbase.thrift.ThriftServer.getProtocolFactory,605,debug,java.lang.String(name)Using compact protocol,o,o,o
HBase,4863,org.apache.hadoop.hbase.thrift.ThriftServer.getProtocolFactory,608,debug,java.lang.String(name)Using binary protocol,o,o,o
HBase,4859,org.apache.hadoop.hbase.thrift.ThriftServer.getTHsHaServer,548,info,"java.lang.String(name)""starting HBase HsHA Thrift server on "" + inetSocketAddress.toString()",o,o,x
HBase,4858,org.apache.hadoop.hbase.thrift.ThriftServer.getTNonBlockingServer,537,info,"java.lang.String(name)""starting HBase Nonblocking Thrift server on "" + inetSocketAddress.toString()",o,o,x
HBase,4860,org.apache.hadoop.hbase.thrift.ThriftServer.getTThreadedSelectorServer,565,info,"java.lang.String(name)""starting HBase ThreadedSelector Thrift server on "" + inetSocketAddress.toString()",o,o,x
HBase,4861,org.apache.hadoop.hbase.thrift.ThriftServer.getTThreadPoolServer,584,info,"java.lang.String(name)""starting HBase ThreadPool Thrift server on "" + inetSocketAddress.toString()",o,o,x
HBase,4853,org.apache.hadoop.hbase.thrift.ThriftServer.setupHTTPServer,420,info,java.lang.String(name)Starting Thrift HTTP Server on {}-java.lang.String(name)Integer.toString(listenPort),o,o,o
HBase,4857,org.apache.hadoop.hbase.thrift.ThriftServer.setupServer,516,info,java.lang.String(name)starting HBase {} server on {}-java.lang.String(name)implType.simpleClassName()-java.lang.String(name)Integer.toString(listenPort),o,o,x
HBase,5140,org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.closeScanner,463,warn,java.lang.String(name)message,x,x,x
HBase,4107,org.apache.hadoop.hbase.tool.Canary.call,513,debug,java.lang.String(name)Reading from {} {} {} {}-java.lang.String(name)serverName-org.apache.hadoop.hbase.TableName(name)region.getTable()-java.lang.String(name)region.getRegionNameAsString()-java.lang.String(name)Bytes.toStringBinary(startKey),x,o,o
HBase,4147,org.apache.hadoop.hbase.tool.Canary.doFilterRegionServerByName,1610,info,"java.lang.String(name)No RegionServerInfo found, regionServerPattern {}-java.lang.String(name)rsName",o,o,o
HBase,4148,org.apache.hadoop.hbase.tool.Canary.doFilterRegionServerByName,1616,info,"java.lang.String(name)No RegionServerInfo found, regionServerName {}-java.lang.String(name)rsName",o,o,o
HBase,4095,org.apache.hadoop.hbase.tool.Canary.publishReadFailure,231,error,java.lang.String(name)Read from {} on {} {} failed-java.lang.String(name)region.getRegionNameAsString()-org.apache.hadoop.hbase.ServerName(name)serverName-java.lang.String(name)column.getNameAsString()-java.lang.Exception(name)e,x,o,o
HBase,4090,org.apache.hadoop.hbase.tool.Canary.publishReadFailure,194,error,java.lang.String(name)Read from {} on {}-java.lang.String(name)table-java.lang.String(name)server,o,o,o
HBase,4096,org.apache.hadoop.hbase.tool.Canary.publishReadTiming,237,info,java.lang.String(name)Read from {} on {} {} in {}ms-java.lang.String(name)region.getRegionNameAsString()-org.apache.hadoop.hbase.ServerName(name)serverName-java.lang.String(name)column.getNameAsString()-long(name)msTime,x,o,o
HBase,4091,org.apache.hadoop.hbase.tool.Canary.publishReadTiming,198,info,java.lang.String(name)Read from {} on {} in {}ms-java.lang.String(name)table-java.lang.String(name)server-long(name)msTime,x,o,o
HBase,4093,org.apache.hadoop.hbase.tool.Canary.publishReadTiming,212,info,java.lang.String(name)Read from {} on {} in {}ms-java.lang.String(name)znode-java.lang.String(name)server-long(name)msTime,x,o,o
HBase,4098,org.apache.hadoop.hbase.tool.Canary.publishWriteFailure,249,error,java.lang.String(name)Write to {} on {} {} failed-java.lang.String(name)region.getRegionNameAsString()-org.apache.hadoop.hbase.ServerName(name)serverName-java.lang.String(name)column.getNameAsString()-java.lang.Exception(name)e,x,o,o
HBase,4099,org.apache.hadoop.hbase.tool.Canary.publishWriteTiming,255,info,java.lang.String(name)Write to {} on {} {} in {}ms-java.lang.String(name)region.getRegionNameAsString()-org.apache.hadoop.hbase.ServerName(name)serverName-java.lang.String(name)column.getNameAsString()-long(name)msTime,x,o,o
HBase,4104,org.apache.hadoop.hbase.tool.Canary.read,402,debug,java.lang.String(name)Reading from {} {} {} {}-org.apache.hadoop.hbase.TableName(name)tableDesc.getTableName()-java.lang.String(name)region.getRegionNameAsString()-java.lang.String(name)column.getNameAsString()-java.lang.String(name)Bytes.toStringBinary(startKey),x,o,o
HBase,4120,org.apache.hadoop.hbase.tool.Canary.run,1165,error,java.lang.String(name)Read operation for {} took {}ms (Configured read timeout {}ms.-java.lang.String(name)tableName-java.lang.Long(name)actual-java.lang.Long(name)configured,x,o,o
HBase,4122,org.apache.hadoop.hbase.tool.Canary.run,1172,error,java.lang.String(name)Read operation for {} failed!-java.lang.String(name)tableName,o,o,o
HBase,4121,org.apache.hadoop.hbase.tool.Canary.run,1168,info,java.lang.String(name)Read operation for {} took {}ms (Configured read timeout {}ms.-java.lang.String(name)tableName-java.lang.Long(name)actual-java.lang.Long(name)configured,x,o,o
HBase,4123,org.apache.hadoop.hbase.tool.Canary.run,1178,info,java.lang.String(name)Write operation for {} took {}ms. Configured write timeout {}ms.-java.lang.String(name)writeTableStringName-long(name)actualWriteLatency-long(name)this.configuredWriteTableTimeout,x,o,o
HBase,4132,org.apache.hadoop.hbase.tool.Canary.sniff,1328,warn,java.lang.String(name)Table {} is not enabled-java.lang.String(name)tableName,o,o,o
HBase,4106,org.apache.hadoop.hbase.tool.Canary.write,456,debug,java.lang.String(name)Writing to {} {} {} {}-org.apache.hadoop.hbase.TableName(name)tableDesc.getTableName()-java.lang.String(name)region.getRegionNameAsString()-java.lang.String(name)column.getNameAsString()-java.lang.String(name)Bytes.toStringBinary(rowToCheck),x,o,o
HBase,4156,org.apache.hadoop.hbase.tool.HFileContentValidator.validateHFileContent,71,info,java.lang.String(name)Validating HFile contents under {}-org.apache.hadoop.fs.Path(name)archiveRootDir,o,o,o
HBase,4158,org.apache.hadoop.hbase.tool.HFileContentValidator.validateHFileContent,90,info,java.lang.String(name)There are no incompatible HFiles.,o,o,o
HBase,4077,org.apache.hadoop.hbase.tool.LoadIncrementalHFiles.cleanup,906,error,java.lang.String(name)err.toString(),x,x,x
HBase,4076,org.apache.hadoop.hbase.tool.LoadIncrementalHFiles.createTable,886,info,"java.lang.String(name)""Table "" + tableName + "" is available!!""",o,o,x
HBase,4061,org.apache.hadoop.hbase.tool.LoadIncrementalHFiles.groupOrSplitPhase,653,error,java.lang.String(name)Unexpected execution exception during splitting-java.util.concurrent.ExecutionException(name)e1,o,o,o
HBase,4062,org.apache.hadoop.hbase.tool.LoadIncrementalHFiles.groupOrSplitPhase,656,error,java.lang.String(name)Unexpected interrupted exception during splitting-java.lang.InterruptedException(name)e1,o,o,o
HBase,381,org.apache.hadoop.hbase.util.ClassSize.getSizeCoefficients,395,debug,"java.lang.String(name)"""" + index + "" ""+ aField.getName()+ "" ""+ aField.getType()",x,x,x
HBase,428,org.apache.hadoop.hbase.util.CommonFSUtils.hasCapability,1050,warn,"java.lang.String(name)Your Hadoop installation's StreamCapabilities implementation doesn't match our understanding of how it's supposed to work. Please file a JIRA and include the following stack trace. In the mean time we're interpreting this behavior difference as a lack of capability support, which will probably cause a failure.-java.lang.Exception(name)exception",o,o,o
HBase,419,org.apache.hadoop.hbase.util.CommonFSUtils.invokeSetStoragePolicy,639,debug,"java.lang.String(name)The underlying FileSystem implementation doesn't support setStoragePolicy. This is probably intentional on their part, since HDFS-9345 appears to be present in your version of Hadoop. For more information check the Hadoop documentation on 'ArchivalStorage', the Hadoop FileSystem specification docs from HADOOP-11981, and/or related documentation from the provider of the underlying FileSystem (its name should appear in the stacktrace that accompanies this message). Note in particular that Hadoop's local filesystem implementation doesn't support storage policies.-java.lang.Throwable(name)exception",o,o,o
HBase,413,org.apache.hadoop.hbase.util.CommonFSUtils.invokeSetStoragePolicy,600,warn,"java.lang.String(name)No access to setStoragePolicy on FileSystem from the SecurityManager; HDFS-6584, HDFS-9345 not available. This is unusual and probably warrants an email to the user@hbase mailing list. Please be sure to include a link to your configs, and logs that include this message and period of time before it. Logs around service start up will probably be useful as well.-java.lang.SecurityException(name)e",o,o,o
HBase,423,org.apache.hadoop.hbase.util.CommonFSUtils.logFSTree,811,debug,"java.lang.String(name)prefix + file.getPath().getName() + ""/""",x,x,x
HBase,4514,org.apache.hadoop.hbase.util.compaction.MajorCompactor.compactAndWait,224,error,"java.lang.String(name)""Not all store files were compacted, this may be due to the regionserver not "" + ""being aware of all store files. Will not reattempt compacting, "" + request",o,o,o
HBase,4507,org.apache.hadoop.hbase.util.compaction.MajorCompactor.shutdown,136,error,java.lang.String(name)builder.toString(),x,x,x
HBase,4303,org.apache.hadoop.hbase.util.FSHDFSUtils.checkIfTimedout,260,warn,"java.lang.String(name)""Cannot recoverLease after trying for "" + conf.getInt(""hbase.lease.recovery.timeout"",900000) + ""ms (hbase.lease.recovery.timeout); continuing, but may be DATALOSS!!!; ""+ getLogMessageDetail(nbAttempt,p,startWaiting)",o,o,x
HBase,4305,org.apache.hadoop.hbase.util.FSHDFSUtils.recoverLease,293,warn,"java.lang.String(name)getLogMessageDetail(nbAttempt,p,startWaiting)-java.io.IOException(name)e",x,x,x
HBase,4246,org.apache.hadoop.hbase.util.FSUtils.getRegionLocalityMappingFromFS,1756,info,java.lang.String(name)overheadMsg,x,x,x
HBase,4401,org.apache.hadoop.hbase.util.HBaseFsck.checkRegionBoundaries,968,error,java.lang.String(name)e.toString()-java.io.IOException(name)e,x,x,x
HBase,4400,org.apache.hadoop.hbase.util.HBaseFsck.checkRegionBoundaries,964,warn,java.lang.String(name)Objects.toString(currentRegionBoundariesInformation),x,x,x
HBase,4465,org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency,2252,warn,"java.lang.String(name)hri + "" start and stop keys are in the range of "" + region+ "". The region might not be cleaned up from hdfs when region ""+ region+ "" split failed. Hence deleting from hdfs.""",o,o,x
HBase,4398,org.apache.hadoop.hbase.util.HBaseFsck.close,862,warn,java.lang.String(name)io.toString()-java.lang.Exception(name)io,x,x,x
HBase,4386,org.apache.hadoop.hbase.util.HBaseFsck.connect,552,error,"java.lang.String(name)""Another instance of hbck is fixing HBase, exiting this instance. "" + ""[If you are sure no other instance is running, delete the lock file "" + HBCK_LOCK_PATH + "" and rerun the tool]""",o,o,x
HBase,4446,org.apache.hadoop.hbase.util.HBaseFsck.createZooKeeperWatcher,1739,error,java.lang.String(name)why-java.lang.Throwable(name)e,x,x,x
HBase,4488,org.apache.hadoop.hbase.util.HBaseFsck.exec,3842,warn,java.lang.String(name)Interrupted while sleeping,o,x,o
HBase,4428,org.apache.hadoop.hbase.util.HBaseFsck.fixOrphanTables,1456,warn,"java.lang.String(name)""Strongly recommend to modify the TableDescriptor if necessary for: "" + tableName",o,o,o
HBase,4390,org.apache.hadoop.hbase.util.HBaseFsck.offlineHdfsIntegrityRepair,677,info,"java.lang.String(name)""Successfully exiting integrity repairs after "" + curIter + "" iterations""",o,o,o
HBase,4442,org.apache.hadoop.hbase.util.HBaseFsck.sidelineRegionDir,1604,error,java.lang.String(name)msg,x,x,x
HBase,4493,org.apache.hadoop.hbase.util.HBaseFsckRepair.waitUntilAssigned,136,warn,"java.lang.String(name)Exception when waiting for region to become assigned, retrying-java.io.IOException(name)e",o,o,o
HBase,4330,org.apache.hadoop.hbase.util.HbckTableInfo.handleRegionStartKeyNotEmpty,287,info,"java.lang.String(name)""Table region start key was not empty. Created new empty region: "" + newRegion + "" ""+ region",o,o,o
HBase,4254,org.apache.hadoop.hbase.util.JvmPauseMonitor.run,178,info,"java.lang.String(name)formatMessage(extraSleepTime,gcDiffs)",x,x,x
HBase,4733,org.apache.hadoop.hbase.util.MockServer.abort,74,error,"org.slf4j.Marker(name)HBaseMarkers.FATAL-java.lang.String(name)""Abort why="" + why-java.lang.Throwable(name)e",o,x,o
HBase,4734,org.apache.hadoop.hbase.util.MockServer.stop,81,debug,"java.lang.String(name)""Stop why="" + why",o,x,o
HBase,4728,org.apache.hadoop.hbase.util.MultiThreadedAction.printLocations,511,info,"java.lang.String(name)""LOCATION "" + h",o,x,x
HBase,4723,org.apache.hadoop.hbase.util.MultiThreadedAction.verifyResultAgainstDataGenerator,470,error,"java.lang.String(name)""Error checking data for key ["" + rowKeyStr + ""], mutation checking failed for column family [""+ cfStr+ ""], column [""+ column+ ""]; mutation [""+ mutation+ ""], hashCode [""+ hashCode+ ""], verificationNeeded [""+ verificationNeeded+ ""]""",o,o,o
HBase,4742,org.apache.hadoop.hbase.util.ProcessBasedLocalHBaseCluster.shutdownAllProcesses,260,error,"java.lang.String(name)""Could not read pid from file "" + pidFile",o,o,o
HBase,4752,org.apache.hadoop.hbase.util.ProcessBasedLocalHBaseCluster.startTailingFile,512,debug,"java.lang.String(name)""Tailing "" + filePath",x,x,x
HBase,4744,org.apache.hadoop.hbase.util.ProcessBasedLocalHBaseCluster.writeStringToFile,295,error,"java.lang.String(name)""Error writing to: "" + fileName-java.io.IOException(name)e",o,o,o
HBase,4276,org.apache.hadoop.hbase.util.RegionSplitter.splitScan,731,info,java.lang.String(name)nsfre.toString()-org.apache.hadoop.hbase.client.NoServerForRegionException(name)nsfre,x,x,x
HBase,4310,org.apache.hadoop.hbase.util.ServerCommandLine.logHBaseConfigs,101,info,"java.lang.String(name)key + "": "" + conf.get(key)",x,x,x
HBase,4543,org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getServerNameFromWALDirectoryName,403,warn,"java.lang.String(name)""Cannot parse a server name from path="" + logFile + ""; ""+ ex.getMessage()",o,o,o
HBase,4574,org.apache.hadoop.hbase.wal.AsyncFSWALProvider.createAsyncWriter,117,error,"java.lang.String(name)""The RegionServer async write ahead log provider "" + ""relies on the ability to call "" + e.getMessage() + "" for proper operation during ""+ ""component failures, but the current FileSystem does not support doing so. Please ""+ ""check the config value of '""+ CommonFSUtils.HBASE_WAL_DIR+ ""' and ensure ""+ ""it points to a FileSystem mount that has suitable capabilities for output streams.""",o,o,o
HBase,4539,org.apache.hadoop.hbase.wal.FSHLogProvider.createWriter,83,error,"java.lang.String(name)""The RegionServer write ahead log provider for FileSystem implementations "" + ""relies on the ability to call "" + e.getMessage() + "" for proper operation during ""+ ""component failures, but the current FileSystem does not support doing so. Please ""+ ""check the config value of '""+ CommonFSUtils.HBASE_WAL_DIR+ ""' and ensure ""+ ""it points to a FileSystem mount that has suitable capabilities for output streams.""",o,o,o
HBase,4582,org.apache.hadoop.hbase.wal.LogRecoveredEditsOutputSink.closeWriter,215,trace,java.lang.String(name)Closing {}-org.apache.hadoop.fs.Path(name)wap.path,o,o,o
HBase,4577,org.apache.hadoop.hbase.wal.LogRecoveredEditsOutputSink.deleteOneWithFewerEntries,119,warn,"java.lang.String(name)""Found existing old edits file. It could be the result of a previous failed"" + "" split attempt or we have duplicated wal entries. Deleting "" + dst + "", length=""+ walFS.getFileStatus(dst).getLen()",o,o,o
HBase,4572,org.apache.hadoop.hbase.wal.OutputSink.doRun,208,trace,java.lang.String(name)Writer thread starting,o,o,o
HBase,4598,org.apache.hadoop.hbase.wal.RegionGroupingProvider.getStrategy,113,debug,java.lang.String(name)Exception details for failure to load region grouping strategy.-java.lang.Exception(name)e,o,o,o
HBase,4789,org.apache.hadoop.hbase.wal.WALPerformanceEvaluation.verify,440,info,"java.lang.String(name)""seqid="" + seqid",x,x,x
HBase,4620,org.apache.hadoop.hbase.wal.WALSplitter.getReader,351,warn,"java.lang.String(name)File {} might be still open, length is 0-org.apache.hadoop.fs.Path(name)path",o,o,o
HBase,4619,org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile,331,info,java.lang.String(name)msg,x,x,x
HBase,1365,org.apache.hadoop.hbase.ZKNamespaceManager.refreshNodes,206,trace,"java.lang.String(name)""Updating namespace cache from node "" + namespace + "" with data: ""+ Bytes.toStringBinary(nodeData)",o,o,o
HBase,1370,org.apache.hadoop.hbase.ZNodeClearer.clear,179,warn,java.lang.String(name)Can't connect to zookeeper to read the master znode-java.io.IOException(name)e,o,o,o
HBase,5464,org.apache.hadoop.hbase.zookeeper.MetaTableLocator.waitMetaRegionLocation,194,error,java.lang.String(name)errorMsg,x,x,x
HBase,285,org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.ReadOnlyZKClient,142,debug,"java.lang.String(name)Connect {} to {} with session timeout={}ms, retries {}, retry interval {}ms, keepAlive={}ms-java.lang.String(name)getId()-java.lang.String(name)connectString-int(name)sessionTimeoutMs-int(name)maxRetries-int(name)retryIntervalMs-int(name)keepAliveTimeMs",x,o,o
HBase,291,org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.run,335,trace,java.lang.String(name){} to {} inactive for {}ms; closing (Will reconnect when new requests)-java.lang.String(name)getId()-java.lang.String(name)connectString-int(name)keepAliveTimeMs,x,o,o
HBase,5447,org.apache.hadoop.hbase.zookeeper.ZKUtil.createEphemeralNodeAndWatch,969,info,java.lang.String(name)Interrupted-java.lang.InterruptedException(name)e,o,x,o
HBase,5437,org.apache.hadoop.hbase.zookeeper.ZKUtil.getData,613,debug,"java.lang.String(name)zkw.prefix(""Unable to get data of znode "" + znode + "" ""+ ""because node does not exist (not an error)"")",o,o,o
HBase,5430,org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenAndWatchForNewChildren,445,debug,"java.lang.String(name)zkw.prefix(""Unable to list children of znode "" + znode + "" ""+ ""because node does not exist (not an error)"")",o,o,o
HBase,5458,org.apache.hadoop.hbase.zookeeper.ZKUtil.logZKTree,2080,debug,java.lang.String(name)Current zk system:,o,o,x
HBase,5459,org.apache.hadoop.hbase.zookeeper.ZKUtil.logZKTree,2082,debug,java.lang.String(name)prefix + root,x,x,x
HBase,5460,org.apache.hadoop.hbase.zookeeper.ZKUtil.logZKTree,2104,debug,java.lang.String(name)prefix + child,x,x,x
JMeter,31,org.apache.jmeter.assertions.BeanShellAssertion.getResult,114,error,java.lang.String(name)BeanShell Jar missing?-java.lang.NoClassDefFoundError(name)ex,o,o,x
JMeter,23,org.apache.jmeter.assertions.XPathAssertion.getResult,95,debug,"java.lang.String(name)Validation is set to {}, Whitespace is set to {}, Tolerant is set to {}-boolean(name)isValidating()-boolean(name)isWhitespace()-boolean(name)isTolerant()",o,o,o
JMeter,71,org.apache.jmeter.config.RandomVariableConfig.getRandomSeedAsLong,212,warn,java.lang.String(name)Cannot parse random seed: '{}' in element {}-java.lang.String(name)randomSeed-java.lang.String(name)getName(),o,o,o
JMeter,90,org.apache.jmeter.control.ThroughputController.getMaxThroughputAsInt,133,warn,java.lang.String(name)Error parsing '{}'-java.lang.String(name)valueString-java.lang.NumberFormatException(name)e,o,o,o
JMeter,374,org.apache.jmeter.control.TransactionController.nextWithTransactionSampler,155,debug,java.lang.String(name)Start of transaction {}-java.lang.String(name)getName(),o,o,o
JMeter,373,org.apache.jmeter.control.TransactionController.nextWithTransactionSampler,144,debug,java.lang.String(name)End of transaction {}-java.lang.String(name)getName(),o,o,o
JMeter,443,org.apache.jmeter.engine.ClientJMeterEngine.exit,206,info,java.lang.String(name)about to exit remote server on {}-java.lang.String(name)hostAndPort,o,o,x
JMeter,439,org.apache.jmeter.engine.ClientJMeterEngine.runTest,167,info,java.lang.String(name)sent run command to {}-java.lang.String(name)hostAndPort,o,o,x
JMeter,437,org.apache.jmeter.engine.ClientJMeterEngine.runTest,158,info,java.lang.String(name)Sending properties {}-java.util.Properties(name)savep,o,x,o
JMeter,438,org.apache.jmeter.engine.ClientJMeterEngine.runTest,163,warn,"java.lang.String(name)Could not set properties: {}, error:{}-java.util.Properties(name)savep-java.lang.String(name)e.getMessage()-java.rmi.RemoteException(name)e",o,o,o
JMeter,442,org.apache.jmeter.engine.ClientJMeterEngine.tidyRMI,196,info,java.lang.String(name)Interrupting {}-java.lang.String(name)name,o,x,o
JMeter,409,org.apache.jmeter.engine.RemoteJMeterEngineImpl.init,87,debug,java.lang.String(name)This = {}-org.apache.jmeter.engine.RemoteJMeterEngineImpl(name)this,o,x,o
JMeter,408,org.apache.jmeter.engine.RemoteJMeterEngineImpl.init,84,info,java.lang.String(name)IP address is a site-local address; this may cause problems with remote access. Can be overridden by defining the system property 'java.rmi.server.hostname' - see jmeter-server script file,o,o,o
JMeter,426,org.apache.jmeter.engine.RemoteJMeterEngineImpl.rexit,191,info,java.lang.String(name)Exiting,o,x,o
JMeter,405,org.apache.jmeter.engine.StandardJMeterEngine.exit,569,info,java.lang.String(name)Bye from {}-java.lang.String(name)host,o,o,x
JMeter,379,org.apache.jmeter.engine.StandardJMeterEngine.notifyTestListenersOfEnd,226,info,java.lang.String(name)Test has ended on host {} -java.lang.String(name)host,o,o,o
JMeter,380,org.apache.jmeter.engine.StandardJMeterEngine.run,328,error,"java.lang.String(name)JMeterUtils.getResString(""stopping_test_failed"")",x,x,x
JMeter,386,org.apache.jmeter.engine.StandardJMeterEngine.run,415,info,java.lang.String(name)Waiting for setup thread group: {} to finish before starting next setup group-java.lang.String(name)groupName,o,o,o
JMeter,390,org.apache.jmeter.engine.StandardJMeterEngine.run,452,info,java.lang.String(name)Waiting for thread group: {} to finish before starting next group-java.lang.String(name)groupName,o,o,o
JMeter,398,org.apache.jmeter.engine.StandardJMeterEngine.startThreadGroup,509,info,java.lang.String(name)Starting {} threads for group {}.-int(name)numThreads-java.lang.String(name)groupName,o,o,o
JMeter,1067,org.apache.jmeter.examples.sampler.ExampleSampler.trace,122,debug,java.lang.String(name){} ({}) {} {} {}-java.lang.String(name)Thread.currentThread().getName()-int(name)classCount.get()-java.lang.String(name)getTitle()-java.lang.String(name)s-java.lang.String(name)this.toString(),x,x,x
JMeter,117,org.apache.jmeter.extractor.BeanShellPostProcessor.process,70,warn,java.lang.String(name)Problem in BeanShell script: {}-java.lang.String(name)e.toString(),o,o,o
JMeter,144,org.apache.jmeter.extractor.json.jsonpath.JSONPostProcessor.handleEmptyResult,189,debug,"java.lang.String(name)No value extracted, storing empty in: {}{}-java.lang.String(name)currentRefName-java.lang.String(name)_ALL",x,o,o
JMeter,122,org.apache.jmeter.extractor.RegexExtractor.process,163,error,java.lang.String(name)Error in pattern: '{}'-java.lang.String(name)regex,o,o,o
JMeter,120,org.apache.jmeter.extractor.RegexExtractor.process,124,warn,java.lang.String(name)Could not parse number: '{}'-java.lang.String(name)prevString,o,o,o
JMeter,121,org.apache.jmeter.extractor.RegexExtractor.process,160,warn,java.lang.String(name)Error while generating result,o,o,o
JMeter,1083,org.apache.jmeter.functions.BeanShell.execute,110,debug,"java.lang.String(name)__Beanshell({},{})={}-java.lang.String(name)script-java.lang.String(name)varName-java.lang.String(name)resultStr",x,x,x
JMeter,1139,org.apache.jmeter.functions.CSVRead.execute,118,warn,java.lang.String(name){} - invalid column number: {} at row {} {}-java.lang.String(name)Thread.currentThread().getName()-java.lang.String(name)columnOrNext-int(name)FileWrapper.getCurrentRow(fileName)-java.lang.String(name)e.toString(),x,o,o
JMeter,1138,org.apache.jmeter.functions.CSVRead.execute,114,warn,java.lang.String(name){} - can't parse column number: {} {}-java.lang.String(name)Thread.currentThread().getName()-java.lang.String(name)columnOrNext-java.lang.String(name)e.toString(),x,o,o
JMeter,1127,org.apache.jmeter.functions.FileRowColContainer.FileRowColContainer,58,debug,"java.lang.String(name)FRCC({},{})-java.lang.String(name)file-java.lang.String(name)delim",x,x,x
JMeter,1128,org.apache.jmeter.functions.FileRowColContainer.FileRowColContainer,67,debug,java.lang.String(name)FRCC({})[{}]-java.lang.String(name)file-java.lang.String(name)DELIMITER,x,x,x
JMeter,1130,org.apache.jmeter.functions.FileRowColContainer.getColumn,109,debug,"java.lang.String(name){}({},{}):{}-java.lang.String(name)fileName-int(name)row-int(name)col-java.lang.String(name)colData",x,x,x
JMeter,1112,org.apache.jmeter.functions.FileToString.execute,111,warn,java.lang.String(name)Could not read file: {} {}-java.lang.String(name)fileName-java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,o,o
JMeter,1111,org.apache.jmeter.functions.FileToString.execute,108,warn,java.lang.String(name)Could not read open: {} -java.lang.String(name)fileName,o,o,x
JMeter,1124,org.apache.jmeter.functions.FileWrapper.clearAll,189,debug,java.lang.String(name)clearAll(),x,x,x
JMeter,1144,org.apache.jmeter.functions.Groovy.execute,132,debug,"java.lang.String(name)__groovy({},{})={}-java.lang.String(name)script-java.lang.String(name)varName-java.lang.String(name)resultStr",x,x,x
JMeter,1085,org.apache.jmeter.functions.Jexl2Function.execute,105,error,"java.lang.String(name)""An error occurred while evaluating the expression \"""" + exp + ""\""\n""-java.lang.Exception(name)e",o,o,o
JMeter,1136,org.apache.jmeter.functions.Jexl3Function.execute,106,error,"java.lang.String(name)An error occurred while evaluating the expression ""{}"" -java.lang.String(name)exp-java.lang.Exception(name)e",o,o,o
JMeter,1106,org.apache.jmeter.functions.LogFunction.logDetails,164,error,java.lang.String(name){} {} {}-java.lang.String(name)threadName-java.lang.String(name)separator-java.lang.String(name)stringToLog-java.lang.Throwable(name)throwable,x,x,x
JMeter,1108,org.apache.jmeter.functions.LogFunction.logDetails,170,info,java.lang.String(name){} {} {}-java.lang.String(name)threadName-java.lang.String(name)separator-java.lang.String(name)stringToLog-java.lang.Throwable(name)throwable,x,x,x
JMeter,1107,org.apache.jmeter.functions.LogFunction.logDetails,167,warn,java.lang.String(name){} {} {}-java.lang.String(name)threadName-java.lang.String(name)separator-java.lang.String(name)stringToLog-java.lang.Throwable(name)throwable,x,x,x
JMeter,1070,org.apache.jmeter.functions.SplitFunction.execute,100,debug,java.lang.String(name)parts[i - 1],x,x,x
JMeter,1101,org.apache.jmeter.functions.StringFromFile.execute,274,error,java.lang.String(name){} error reading file {}-java.lang.String(name)tn-java.lang.String(name)e.toString(),o,o,o
JMeter,1091,org.apache.jmeter.functions.StringFromFile.openFile,166,warn,"java.lang.String(name)Exception parsing {} as int, value will not be considered as Start Number sequence-java.lang.String(name)start",o,o,o
JMeter,1092,org.apache.jmeter.functions.StringFromFile.openFile,182,warn,"java.lang.String(name)Exception parsing {} as int, value will not be considered as End Number sequence-java.lang.String(name)tmp",o,o,o
JMeter,1104,org.apache.jmeter.functions.StringFromFile.setParameters,304,debug,java.lang.String(name){}::StringFromFile.setParameters()-org.apache.jmeter.functions.StringFromFile(name)this,x,x,x
JMeter,1105,org.apache.jmeter.functions.StringFromFile.setParameters,317,info,java.lang.String(name){}-java.lang.StringBuilder(name)sb,x,x,x
JMeter,1157,org.apache.jmeter.functions.StringToFile.execute,137,error,java.lang.String(name)The encoding of file is not supported-java.nio.charset.UnsupportedCharsetException(name)ue,o,o,o
JMeter,1158,org.apache.jmeter.functions.StringToFile.execute,140,error,java.lang.String(name)The encoding of file contains illegal characters-java.nio.charset.IllegalCharsetNameException(name)ie,o,o,o
JMeter,1164,org.apache.jmeter.functions.XPath.execute,74,debug,java.lang.String(name)execute ({} {})-java.lang.String(name)fileName-java.lang.String(name)xpathString,x,o,x
JMeter,1167,org.apache.jmeter.functions.XPath.setParameters,106,debug,java.lang.String(name)i:{}-java.lang.String(name)((CompoundVariable)values[i]).execute(),x,x,x
JMeter,1133,org.apache.jmeter.functions.XPathFileContainer.load,75,debug,java.lang.String(name)found {}-int(name)nl.getLength(),o,x,x
JMeter,1080,org.apache.jmeter.functions.XPathWrapper.clearAll,115,debug,java.lang.String(name)clearAll(),x,x,x
JMeter,1078,org.apache.jmeter.functions.XPathWrapper.getXPathString,104,warn,java.lang.String(name)XPathFileContainer has no nodes: {} {}-java.lang.String(name)file-java.lang.String(name)xpathString,x,o,o
JMeter,1076,org.apache.jmeter.functions.XPathWrapper.open,69,warn,java.lang.String(name)e.getLocalizedMessage(),x,x,x
JMeter,590,org.apache.jmeter.gui.action.AbstractAction.popupShouldSave,70,debug,java.lang.String(name)popupShouldSave,x,x,x
JMeter,545,org.apache.jmeter.gui.action.ActionRouter.populateCommandMap,366,error,"java.lang.String(name)!!!!!Uh-oh, didn't find any action handlers!!!!!",o,o,x
JMeter,550,org.apache.jmeter.gui.action.ApplyNamingConvention.doAction,72,error,java.lang.String(name)Failed to apply naming policy-java.lang.Exception(name)err,o,o,o
JMeter,552,org.apache.jmeter.gui.action.Clear.doAction,73,error,java.lang.String(name)Can't clear: {} {}-org.apache.jmeter.gui.tree.JMeterTreeNode(name)node-org.apache.jmeter.gui.JMeterGUIComponent(name)guiComp-java.lang.Exception(name)ex,x,o,o
JMeter,593,org.apache.jmeter.gui.action.EnableComponent.doAction,55,debug,java.lang.String(name)enabling currently selected gui objects,o,o,x
JMeter,594,org.apache.jmeter.gui.action.EnableComponent.doAction,59,debug,java.lang.String(name)disabling currently selected gui objects,o,o,x
JMeter,595,org.apache.jmeter.gui.action.EnableComponent.doAction,63,debug,java.lang.String(name)toggling currently selected gui objects,o,o,x
JMeter,557,org.apache.jmeter.gui.action.HtmlReportGenerator.run,92,error,java.lang.String(name)Error during HTML report generation:-java.lang.Exception(name)e,o,o,o
JMeter,556,org.apache.jmeter.gui.action.HtmlReportGenerator.run,87,info,java.lang.String(name)The HTML report generation failed and returned: {}-java.io.ByteArrayOutputStream(name)commandExecutionOutput,o,o,o
JMeter,569,org.apache.jmeter.gui.action.Load.loadProjectFile,142,warn,java.lang.String(name)Could not convert file. {}-java.lang.String(name)ex.toString(),o,o,o
JMeter,580,org.apache.jmeter.gui.action.Save.createBackupFile,352,error,"java.lang.String(name)Could not backup file! Backup directory does not exist, is not a directory or could not be created ! <{}>-java.lang.String(name)backupDir.getAbsolutePath()",o,o,o
JMeter,532,org.apache.jmeter.gui.action.SelectTemplatesDialog.replaceTemplateParametersAndLoad,192,error,java.lang.String(name)Error generating output file {} from template {}-java.io.File(name)temporaryGeneratedFile-java.io.File(name)jmxFile-java.lang.Exception(name)ex,o,o,o
JMeter,584,org.apache.jmeter.gui.action.Start.doAction,119,info,java.lang.String(name)Stopping test,o,o,o
JMeter,588,org.apache.jmeter.gui.action.Start.startEngine,204,debug,java.lang.String(name)test plan after cloning and running test is running version: {}-boolean(name)((TestPlan)treeToUse.getArray()[0]).isRunningVersion(),o,o,x
JMeter,597,org.apache.jmeter.gui.action.template.TemplateManager.readTemplates,111,warn,java.lang.String(name)Ignoring template file:'{}' as it does not exist or is not readable-java.lang.String(name)file.getAbsolutePath(),o,o,o
JMeter,574,org.apache.jmeter.gui.action.What.doAction,76,info,java.lang.String(name)Log level set to DEBUG for {}-java.lang.String(name)loggerName,o,o,o
JMeter,575,org.apache.jmeter.gui.action.What.doAction,80,info,java.lang.String(name)Log level set to INFO for {}-java.lang.String(name)loggerName,o,o,o
JMeter,624,org.apache.jmeter.gui.util.FileDialoger.setCurrentDirOnJFC,215,info,java.lang.String(name)No valid initial directory found for: {}-java.util.List<String>(name)Arrays.asList(dirNames),o,o,o
JMeter,631,org.apache.jmeter.gui.util.JMeterMenuBar.findMenuCreators,229,debug,java.lang.String(name)Loading menu creator class: {}-java.lang.String(name)strClassName,o,o,o
JMeter,637,org.apache.jmeter.gui.util.JMeterMenuBar.setRunning,621,debug,java.lang.String(name)Found stop host: {}-java.lang.String(name)stop.getText(),o,o,o
JMeter,638,org.apache.jmeter.gui.util.JMeterMenuBar.setRunning,627,debug,java.lang.String(name)Found exit host: {}-java.lang.String(name)exit.getText(),o,o,o
JMeter,639,org.apache.jmeter.gui.util.JMeterMenuBar.setRunning,633,debug,java.lang.String(name)Found shut host: {}-java.lang.String(name)exit.getText(),o,o,o
JMeter,629,org.apache.jmeter.gui.util.JSyntaxTextArea.JSyntaxTextArea,226,debug,java.lang.String(name)Font is set to: {}-java.awt.Font(name)getFont(),o,o,o
JMeter,616,org.apache.jmeter.gui.util.MenuFactory.getGUIComponent,210,warn,"java.lang.String(name)Configuration error, probably corrupt or missing third party library(jar)? Could not create class: {}.-java.lang.String(name)name-java.lang.NoClassDefFoundError(name)e",o,o,x
JMeter,617,org.apache.jmeter.gui.util.MenuFactory.getGUIComponent,213,warn,java.lang.String(name)Could not instantiate class: {}-java.lang.String(name)name-java.awt.HeadlessException(name)e,o,o,o
JMeter,614,org.apache.jmeter.gui.util.MenuFactory.initializeMenus,185,debug,java.lang.String(name){}ms total menu initialization time-long(name)a1 - a0,x,o,o
JMeter,336,org.apache.jmeter.JMeter.initializeProperties,865,info,java.lang.String(name)Setting System properties from file: {}-java.lang.String(name)name,o,o,o
JMeter,349,org.apache.jmeter.JMeter.initializeProperties,933,warn,"java.lang.String(name)Invalid log level, '{}', for the root logger.-java.lang.String(name)name",o,o,o
JMeter,330,org.apache.jmeter.JMeter.initializeProperties,819,warn,java.lang.String(name)Error loading user property file: {}-java.lang.String(name)userProp-java.io.IOException(name)e,o,o,o
JMeter,332,org.apache.jmeter.JMeter.initializeProperties,833,warn,java.lang.String(name)Error loading system property file: {}-java.lang.String(name)sysProp-java.io.IOException(name)e,o,o,o
JMeter,334,org.apache.jmeter.JMeter.initializeProperties,859,warn,java.lang.String(name)Can't find additional property file: {}-java.lang.String(name)name-java.io.FileNotFoundException(name)e,o,o,o
JMeter,335,org.apache.jmeter.JMeter.initializeProperties,861,warn,java.lang.String(name)Error loading additional property file: {}-java.lang.String(name)name-java.io.IOException(name)e,o,o,o
JMeter,337,org.apache.jmeter.JMeter.initializeProperties,870,warn,java.lang.String(name)Cannot find system property file. {}-java.lang.String(name)e.getLocalizedMessage(),o,o,o
JMeter,344,org.apache.jmeter.JMeter.initializeProperties,904,warn,java.lang.String(name)Could not find properties file: {}-java.lang.String(name)e.getLocalizedMessage(),o,o,o
JMeter,345,org.apache.jmeter.JMeter.initializeProperties,908,warn,java.lang.String(name)Could not load properties file: {}-java.lang.String(name)e.getLocalizedMessage(),o,o,o
JMeter,324,org.apache.jmeter.JMeter.runInitScripts,725,error,java.lang.String(name)Script {} referenced by property {} is not readable or does not exist-java.lang.String(name)file.getAbsolutePath()-java.lang.String(name)jsr223.init.file,o,o,o
JMeter,322,org.apache.jmeter.JMeter.runInitScripts,707,warn,"java.lang.String(name)No script engine found for [{}]. Will try to use Groovy. Possible engines and their extensions are: {}-java.lang.String(name)extension-java.util.Map<String,List<String>>(name)getEnginesAndExtensions(scriptEngineManager)",o,o,o
JMeter,320,org.apache.jmeter.JMeter.runInitScripts,691,warn,java.lang.String(name)Could not process Beanshell file: {}-java.lang.String(name)e.getMessage(),o,o,o
JMeter,308,org.apache.jmeter.JMeter.start,502,debug,java.lang.String(name)bit,x,x,x
JMeter,309,org.apache.jmeter.JMeter.start,528,error,"java.lang.String(name)Giving up, as server failed with:-java.lang.Exception(name)ex",o,x,o
JMeter,291,org.apache.jmeter.JMeter.start,475,info,java.lang.String(name)Version {}-java.lang.String(name)JMeterUtils.getJMeterVersion(),o,x,o
JMeter,298,org.apache.jmeter.JMeter.start,482,info,"java.lang.String(name)java.awt.headless={}-java.lang.String(name)System.getProperty(""java.awt.headless"")",x,x,x
JMeter,297,org.apache.jmeter.JMeter.start,481,info,"java.lang.String(name)file.encoding={}-java.lang.String(name)System.getProperty(""file.encoding"")",x,x,x
JMeter,292,org.apache.jmeter.JMeter.start,476,info,"java.lang.String(name)java.version={}-java.lang.String(name)System.getProperty(""java.version"")",x,x,x
JMeter,293,org.apache.jmeter.JMeter.start,477,info,"java.lang.String(name)java.vm.name={}-java.lang.String(name)System.getProperty(""java.vm.name"")",x,x,x
JMeter,294,org.apache.jmeter.JMeter.start,478,info,"java.lang.String(name)os.name={}-java.lang.String(name)System.getProperty(""os.name"")",x,x,x
JMeter,295,org.apache.jmeter.JMeter.start,479,info,"java.lang.String(name)os.arch={}-java.lang.String(name)System.getProperty(""os.arch"")",x,x,x
JMeter,296,org.apache.jmeter.JMeter.start,480,info,"java.lang.String(name)os.version={}-java.lang.String(name)System.getProperty(""os.version"")",x,x,x
JMeter,285,org.apache.jmeter.JMeter.startGui,379,info,java.lang.String(name)Setting LAF to: {}-java.lang.String(name)jMeterLaf,o,o,x
JMeter,286,org.apache.jmeter.JMeter.startGui,382,warn,java.lang.String(name)Could not set LAF to: {}-java.lang.String(name)jMeterLaf-java.lang.IllegalArgumentException(name)ex,o,o,x
JMeter,354,org.apache.jmeter.JMeter.testStarted,1294,info,java.lang.String(name)Started remote host: {} ({})-java.lang.String(name)host-long(name)now,o,o,o
JMeter,314,org.apache.jmeter.JMeter.updatePath,639,info,java.lang.String(name)Adding to classpath and loader: {}-java.lang.String(name)path,o,o,o
JMeter,312,org.apache.jmeter.JMeter.updatePath,630,info,java.lang.String(name){}={}-java.lang.String(name)property-java.lang.String(name)userpath,x,x,x
JMeter,156,org.apache.jmeter.modifiers.SampleTimeout.createTask,142,debug,java.lang.String(name)Scheduled timer: @{} {}-int(name)System.identityHashCode(future)-java.lang.String(name)getInfo(samp),x,o,o
JMeter,155,org.apache.jmeter.modifiers.SampleTimeout.sampleEnded,109,debug,"java.lang.String(name)whoAmI(""sampleEnded()"",this)",x,x,x
JMeter,154,org.apache.jmeter.modifiers.SampleTimeout.sampleStarting,101,debug,"java.lang.String(name)whoAmI(""sampleStarting()"",this)",x,x,x
JMeter,153,org.apache.jmeter.modifiers.SampleTimeout.SampleTimeout,77,debug,"java.lang.String(name)whoAmI(""InterruptTimer()"",this)",x,x,x
JMeter,158,org.apache.jmeter.modifiers.SampleTimeout.threadFinished,156,debug,"java.lang.String(name)whoAmI(""threadFinished()"",this)",x,x,x
JMeter,157,org.apache.jmeter.modifiers.SampleTimeout.threadStarted,149,debug,"java.lang.String(name)whoAmI(""threadStarted()"",this)",x,x,x
JMeter,640,org.apache.jmeter.plugin.PluginManager.installPlugin,59,warn,java.lang.String(name)Can't find icon for {} - {}-java.lang.String(name)icon[0]-java.lang.String(name)icon[1],o,o,o
JMeter,1245,org.apache.jmeter.protocol.http.control.AuthManager.addFile,420,error,java.lang.String(name)Error parsing auth line: '{}'-java.lang.String(name)line-java.util.NoSuchElementException(name)e,o,o,o
JMeter,1240,org.apache.jmeter.protocol.http.control.AuthManager.getAuthForURL,258,debug,java.lang.String(name)Target URL strings to match against: {} and {}-java.lang.String(name)s1-java.lang.String(name)s2,o,o,o
JMeter,1246,org.apache.jmeter.protocol.http.control.AuthManager.setupCredentials,483,debug,java.lang.String(name){} > D={} R={} M={}-java.lang.String(name)username-java.lang.String(name)domain-java.lang.String(name)realm-org.apache.jmeter.protocol.http.control.Mechanism(name)auth.getMechanism(),o,x,x
JMeter,1204,org.apache.jmeter.protocol.http.control.CacheManager.calcExpiresDate,329,warn,"java.lang.String(name)""Failed computing expiration date with following info:"" + lastModified + "",""+ cacheControl+ "",""+ expires+ "",""+ etag+ "",""+ url+ "",""+ date",x,o,o
JMeter,1210,org.apache.jmeter.protocol.http.control.CacheManager.entryStillValid,515,debug,java.lang.String(name)expiresDate is null for url {}-java.net.URL(name)url,o,o,x
JMeter,1198,org.apache.jmeter.protocol.http.control.CacheManager.getVaryHeader,208,debug,java.lang.String(name)Found vary value {} for {} in response-org.apache.http.Header(name)header-java.lang.String(name)headerName,o,o,o
JMeter,1257,org.apache.jmeter.protocol.http.control.DNSCacheManager.clearHosts,356,debug,java.lang.String(name)Clear all hosts from store,o,o,o
JMeter,1256,org.apache.jmeter.protocol.http.control.DNSCacheManager.clearServers,332,debug,java.lang.String(name)Clear all servers from store,o,o,o
JMeter,1248,org.apache.jmeter.protocol.http.control.DNSCacheManager.createResolver,133,warn,java.lang.String(name)Failed to create Extended resolver: {}-java.lang.String(name)uhe.getMessage()-java.net.UnknownHostException(name)uhe,o,o,o
JMeter,1252,org.apache.jmeter.protocol.http.control.DNSCacheManager.isStaticHost,199,debug,java.lang.String(name)No static host found for {}-java.lang.String(name)host,o,o,o
JMeter,1177,org.apache.jmeter.protocol.http.control.HttpMirrorServer.run,171,warn,java.lang.String(name)Could not bind HttpMirror to port {}. Maybe there is already a HttpMirror running?-int(name)daemonPort,o,o,x
JMeter,1181,org.apache.jmeter.protocol.http.control.HttpMirrorServer.setLogLevel,267,info,java.lang.String(name)Setting log level to '{}' for '{}'.-java.lang.String(name)value-java.lang.String(name)loggerName,o,o,o
JMeter,1221,org.apache.jmeter.protocol.http.control.HttpMirrorThread.run,83,debug,java.lang.String(name)Starting thread,o,x,o
JMeter,1233,org.apache.jmeter.protocol.http.control.HttpMirrorThread.run,266,debug,java.lang.String(name)Chunked,o,x,o
JMeter,1234,org.apache.jmeter.protocol.http.control.HttpMirrorThread.run,274,debug,java.lang.String(name)Other,o,x,o
JMeter,1236,org.apache.jmeter.protocol.http.control.HttpMirrorThread.run,280,debug,java.lang.String(name)Flush,o,x,o
JMeter,1263,org.apache.jmeter.protocol.http.curl.BasicCurlParser.readFromFile,1018,error,java.lang.String(name)Failed to read from File {}-java.lang.String(name)filePath-java.io.IOException(name)e,o,o,o
JMeter,1290,org.apache.jmeter.protocol.http.modifier.AnchorModifier.addFormUrls,153,debug,java.lang.String(name)Matched!,o,x,o
JMeter,1284,org.apache.jmeter.protocol.http.modifier.AnchorModifier.process,90,debug,"java.lang.String(name)""Selected: "" + url.toString()",o,x,o
JMeter,1282,org.apache.jmeter.protocol.http.modifier.AnchorModifier.process,68,info,java.lang.String(name)Can't apply HTML Link Parser when the previous sampler run is not an HTTP Request.,o,o,o
JMeter,1318,org.apache.jmeter.protocol.http.parser.HTMLParser.extractIEVersion,205,info,java.lang.String(name)userAgent is null,o,o,x
JMeter,1314,org.apache.jmeter.protocol.http.parser.HtmlParsingUtils.recurseForm,332,warn,"java.lang.String(name)""Some bad HTML "" + printNode(tempNode)-java.lang.Exception(name)ex",o,x,o
JMeter,1321,org.apache.jmeter.protocol.http.parser.RegexpHTMLParser.getEmbeddedResourceURLs,149,debug,java.lang.String(name)match groups {} {}-int(name)match.groups()-org.apache.oro.text.regex.MatchResult(name)match,x,o,x
JMeter,1440,org.apache.jmeter.protocol.http.proxy.gui.ProxyControlGui.keyReleased,696,debug,java.lang.String(name)Using port {} for recording-int(name)port,o,o,o
JMeter,1443,org.apache.jmeter.protocol.http.proxy.gui.ProxyControlGui.reinitializeTargetCombo,1197,debug,java.lang.String(name)Reinitialization complete,o,o,o
JMeter,1425,org.apache.jmeter.protocol.http.proxy.HttpRequestHdr.parseFirstLine,185,debug,"java.lang.String(name)parsed method: {}, url/host: {}, version: {}-java.lang.String(name)method-java.lang.String(name)url-java.lang.String(name)version",o,o,x
JMeter,1391,org.apache.jmeter.protocol.http.proxy.Proxy.run,165,debug,java.lang.String(name){} ====================================================================-java.lang.String(name)port,x,x,x
JMeter,1397,org.apache.jmeter.protocol.http.proxy.Proxy.run,203,warn,java.lang.String(name){} Problem with SSL certificate for url {}? Ensure browser is set to accept the JMeter proxy cert: {}-java.lang.String(name)port-java.lang.String(name)url-java.lang.String(name)ioe.getMessage(),o,o,x
JMeter,1399,org.apache.jmeter.protocol.http.proxy.Proxy.run,213,warn,java.lang.String(name){} Empty response to http over SSL. Probably waiting for user to authorize the certificate for {}-java.lang.String(name)port-java.lang.String(name)request.getUrl(),o,o,x
JMeter,1414,org.apache.jmeter.protocol.http.proxy.Proxy.startSSL,445,warn,"java.lang.String(name){} Unable to negotiate SSL transaction, no keystore?-java.lang.String(name)port",o,o,x
JMeter,1348,org.apache.jmeter.protocol.http.proxy.ProxyControl.deliverSampler,623,debug,java.lang.String(name)Sample excluded based on url or content-type: {} - {}-java.lang.String(name)result.getUrlAsString()-java.lang.String(name)result.getContentType(),o,o,o
JMeter,1377,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1535,info,"java.lang.String(name)Creating HTTP(S) Test Script Recorder Root CA in {}, ensure you install certificate in your Browser for recording-java.lang.String(name)CERT_PATH_ABS",o,o,o
JMeter,1371,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1500,info,java.lang.String(name)Valid alias found for {}-java.lang.String(name)alias,o,o,o
JMeter,1379,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1549,info,java.lang.String(name)Creating entry {} in {}-java.lang.String(name)subject-java.lang.String(name)CERT_PATH_ABS,o,o,o
JMeter,1375,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1521,warn,"java.lang.String(name)Existing ROOT Certificate is not yet valid, a new one will be created, ensure you install it in browser, message: {}-java.lang.String(name)e.getMessage()-java.security.cert.CertificateNotYetValidException(name)e",o,o,o
JMeter,1372,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1506,warn,"java.lang.String(name)Could not read key store {}; cause: {}, a new one will be created, ensure you install it in browser-java.lang.String(name)e.getMessage()-java.lang.String(name)e.getCause().getMessage()-java.io.IOException(name)e",o,o,o
JMeter,1374,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1516,warn,"java.lang.String(name)Existing ROOT Certificate has expired, a new one will be created, ensure you install it in browser, message: {}-java.lang.String(name)e.getMessage()-java.security.cert.CertificateExpiredException(name)e",o,o,o
JMeter,1373,org.apache.jmeter.protocol.http.proxy.ProxyControl.initDynamicKeyStore,1510,warn,"java.lang.String(name)Could not open/read key store {}, a new one will be created, ensure you install it in browser-java.lang.String(name)e.getMessage()-java.io.IOException(name)e",o,o,o
JMeter,1383,org.apache.jmeter.protocol.http.proxy.ProxyControl.initJMeterKeyStore,1600,info,java.lang.String(name)Generating standard keypair in {}-java.lang.String(name)CERT_PATH_ABS,o,o,o
JMeter,1384,org.apache.jmeter.protocol.http.proxy.ProxyControl.initJMeterKeyStore,1602,warn,"java.lang.String(name)Could not delete {}, this could create issues, stop jmeter, ensure file is deleted and restart again-java.lang.String(name)CERT_PATH.getAbsolutePath()",o,o,x
JMeter,1382,org.apache.jmeter.protocol.http.proxy.ProxyControl.initJMeterKeyStore,1591,warn,java.lang.String(name)Could not open expected file or certificate is not valid {} {}-java.lang.String(name)CERT_PATH_ABS-java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,x,o,o
JMeter,1368,org.apache.jmeter.protocol.http.proxy.ProxyControl.initKeyStore,1455,info,java.lang.String(name)HTTP(S) Test Script Recorder will use the keystore '{}' with the alias: '{}'-java.lang.String(name)CERT_PATH_ABS-java.lang.String(name)CERT_ALIAS,o,o,o
JMeter,1370,org.apache.jmeter.protocol.http.proxy.ProxyControl.initUserKeyStore,1480,error,java.lang.String(name)Could not open keystore or certificate is not valid {} {}-java.lang.String(name)CERT_PATH_ABS-java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,x,o,o
JMeter,1369,org.apache.jmeter.protocol.http.proxy.ProxyControl.initUserKeyStore,1473,error,java.lang.String(name)Could not find key with alias {}-java.lang.String(name)CERT_ALIAS,o,o,o
JMeter,1363,org.apache.jmeter.protocol.http.proxy.ProxyControl.putSamplesIntoModel,1176,error,java.lang.String(name)Error placing sampler-org.apache.jmeter.exceptions.IllegalUserActionException(name)ex,o,o,o
JMeter,1345,org.apache.jmeter.protocol.http.proxy.ProxyControl.startProxy,505,error,java.lang.String(name)Could not initialise key store-java.security.GeneralSecurityException(name)e,o,o,o
JMeter,1346,org.apache.jmeter.protocol.http.proxy.ProxyControl.startProxy,508,error,java.lang.String(name)Could not initialise key store-java.io.IOException(name)e,o,o,o
JMeter,1356,org.apache.jmeter.protocol.http.proxy.ProxyControl.testPattern,857,warn,java.lang.String(name)Skipped invalid content pattern: {}-java.lang.String(name)expression-org.apache.oro.text.MalformedCachePatternException(name)e,o,o,o
JMeter,1343,org.apache.jmeter.protocol.http.proxy.SamplerCreatorFactory.init,78,error,java.lang.String(name)Exception registering {} with implementation:{}-java.lang.String(name)SamplerCreator.class.getName()-java.lang.String(name)strClassName-java.lang.Exception(name)e,o,o,o
JMeter,1344,org.apache.jmeter.protocol.http.proxy.SamplerCreatorFactory.init,83,error,java.lang.String(name)Exception finding implementations of {}-java.lang.Class<SamplerCreator>(name)SamplerCreator.class-java.io.IOException(name)e,o,o,o
JMeter,1484,org.apache.jmeter.protocol.http.sampler.AccessLogSamplerBeanInfo.logParsers,42,warn,java.lang.String(name)Could not find log parsers.-java.io.IOException(name)e,o,o,o
JMeter,1533,org.apache.jmeter.protocol.http.sampler.AjpSampler.threadFinished,138,debug,java.lang.String(name)Error closing channel-java.io.IOException(name)iex,o,o,o
JMeter,1483,org.apache.jmeter.protocol.http.sampler.HttpClientDefaultParameters.load,126,error,"java.lang.String(name)""Problem loading properties "" + e.toString()",o,o,o
JMeter,1512,org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.notifyFirstSampleAfterLoopRestart,1849,debug,java.lang.String(name)Thread state will be reset ?: {}-boolean(name)RESET_STATE_ON_THREAD_GROUP_ITERATION,o,o,x
JMeter,1499,org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.sample,734,debug,java.lang.String(name)IOException-java.io.IOException(name)e,x,x,x
JMeter,1506,org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.setupClient,1158,debug,java.lang.String(name)Created new HttpClient: @{} {}-int(name)System.identityHashCode(httpClient)-org.apache.jmeter.protocol.http.sampler.HttpClientKey(name)key,x,o,o
JMeter,1507,org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.setupClient,1164,debug,java.lang.String(name)Reusing the HttpClient: @{} {}-int(name)System.identityHashCode(httpClient)-org.apache.jmeter.protocol.http.sampler.HttpClientKey(name)key,x,o,o
JMeter,1456,org.apache.jmeter.protocol.http.sampler.HTTPSamplerBase.getQueryString,1178,warn,"java.lang.String(name)Unable to encode parameter in encoding {}, parameter value not included in query string-java.lang.String(name)lContentEncoding",o,o,o
JMeter,1444,org.apache.jmeter.protocol.http.sampler.HTTPSamplerBase.hasNoMissingFile,439,warn,java.lang.String(name)File {} is invalid as no path is defined-org.apache.jmeter.protocol.http.util.HTTPFileArg(name)httpFileArg,o,o,o
JMeter,1465,org.apache.jmeter.protocol.http.sampler.HTTPSamplerBase.registerParser,1464,info,java.lang.String(name)Parser for {} is {}-java.lang.String(name)contentType-java.lang.String(name)className,o,o,o
JMeter,1560,org.apache.jmeter.protocol.http.util.accesslog.TCLogParser.parseLine,300,debug,java.lang.String(name)filter was null,o,x,x
JMeter,1556,org.apache.jmeter.protocol.http.util.accesslog.TCLogParser.parseLine,280,debug,"java.lang.String(name)""parsing line: "" + line",o,o,x
JMeter,1570,org.apache.jmeter.protocol.java.sampler.BeanShellSampler.interrupt,175,debug,java.lang.String(name){} : {}-java.lang.Class<>(name)getClass()-java.lang.String(name)ignored.getLocalizedMessage(),x,x,x
JMeter,1568,org.apache.jmeter.protocol.java.sampler.BeanShellSampler.sample,145,error,java.lang.String(name)BeanShell Jar missing? {}-java.lang.String(name)ex.toString(),o,o,x
JMeter,1575,org.apache.jmeter.protocol.java.sampler.BSFSampler.sample,65,debug,java.lang.String(name){} {}-java.lang.String(name)label-java.lang.String(name)fileName,x,x,x
JMeter,1755,org.apache.jmeter.protocol.java.sampler.JUnitSampler.getClassInstance,536,error,java.lang.String(name)No empty constructor nor string constructor found for class:{}-java.lang.Class<>(name)theclazz,o,o,o
JMeter,1756,org.apache.jmeter.protocol.java.sampler.JUnitSampler.getClassInstance,540,error,java.lang.String(name)Error instantiating class:{}:{}-java.lang.Class<>(name)theclazz-java.lang.String(name)e.getMessage()-java.lang.ReflectiveOperationException(name)e,o,o,o
JMeter,1752,org.apache.jmeter.protocol.java.sampler.JUnitSampler.getClassInstance,496,warn,java.lang.String(name)ClassNotFoundException:: {}-java.lang.String(name)e.getMessage(),x,x,x
JMeter,1750,org.apache.jmeter.protocol.java.sampler.JUnitSampler.sample,413,info,java.lang.String(name)caught exception-java.lang.reflect.InvocationTargetException(name)e,o,x,x
JMeter,1751,org.apache.jmeter.protocol.java.sampler.JUnitSampler.sample,416,info,java.lang.String(name)caught exception-java.lang.reflect.InvocationTargetException(name)e,o,x,x
JMeter,1589,org.apache.jmeter.protocol.jdbc.config.DataSourceElement.testEnded,99,error,java.lang.String(name)Error closing pool:{}-java.lang.String(name)getName()-java.sql.SQLException(name)ex,o,o,o
JMeter,1602,org.apache.jmeter.protocol.jdbc.processor.AbstractJDBCProcessor.process,49,warn,java.lang.String(name)SQL Problem in {}: {}-java.lang.String(name)getName()-java.lang.String(name)ex.toString(),o,o,o
JMeter,1603,org.apache.jmeter.protocol.jdbc.processor.AbstractJDBCProcessor.process,51,warn,"java.lang.String(name)""IO Problem in {}: {}"" + getName()-java.lang.String(name)ex.toString()",o,o,o
JMeter,1604,org.apache.jmeter.protocol.jdbc.processor.AbstractJDBCProcessor.process,53,warn,java.lang.String(name)Execution Problem in {}: {}-java.lang.String(name)getName()-java.lang.String(name)ex.toString(),o,o,o
JMeter,1615,org.apache.jmeter.protocol.jms.client.ReceiveSubscriber.close,348,debug,java.lang.String(name)close(),x,x,x
JMeter,1617,org.apache.jmeter.protocol.jms.client.ReceiveSubscriber.onMessage,369,warn,java.lang.String(name)Could not add message to queue,o,o,o
JMeter,1613,org.apache.jmeter.protocol.jms.client.ReceiveSubscriber.start,295,debug,java.lang.String(name)start(),x,x,x
JMeter,1614,org.apache.jmeter.protocol.jms.client.ReceiveSubscriber.stop,305,debug,java.lang.String(name)stop(),x,x,x
JMeter,1669,org.apache.jmeter.protocol.jms.sampler.FixedQueueExecutor.sendAndReceive,91,debug,java.lang.String(name){} will wait for reply {} started on {}-java.lang.String(name)Thread.currentThread().getName()-java.lang.String(name)id-long(name)System.currentTimeMillis(),o,o,o
JMeter,1671,org.apache.jmeter.protocol.jms.sampler.FixedQueueExecutor.sendAndReceive,101,debug,java.lang.String(name)Timeout {} ms reached before getting a reply message-int(name)timeout,o,o,o
JMeter,1642,org.apache.jmeter.protocol.jms.sampler.JMSSampler.browseQueueDetails,429,error,java.lang.String(name)Error browsing messages on the queue {}-java.lang.String(name)queueName-java.lang.Exception(name)e,o,o,o
JMeter,1633,org.apache.jmeter.protocol.jms.sampler.JMSSampler.handleBrowse,220,debug,java.lang.String(name)isBrowseOnly,x,x,x
JMeter,1634,org.apache.jmeter.protocol.jms.sampler.JMSSampler.handleClearQueue,231,debug,java.lang.String(name)isClearQueue,x,x,x
JMeter,1635,org.apache.jmeter.protocol.jms.sampler.JMSSampler.handleOneWay,242,debug,java.lang.String(name)isOneWay,x,x,x
JMeter,1636,org.apache.jmeter.protocol.jms.sampler.JMSSampler.handleRead,252,debug,java.lang.String(name)isRead,x,x,x
JMeter,1637,org.apache.jmeter.protocol.jms.sampler.JMSSampler.handleRequestResponse,298,debug,java.lang.String(name)NO TEMP QUEUE,x,x,x
JMeter,1654,org.apache.jmeter.protocol.jms.sampler.JMSSampler.printEnvironment,734,debug,java.lang.String(name){}={}-(name)entry.getKey()-(name)entry.getValue(),x,x,x
JMeter,1626,org.apache.jmeter.protocol.jms.sampler.SubscriberSampler.sample,190,warn,java.lang.String(name)Error [{}] {}-java.lang.String(name)errorCode-java.lang.String(name)e.toString()-javax.jms.JMSException(name)e,o,x,o
JMeter,1605,org.apache.jmeter.protocol.jms.Utils.close,59,error,java.lang.String(name)Error during close: -javax.jms.JMSException(name)e,o,x,o
JMeter,1759,org.apache.jmeter.protocol.ldap.sampler.LdapExtClient.connect,109,info,java.lang.String(name)prov_url= {}-java.lang.String(name)env.get(Context.PROVIDER_URL),x,x,x
JMeter,1769,org.apache.jmeter.protocol.ldap.sampler.LDAPExtSampler.sample,746,debug,java.lang.String(name)performing test: {}-java.lang.String(name)testType,o,o,x
JMeter,1774,org.apache.jmeter.protocol.mail.sampler.MailReaderSampler.sample,182,info,"java.lang.String(name)""load local truststore -Failed to load truststore from: "" + truststore.getAbsolutePath() + "". Local truststore not available, aborting execution.""",o,o,x
JMeter,1799,org.apache.jmeter.protocol.mongodb.config.MongoSourceElement.testEnded,169,debug,"java.lang.String(name)getTitle() + "" testEnded""",x,x,x
JMeter,1798,org.apache.jmeter.protocol.mongodb.config.MongoSourceElement.testStarted,150,debug,"java.lang.String(name)getSource() + "" is being defined.""",o,o,x
JMeter,1796,org.apache.jmeter.protocol.mongodb.config.MongoSourceElement.testStarted,140,debug,"java.lang.String(name)""options : "" + mongoOptions.toString()",o,x,x
JMeter,1795,org.apache.jmeter.protocol.mongodb.config.MongoSourceElement.testStarted,112,debug,"java.lang.String(name)getTitle() + "" testStarted""",x,x,x
JMeter,1804,org.apache.jmeter.protocol.mongodb.mongo.MongoDB.clear,67,debug,java.lang.String(name)clearing,o,x,x
JMeter,1790,org.apache.jmeter.protocol.smtp.sampler.protocol.SendMailCommand.execute,341,debug,java.lang.String(name)transport closed,o,x,x
JMeter,1791,org.apache.jmeter.protocol.smtp.sampler.protocol.SendMailCommand.execute,344,debug,java.lang.String(name)message sent,o,x,x
JMeter,1792,org.apache.jmeter.protocol.smtp.sampler.protocol.SynchronousTransportListener.messageDelivered,51,debug,java.lang.String(name)Message delivered,o,x,o
JMeter,1783,org.apache.jmeter.protocol.smtp.sampler.SmtpSampler.attachmentToFile,222,debug,"java.lang.String(name)""file path set to: "" + attachment",o,o,x
JMeter,1816,org.apache.jmeter.protocol.tcp.sampler.BinaryTCPClientImpl.write,93,debug,"java.lang.String(name)""Wrote: "" + hexEncodedBinary",o,x,o
JMeter,1847,org.apache.jmeter.protocol.tcp.sampler.TCPClientImpl.read,118,debug,java.lang.String(name)Read: {} {}-int(name)w.size()-java.lang.String(name)w.toString(),x,x,o
JMeter,1830,org.apache.jmeter.protocol.tcp.sampler.TCPSampler.getClass,322,error,java.lang.String(name)Could not find protocol class '{}'-java.lang.String(name)className,o,o,o
JMeter,1827,org.apache.jmeter.protocol.tcp.sampler.TCPSampler.getSocket,177,warn,java.lang.String(name)Could not create socket for {}-java.lang.String(name)getLabel()-java.io.IOException(name)e,o,o,o
JMeter,647,org.apache.jmeter.report.config.ReportGeneratorConfiguration.getOptionalProperty,500,debug,java.lang.String(name)Use '{}' value for optional property '{}'-TProperty(name)property-java.lang.String(name)key,o,o,o
JMeter,648,org.apache.jmeter.report.config.ReportGeneratorConfiguration.getRequiredProperty,509,debug,java.lang.String(name)Use '{}' value for required property '{}'-TProperty(name)property-java.lang.String(name)key,o,o,o
JMeter,642,org.apache.jmeter.report.config.ReportGeneratorConfiguration.initialize,145,debug,java.lang.String(name)Load configuration for exporter '{}'-java.lang.String(name)exportId,o,o,o
JMeter,644,org.apache.jmeter.report.config.ReportGeneratorConfiguration.initialize,231,debug,java.lang.String(name)Load configuration for graph '{}'-java.lang.String(name)graphId,o,o,o
JMeter,651,org.apache.jmeter.report.config.ReportGeneratorConfiguration.loadFromProperties,616,debug,java.lang.String(name)Loading properties: {}-java.util.Properties(name)properties,o,o,o
JMeter,649,org.apache.jmeter.report.config.ReportGeneratorConfiguration.loadSubConfiguration,571,warn,"java.lang.String(name)Invalid property '{}', skip it.-java.lang.String(name)key",o,o,o
JMeter,662,org.apache.jmeter.report.core.CsvSampleReader.assertCorrectColumns,201,warn,"java.lang.String(name)Short CSV read around line {} of file '{}'. Could only read {} elements of {} expected. Data is [{}]-long(name)row + 2-java.io.File(name)file-int(name)data.length-int(name)columnCount-java.lang.String(name)String.join("", "",data)",o,o,o
JMeter,661,org.apache.jmeter.report.core.CsvSampleReader.readMetadata,154,warn,"java.lang.String(name)File '{}' does not contain the field names header, ensure the jmeter.save.saveservice.* properties are the same as when the CSV file was created or the file may be read incorrectly when generating report-java.lang.String(name)file.getAbsolutePath()",o,o,o
JMeter,668,org.apache.jmeter.report.dashboard.HtmlTemplateExporter.export,297,debug,java.lang.String(name)Start template processing,o,o,o
JMeter,696,org.apache.jmeter.report.dashboard.JsonExporter.export,76,info,java.lang.String(name)Checking output folder,o,o,o
JMeter,690,org.apache.jmeter.report.dashboard.ReportGenerator.addGraphConsumer,359,warn,"java.lang.String(name)Unable to add class:{} as consumer for HTML report generation, check class name or that the plugin that contains it is on classpath-java.lang.String(name)className-java.lang.ClassNotFoundException(name)ex",o,o,x
JMeter,681,org.apache.jmeter.report.dashboard.ReportGenerator.generate,199,debug,java.lang.String(name)Start report generation,o,o,o
JMeter,682,org.apache.jmeter.report.dashboard.ReportGenerator.generate,239,debug,java.lang.String(name)Start samples processing,o,o,o
JMeter,684,org.apache.jmeter.report.dashboard.ReportGenerator.generate,247,debug,java.lang.String(name)Start data exporting,o,o,o
JMeter,676,org.apache.jmeter.report.dashboard.ReportGenerator.ReportGenerator,141,info,java.lang.String(name)Will generate report at end of test from results file: {}-java.lang.String(name)resultsFile,o,o,o
JMeter,691,org.apache.jmeter.report.dashboard.ReportGenerator.setProperty,546,warn,"java.lang.String(name)'{}' is not a valid property for class '{}', skip it-java.lang.String(name)propertyName-java.lang.String(name)className",o,o,o
JMeter,663,org.apache.jmeter.report.dashboard.TemplateVisitor.preVisitDirectory,92,info,"java.lang.String(name)Copying folder from '{}' to '{}', got message: {}, found non empty folder with following content {}, will be ignored-java.nio.file.Path(name)file-java.nio.file.Path(name)newDir-java.lang.String(name)ex.getMessage()-File[](name)newDir.toFile().listFiles()",o,o,o
JMeter,708,org.apache.jmeter.report.processor.CsvFileSampleSource.produce,186,info,java.lang.String(name)produce(): {} samples produced in {} on channel {}-long(name)sampleCount-java.lang.String(name)time(now() - start)-int(name)i,o,o,x
JMeter,177,org.apache.jmeter.reporters.MailerModel.sendTestMail,406,info,java.lang.String(name)Test mail sent successfully!!,o,o,x
JMeter,176,org.apache.jmeter.reporters.MailerModel.sendTestMail,398,info,java.lang.String(name)attText,x,x,x
JMeter,727,org.apache.jmeter.reporters.ResultCollector.getFileWriter,484,debug,java.lang.String(name)Writing header to file: {}-java.lang.String(name)filename,o,o,o
JMeter,725,org.apache.jmeter.reporters.ResultCollector.getFileWriter,471,warn,java.lang.String(name)Error creating directories for {}-java.io.File(name)pdir,o,o,o
JMeter,721,org.apache.jmeter.reporters.ResultCollector.loadExistingFile,390,warn,java.lang.String(name)Failed to load {} using XStream. Error was: {}-java.lang.String(name)filename-java.lang.String(name)e.toString(),o,o,o
JMeter,720,org.apache.jmeter.reporters.ResultCollector.loadExistingFile,377,warn,java.lang.String(name){} is empty-java.lang.String(name)filename,o,o,o
JMeter,716,org.apache.jmeter.reporters.ResultCollector.run,81,info,java.lang.String(name)Shutdown hook started,o,o,o
JMeter,717,org.apache.jmeter.reporters.ResultCollector.run,85,info,java.lang.String(name)Shutdown hook ended,o,o,o
JMeter,740,org.apache.jmeter.reporters.ResultSaver.saveSample,205,error,java.lang.String(name)Error saving sample {}-java.lang.String(name)s.getSampleLabel()-java.io.IOException(name)e,o,o,o
JMeter,793,org.apache.jmeter.samplers.BatchSampleSender.sampleOccurred,185,error,java.lang.String(name)sampleOccurred-java.rmi.RemoteException(name)err,x,x,x
JMeter,814,org.apache.jmeter.samplers.DiskStoreSampleSender.testEnded,82,error,java.lang.String(name)Executor did not terminate in a timely fashion,o,o,o
JMeter,808,org.apache.jmeter.samplers.RemoteListenerWrapper.sampleStarted,111,error,java.lang.String(name)sampleStarted-java.rmi.RemoteException(name)err,x,x,x
JMeter,810,org.apache.jmeter.samplers.RemoteListenerWrapper.sampleStopped,121,error,java.lang.String(name)sampleStopped-java.rmi.RemoteException(name)err,x,x,x
JMeter,802,org.apache.jmeter.samplers.RemoteListenerWrapper.testStarted,62,error,java.lang.String(name)testStarted()-java.lang.Throwable(name)ex,x,x,x
JMeter,786,org.apache.jmeter.samplers.SampleSenderFactory.getInstance,81,error,"java.lang.String(name)Unable to create a sample sender from class:'{}', search for mode property in jmeter.properties for correct configuration options-java.lang.String(name)type",o,o,o
JMeter,827,org.apache.jmeter.samplers.StandardSampleSender.sampleOccurred,71,error,java.lang.String(name)sampleOccurred-java.rmi.RemoteException(name)err,x,x,x
JMeter,761,org.apache.jmeter.samplers.StatisticalSampleSender.testEnded,129,warn,java.lang.String(name)testEnded(hostname)-java.rmi.RemoteException(name)err,x,x,x
JMeter,855,org.apache.jmeter.save.converters.SampleResultConverter.readFile,470,warn,java.lang.String(name)Failed to read result file.-java.io.IOException(name)e,o,o,o
JMeter,837,org.apache.jmeter.save.CSVSaveService.getSampleSaveConfiguration,546,warn,java.lang.String(name)Default delimiter '{}' did not work; using alternate '{}' for reading {}-java.lang.String(name)_saveConfig.getDelimiter()-java.lang.String(name)delim-java.lang.String(name)filename,o,o,o
JMeter,830,org.apache.jmeter.save.CSVSaveService.makeResultFromDelimitedString,214,warn,"java.lang.String(name)Cannot parse timestamp: '{}', will try following formats {}-java.lang.String(name)text-java.util.List<String>(name)Arrays.asList(DATE_FORMAT_STRINGS)",o,o,o
JMeter,836,org.apache.jmeter.save.CSVSaveService.makeResultFromDelimitedString,377,warn,java.lang.String(name)Insufficient columns to parse field '{}' at line {}-java.lang.String(name)field-long(name)lineNumber,o,o,o
JMeter,829,org.apache.jmeter.save.CSVSaveService.processSamples,156,info,java.lang.String(name){} does not appear to have a valid header. Using default configuration.-java.lang.String(name)filename,o,o,o
JMeter,862,org.apache.jmeter.services.FileServer.readLine,356,debug,java.lang.String(name)Read:{}-java.lang.String(name)line,o,x,o
JMeter,863,org.apache.jmeter.services.FileServer.write,449,debug,java.lang.String(name)Write:{}-java.lang.String(name)value,o,x,o
JMeter,909,org.apache.jmeter.threads.JMeterThread.delay,984,debug,"java.lang.String(name)The delay would be longer than the scheduled period, so stop thread now.",o,o,o
JMeter,910,org.apache.jmeter.threads.JMeterThread.delay,991,warn,java.lang.String(name)The delay timer was interrupted - probably did not wait as long as intended.,o,o,o
JMeter,911,org.apache.jmeter.threads.JMeterThread.delayBy,1049,warn,java.lang.String(name){} delay for {} was interrupted. Waited {} milli-seconds out of {}-java.lang.String(name)type-java.lang.String(name)threadName-long(name)now - start-long(name)delay,o,o,o
JMeter,889,org.apache.jmeter.threads.JMeterThread.processSampler,504,info,java.lang.String(name)Stopping Test with interruption of current samplers: {}-java.lang.String(name)e.toString(),o,o,o
JMeter,881,org.apache.jmeter.threads.JMeterThread.run,267,debug,"java.lang.String(name)Start Next Thread Loop option is on, Last sample failed, starting next thread loop",o,o,o
JMeter,885,org.apache.jmeter.threads.JMeterThread.run,316,info,"java.lang.String(name)Stop Thread seen for thread {}, reason: {}-java.lang.String(name)getThreadName()-java.lang.String(name)e.toString()",o,o,o
JMeter,868,org.apache.jmeter.threads.RemoteThreadsListenerImpl.RemoteThreadsListenerImpl,78,error,java.lang.String(name)Exception finding implementations of {}-java.lang.Class<RemoteThreadsLifeCycleListener>(name)RemoteThreadsLifeCycleListener.class-java.io.IOException(name)e,o,o,o
JMeter,871,org.apache.jmeter.threads.ThreadGroup.start,245,info,java.lang.String(name)Started thread group number {}-int(name)groupNumber,o,o,o
JMeter,193,org.apache.jmeter.timers.poissonarrivals.ConstantPoissonProcessGenerator.generateNext,118,info,java.lang.String(name)sb.toString(),x,x,x
JMeter,191,org.apache.jmeter.timers.poissonarrivals.ConstantPoissonProcessGenerator.generateNext,89,warn,"java.lang.String(name)Spent {} ms while generating sequence of delays for {} samples, {} throughput, {} duration-long(name)t-int(name)samples-double(name)throughput-long(name)duration",o,o,o
JMeter,185,org.apache.jmeter.timers.SyncTimer.delay,189,warn,java.lang.String(name)SyncTimer {} timeouted waiting for users after: {}ms-java.lang.String(name)getName()-long(name)getTimeoutInMs(),x,o,o
JMeter,952,org.apache.jmeter.util.BeanShellInterpreter.init,139,warn,java.lang.String(name)Cannot read init file: {}-java.lang.String(name)fileToUse,o,o,o
JMeter,953,org.apache.jmeter.util.BeanShellInterpreter.init,144,warn,java.lang.String(name)Cannot source init file: {}-java.lang.String(name)fileToUse-org.apache.jorphan.util.JMeterException(name)e,o,o,o
JMeter,951,org.apache.jmeter.util.BeanShellInterpreter.init,135,warn,java.lang.String(name)Cannot find init file: {}-java.lang.String(name)initFile,o,o,o
JMeter,1006,org.apache.jmeter.util.BeanShellServer.run,107,error,java.lang.String(name)Problem starting BeanShell server-java.lang.Exception(name)e,o,o,o
JMeter,983,org.apache.jmeter.util.Document.getTextFromDocument,74,warn,java.lang.String(name)Error closing document stream-java.io.IOException(name)ioe,o,o,o
JMeter,931,org.apache.jmeter.util.JMeterUtils.getImage,645,info,java.lang.String(name)no icon for {} {}-java.lang.String(name)name-java.lang.String(name)e.getMessage(),x,o,x
JMeter,932,org.apache.jmeter.util.JMeterUtils.getPropDefault,701,warn,"java.lang.String(name)Exception '{}' occurred when fetching int property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-int(name)defaultVal",o,o,o
JMeter,933,org.apache.jmeter.util.JMeterUtils.getPropDefault,726,warn,"java.lang.String(name)Exception '{}' occurred when fetching boolean property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-boolean(name)defaultVal",o,o,o
JMeter,934,org.apache.jmeter.util.JMeterUtils.getPropDefault,744,warn,"java.lang.String(name)Exception '{}' occurred when fetching long property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-long(name)defaultVal",o,o,o
JMeter,935,org.apache.jmeter.util.JMeterUtils.getPropDefault,762,warn,"java.lang.String(name)Exception '{}' occurred when fetching float property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-float(name)defaultVal",o,o,o
JMeter,936,org.apache.jmeter.util.JMeterUtils.getPropDefault,780,warn,"java.lang.String(name)Exception '{}' occurred when fetching double property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-double(name)defaultVal",o,o,o
JMeter,937,org.apache.jmeter.util.JMeterUtils.getPropDefault,801,warn,"java.lang.String(name)Exception '{}' occurred when fetching String property:'{}', defaulting to: {}-java.lang.String(name)e.getMessage()-java.lang.String(name)propName-java.lang.String(name)defaultVal",o,o,o
JMeter,926,org.apache.jmeter.util.JMeterUtils.getResStringDefault,522,debug,"java.lang.String(name)Resource string not found: [{}], using default value {}-java.lang.String(name)resKey-java.lang.String(name)defaultValue",o,o,o
JMeter,920,org.apache.jmeter.util.JMeterUtils.loadProperties,256,warn,java.lang.String(name)Error reading {} {}-java.lang.String(name)file-java.lang.String(name)ex.toString(),x,o,o
JMeter,918,org.apache.jmeter.util.JMeterUtils.loadProperties,246,warn,java.lang.String(name)Cannot find {}-java.lang.String(name)file,o,o,o
JMeter,919,org.apache.jmeter.util.JMeterUtils.loadProperties,251,warn,java.lang.String(name)Cannot open {}-java.lang.String(name)file,o,o,o
JMeter,923,org.apache.jmeter.util.JMeterUtils.setLocale,391,info,"java.lang.String(name)Could not find resources for '{}', using '{}'-java.util.Locale(name)loc-java.util.Locale(name)resBundLocale",o,o,o
JMeter,988,org.apache.jmeter.util.JsseSSLManager.JsseSSLManager,117,error,java.lang.String(name)Could not set up SSLContext-java.security.GeneralSecurityException(name)ex,o,o,o
JMeter,1014,org.apache.jmeter.util.keystore.JmeterKeyStore.getAlias,198,error,java.lang.String(name)No var called '{}' found-java.lang.String(name)clientCertAliasVarName,o,o,o
JMeter,1017,org.apache.jmeter.util.LogRecordingDelegatingLogger.trace,62,trace,java.lang.String(name)format-java.lang.Object(name)arg,x,x,x
JMeter,1018,org.apache.jmeter.util.LogRecordingDelegatingLogger.trace,68,trace,java.lang.String(name)format-java.lang.Object(name)arg1-java.lang.Object(name)arg2,x,x,x
JMeter,1019,org.apache.jmeter.util.LogRecordingDelegatingLogger.trace,74,trace,java.lang.String(name)format-Object[](name)argArray,x,x,x
JMeter,1020,org.apache.jmeter.util.LogRecordingDelegatingLogger.trace,80,trace,java.lang.String(name)msg-java.lang.Throwable(name)t,x,x,x
JMeter,1022,org.apache.jmeter.util.LogRecordingDelegatingLogger.trace,92,trace,org.slf4j.Marker(name)marker-java.lang.String(name)format-java.lang.Object(name)arg,x,x,x
JMeter,958,org.apache.jmeter.util.SSLManager.getKeyStore,134,info,java.lang.String(name)Total of {} aliases loaded OK from PKCS11-int(name)keyStore.getAliasCount(),o,o,x
JMeter,959,org.apache.jmeter.util.SSLManager.getKeyStore,142,info,java.lang.String(name)Total of {} aliases loaded OK from keystore-int(name)keyStore.getAliasCount(),o,o,x
JMeter,965,org.apache.jmeter.util.SSLManager.getTrustStore,232,info,"java.lang.String(name)TrustStore created OK, Type: JKS",o,o,x
JMeter,966,org.apache.jmeter.util.SSLManager.getTrustStore,245,info,java.lang.String(name)Truststore loaded OK from file,o,o,x
JMeter,975,org.apache.jmeter.util.XPathUtil.putValuesForXPathInListUsingSaxon,469,warn,"java.lang.String(name)Error : {}{}-java.lang.String(name)JMeterUtils.getResString(""xpath2_extractor_match_number_failure"")-int(name)indexToMatch",x,x,o
JMeter,245,org.apache.jmeter.visualizers.backend.BackendListener.testStarted,286,debug,java.lang.String(name){} testStarted({})-java.lang.String(name)whoAmI()-java.lang.String(name)host,x,x,x
JMeter,234,org.apache.jmeter.visualizers.backend.BackendListenerGui.configure,265,error,"java.lang.String(name)Error setting class: '{}' in BackendListener: {}, check for a missing jar in your jmeter 'search_paths' and 'plugin_dependency_paths' properties-java.lang.String(name)className-java.lang.String(name)getName()",o,o,o
JMeter,263,org.apache.jmeter.visualizers.backend.graphite.GraphiteBackendListenerClient.teardownTest,359,error,java.lang.String(name)Error waiting for end of scheduler,o,o,o
JMeter,277,org.apache.jmeter.visualizers.backend.influxdb.HttpMetricsSender.destroy,243,info,java.lang.String(name)Destroying,o,x,o
JMeter,272,org.apache.jmeter.visualizers.backend.influxdb.HttpMetricsSender.writeAndSendMetrics,202,debug,"java.lang.String(name)Success, number of metrics written: {}-int(name)copyMetrics.size()",o,o,o
JMeter,274,org.apache.jmeter.visualizers.backend.influxdb.HttpMetricsSender.writeAndSendMetrics,211,error,java.lang.String(name)failed to send data to influxDB server.-java.lang.Exception(name)ex,o,o,x
JMeter,1015,org.apache.jmeter.visualizers.gui.AbstractVisualizer.stateChanged,245,debug,java.lang.String(name)getting new collector,o,o,x
JMeter,222,org.apache.jmeter.visualizers.SearchTextExtension.executeAndShowTextFind,277,debug,java.lang.String(name)lastPosition={}-int(name)lastPosition,x,x,x
JMeter,283,org.apache.jmeter.visualizers.utils.Colors.getColors,70,warn,java.lang.String(name)Could not find order list,o,o,o
JMeter,207,org.apache.jmeter.visualizers.ViewResultsFullVisualizer.actionPerformed,531,debug,java.lang.String(name)selectedTab={}-int(name)selectedTab,x,x,x
JMeter,204,org.apache.jmeter.visualizers.ViewResultsFullVisualizer.createComboRender,473,info,java.lang.String(name)Add JavaFX to your Java installation if you want to use renderer: {}-java.lang.String(name)clazz,o,o,o
JMeter,1692,org.apache.jorphan.collections.Data.removeRow,128,debug,java.lang.String(name)got to here,o,x,x
JMeter,1695,org.apache.jorphan.exec.KeyToolUtils.generateProxyCA,226,warn,"java.lang.String(name)""Problem deleting the keystore '"" + keystore + ""'""",o,o,o
JMeter,1706,org.apache.jorphan.exec.StreamCopier.run,62,warn,java.lang.String(name)Error writing stream-java.io.IOException(name)e,o,o,o
JMeter,1713,org.apache.jorphan.gui.ObjectTableModel.checkFunctors,261,error,java.lang.String(name)Cannot create instance of class {}-java.lang.String(name)objectClass.getName()-java.lang.ReflectiveOperationException(name)e,o,o,o
JMeter,1745,org.apache.jorphan.reflect.ClassFinder.findClasses,332,debug,java.lang.String(name)listClasses.size()={}-int(name)listClasses.size(),x,x,x
JMeter,1748,org.apache.jorphan.reflect.ClassFinder.findClassesInPathsDir,411,warn,java.lang.String(name){} is not a folder-java.lang.String(name)dir.getAbsolutePath(),o,o,o
Kafka,108,org.apache.kafka.clients.admin.internals.AdminMetadataManager.requestUpdate,177,debug,java.lang.String(name)Requesting metadata update.,o,o,o
Kafka,113,org.apache.kafka.clients.admin.internals.AdminMetadataManager.update,250,debug,java.lang.String(name)Updating cluster metadata to {}-org.apache.kafka.common.Cluster(name)cluster,o,o,o
Kafka,67,org.apache.kafka.clients.admin.KafkaAdminClient.close,494,debug,java.lang.String(name)Waiting for the I/O thread to exit. Hard shutdown in {} ms.-long(name)deltaMs,o,o,o
Kafka,69,org.apache.kafka.clients.admin.KafkaAdminClient.close,504,debug,java.lang.String(name)Interrupted while joining I/O thread-java.lang.InterruptedException(name)e,o,o,o
Kafka,64,org.apache.kafka.clients.admin.KafkaAdminClient.close,478,debug,java.lang.String(name)Initiating close operation.,o,o,o
Kafka,99,org.apache.kafka.clients.admin.KafkaAdminClient.enqueue,1193,debug,java.lang.String(name)Queueing {} with a timeout {} ms from now.-org.apache.kafka.clients.admin.Call(name)call-long(name)call.deadlineMs - now,o,o,o
Kafka,70,org.apache.kafka.clients.admin.KafkaAdminClient.fail,618,debug,java.lang.String(name){} aborted at {} after {} attempt(s)-org.apache.kafka.clients.admin.Call(name)this-long(name)now-int(name)tries-java.lang.Exception(name)new Exception(prettyPrintException(throwable)),o,o,o
Kafka,74,org.apache.kafka.clients.admin.KafkaAdminClient.fail,657,debug,java.lang.String(name){} failed after {} attempt(s)-org.apache.kafka.clients.admin.Call(name)this-int(name)tries-java.lang.Exception(name)new Exception(prettyPrintException(throwable)),o,o,o
Kafka,81,org.apache.kafka.clients.admin.KafkaAdminClient.maybeDrainPendingCall,904,debug,java.lang.String(name)Unable to choose node for {}-org.apache.kafka.clients.admin.Call(name)call-java.lang.Throwable(name)t,o,o,o
Kafka,79,org.apache.kafka.clients.admin.KafkaAdminClient.maybeDrainPendingCall,894,trace,java.lang.String(name)Assigned {} to node {}-org.apache.kafka.clients.admin.Call(name)call-org.apache.kafka.common.Node(name)node,o,o,o
Kafka,97,org.apache.kafka.clients.admin.KafkaAdminClient.run,1174,debug,java.lang.String(name)Timed out {} remaining operation(s).-int(name)numTimedOut,o,o,o
Kafka,94,org.apache.kafka.clients.admin.KafkaAdminClient.run,1108,trace,java.lang.String(name)Thread starting,o,x,x
Kafka,82,org.apache.kafka.clients.admin.KafkaAdminClient.sendEligibleCalls,929,trace,java.lang.String(name)Client is not ready to send to {}. Must delay {} ms-org.apache.kafka.common.Node(name)node-long(name)nodeTimeout,o,o,o
Kafka,76,org.apache.kafka.clients.admin.KafkaAdminClient.timeoutPendingCalls,828,debug,java.lang.String(name)Timed out {} pending calls.-int(name)numTimedOut,o,o,o
Kafka,3,org.apache.kafka.clients.ClusterConnectionStates.connecting,136,info,java.lang.String(name)Hostname for node {} changed from {} to {}.-java.lang.String(name)id-java.lang.String(name)connectionState.host()-java.lang.String(name)host,o,o,o
Kafka,272,org.apache.kafka.clients.consumer.internals.AbstractCoordinator.disable,1038,debug,java.lang.String(name)Disabling heartbeat thread,o,o,o
Kafka,271,org.apache.kafka.clients.consumer.internals.AbstractCoordinator.enable,1029,debug,java.lang.String(name)Enabling heartbeat thread,o,o,o
Kafka,265,org.apache.kafka.clients.consumer.internals.AbstractCoordinator.handle,902,debug,java.lang.String(name)Received successful Heartbeat response,o,o,o
Kafka,257,org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onSuccess,681,debug,java.lang.String(name)Received FindCoordinator response {}-org.apache.kafka.clients.ClientResponse(name)resp,o,o,o
Kafka,258,org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onSuccess,696,info,java.lang.String(name)Discovered group coordinator {}-org.apache.kafka.common.Node(name)coordinator,o,o,o
Kafka,280,org.apache.kafka.clients.consumer.internals.AsyncClient.sendAsyncRequest,47,error,java.lang.String(name)Could not cast response body-java.lang.ClassCastException(name)cce,o,o,o
Kafka,281,org.apache.kafka.clients.consumer.internals.AsyncClient.sendAsyncRequest,51,trace,java.lang.String(name)Received {} {} from broker {}-java.lang.String(name)resp.getClass().getSimpleName()-Resp(name)resp-org.apache.kafka.common.Node(name)node,x,o,o
Kafka,178,org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.doAutoCommitOffsetsAsync,857,debug,"java.lang.String(name)Sending asynchronous auto-commit of offsets {}-java.util.Map<TopicPartition,OffsetAndMetadata>(name)allConsumedOffsets",o,o,o
Kafka,179,org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.maybeAutoCommitOffsetsSync,878,debug,"java.lang.String(name)Sending synchronous auto-commit of offsets {}-java.util.Map<TopicPartition,OffsetAndMetadata>(name)allConsumedOffsets",o,o,o
Kafka,175,org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare,595,info,java.lang.String(name)Revoking previously assigned partitions {}-java.util.Set<TopicPartition>(name)revokedPartitions,o,o,o
Kafka,234,org.apache.kafka.clients.consumer.internals.Fetcher.nextFetchedRecord,1441,debug,java.lang.String(name)Skipping aborted record batch from partition {} with producerId {} and offsets {} to {}-org.apache.kafka.common.TopicPartition(name)partition-long(name)producerId-long(name)currentBatch.baseOffset()-long(name)currentBatch.lastOffset(),o,o,o
Kafka,216,org.apache.kafka.clients.consumer.internals.Fetcher.selectReadReplica,1044,trace,"java.lang.String(name)Not fetching from {} for partition {} since it is marked offline or is missing from our metadata, using the leader instead.-java.util.Optional<Integer>(name)nodeId-org.apache.kafka.common.TopicPartition(name)partition",o,o,o
Kafka,195,org.apache.kafka.clients.consumer.internals.Fetcher.sendFetches,248,debug,java.lang.String(name)Sending {} {} to broker {}-org.apache.kafka.common.requests.IsolationLevel(name)isolationLevel-java.lang.String(name)data.toString()-org.apache.kafka.common.Node(name)fetchTarget,x,o,o
Kafka,158,org.apache.kafka.clients.consumer.internals.SubscriptionState.maybeCompleteValidation,440,warn,"java.lang.String(name)Truncation detected for partition {} at offset {} (the end offset from the broker is {}), but no reset policy is set-org.apache.kafka.common.TopicPartition(name)tp-org.apache.kafka.clients.consumer.internals.FetchPosition(name)currentPosition-org.apache.kafka.common.requests.EpochEndOffset(name)epochEndOffset",o,o,o
Kafka,137,org.apache.kafka.clients.consumer.KafkaConsumer.close,2201,error,java.lang.String(name)Failed to close coordinator-java.lang.Throwable(name)t,o,o,o
Kafka,53,org.apache.kafka.clients.FetchSessionHandler.handleError,442,info,java.lang.String(name)Error sending fetch request {} to node {}: {}.-org.apache.kafka.common.requests.FetchMetadata(name)nextMetadata-int(name)node-java.lang.String(name)t.toString(),o,o,o
Kafka,49,org.apache.kafka.clients.FetchSessionHandler.handleResponse,406,debug,java.lang.String(name)Node {} sent a full fetch response that created a new incremental fetch session {}{}-int(name)node-int(name)response.sessionId()-java.lang.String(name)responseDataToLogString(response),x,o,o
Kafka,51,org.apache.kafka.clients.FetchSessionHandler.handleResponse,419,debug,java.lang.String(name)Node {} sent an incremental fetch response closing session {}{}-int(name)node-int(name)nextMetadata.sessionId()-java.lang.String(name)responseDataToLogString(response),x,o,o
Kafka,52,org.apache.kafka.clients.FetchSessionHandler.handleResponse,425,debug,java.lang.String(name)Node {} sent an incremental fetch response for session {}{}-int(name)node-int(name)response.sessionId()-java.lang.String(name)responseDataToLogString(response),x,o,o
Kafka,46,org.apache.kafka.clients.FetchSessionHandler.handleResponse,385,info,java.lang.String(name)Node {} was unable to process the fetch request with {}: {}.-int(name)node-org.apache.kafka.common.requests.FetchMetadata(name)nextMetadata-org.apache.kafka.common.protocol.Errors(name)response.error(),o,o,o
Kafka,43,org.apache.kafka.clients.ManualMetadataUpdater.handleFatalException,80,debug,java.lang.String(name)An error occurred in broker-to-broker communication.-org.apache.kafka.common.KafkaException(name)exception,o,o,o
Kafka,56,org.apache.kafka.clients.Metadata.updateLastSeenEpoch,180,debug,java.lang.String(name)Not replacing existing epoch {} with new epoch {} for partition {}-java.lang.Integer(name)oldEpoch-int(name)epoch-org.apache.kafka.common.TopicPartition(name)topicPartition,o,o,o
Kafka,54,org.apache.kafka.clients.Metadata.updateLastSeenEpoch,171,trace,java.lang.String(name)Determining if we should replace existing epoch {} with new epoch {}-java.lang.Integer(name)oldEpoch-int(name)epoch,o,o,o
Kafka,11,org.apache.kafka.clients.NetworkClient.close,634,warn,java.lang.String(name)Attempting to close NetworkClient that has already been closed.,o,o,o
Kafka,8,org.apache.kafka.clients.NetworkClient.doSend,499,debug,java.lang.String(name)Using older server API v{} to send {} {} with correlation id {} to node {}-short(name)header.apiVersion()-org.apache.kafka.common.protocol.ApiKeys(name)clientRequest.apiKey()-org.apache.kafka.common.requests.AbstractRequest(name)request-int(name)clientRequest.correlationId()-java.lang.String(name)destination,x,o,o
Kafka,5,org.apache.kafka.clients.NetworkClient.doSend,466,trace,java.lang.String(name)No version information found when sending {} with correlation id {} to node {}. Assuming version {}.-org.apache.kafka.common.protocol.ApiKeys(name)clientRequest.apiKey()-int(name)clientRequest.correlationId()-java.lang.String(name)nodeId-short(name)version,o,o,o
Kafka,7,org.apache.kafka.clients.NetworkClient.doSend,496,trace,java.lang.String(name)Sending {} {} with correlation id {} to node {}-org.apache.kafka.common.protocol.ApiKeys(name)clientRequest.apiKey()-org.apache.kafka.common.requests.AbstractRequest(name)request-int(name)clientRequest.correlationId()-java.lang.String(name)destination,x,o,o
Kafka,32,org.apache.kafka.clients.NetworkClient.initiateConnect,950,warn,java.lang.String(name)Error connecting to node {}-org.apache.kafka.common.Node(name)node-java.io.IOException(name)e,o,o,o
Kafka,16,org.apache.kafka.clients.NetworkClient.leastLoadedNode,692,trace,java.lang.String(name)Found least loaded node {} with no active connection-org.apache.kafka.common.Node(name)foundCanConnect,o,o,o
Kafka,14,org.apache.kafka.clients.NetworkClient.leastLoadedNode,686,trace,java.lang.String(name)Found least loaded node {} with {} inflight requests-org.apache.kafka.common.Node(name)foundReady-int(name)inflight,o,o,o
Kafka,15,org.apache.kafka.clients.NetworkClient.leastLoadedNode,689,trace,java.lang.String(name)Found least loaded connecting node {}-org.apache.kafka.common.Node(name)foundConnecting,o,o,o
Kafka,23,org.apache.kafka.clients.NetworkClient.maybeThrottle,819,trace,java.lang.String(name)Connection to node {} is throttled for {} ms until timestamp {}-java.lang.String(name)nodeId-int(name)throttleTimeMs-long(name)now + throttleTimeMs,o,o,o
Kafka,39,org.apache.kafka.clients.NetworkClient.maybeUpdate,1113,debug,java.lang.String(name)Initialize connection to node {} for sending metadata request-org.apache.kafka.common.Node(name)node,o,o,o
Kafka,38,org.apache.kafka.clients.NetworkClient.maybeUpdate,1097,debug,java.lang.String(name)Sending metadata request {} to node {}-org.apache.kafka.common.requests.Builder(name)metadataRequest-org.apache.kafka.common.Node(name)node,o,o,o
Kafka,21,org.apache.kafka.clients.NetworkClient.processDisconnection,754,trace,java.lang.String(name)Cancelled request {} {} with correlation id {} due to node {} being disconnected-org.apache.kafka.common.protocol.ApiKeys(name)request.header.apiKey()-org.apache.kafka.common.requests.AbstractRequest(name)request.request-int(name)request.header.correlationId()-java.lang.String(name)nodeId,x,o,o
Kafka,19,org.apache.kafka.clients.NetworkClient.processDisconnection,741,warn,"java.lang.String(name)Connection to node {} ({}) terminated during authentication. This may happen due to any of the following reasons: (1) Authentication failed due to invalid credentials with brokers older than 1.0.0, (2) Firewall blocking Kafka TLS traffic (eg it may only allow HTTPS traffic), (3) Transient network issue.-java.lang.String(name)nodeId-java.lang.String(name)disconnectState.remoteAddress()",o,o,o
Kafka,362,org.apache.kafka.clients.producer.internals.Sender.completeBatch,667,warn,java.lang.String(name)Received unknown topic or partition error in produce request on partition {}. The topic-partition may not exist or the user may not have Describe access to it-org.apache.kafka.common.TopicPartition(name)batch.topicPartition,o,o,o
Kafka,350,org.apache.kafka.clients.producer.internals.Sender.maybeSendTransactionalRequest,461,debug,java.lang.String(name)Disconnect from {} while trying to send request {}. Going to back off and retry.-org.apache.kafka.common.Node(name)targetNode-org.apache.kafka.common.requests.Builder<>(name)requestBuilder-java.io.IOException(name)e,o,o,o
Kafka,353,org.apache.kafka.clients.producer.internals.Sender.maybeWaitForProducerId,545,debug,java.lang.String(name)Could not find an available broker to send InitProducerIdRequest to. Will back off and retry.,o,o,o
Kafka,355,org.apache.kafka.clients.producer.internals.Sender.maybeWaitForProducerId,553,trace,java.lang.String(name)Retry InitProducerIdRequest in {}ms.-long(name)retryBackoffMs,x,o,o
Kafka,343,org.apache.kafka.clients.producer.internals.Sender.run,283,error,java.lang.String(name)Failed to close network client-java.lang.Exception(name)e,o,o,o
Kafka,321,org.apache.kafka.clients.producer.internals.TransactionManager.handleResponse,1209,debug,java.lang.String(name)Did not attempt to add partition {} to transaction because other partitions in the batch had errors.-org.apache.kafka.common.TopicPartition(name)topicPartition,o,o,o
Kafka,317,org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo,906,debug,java.lang.String(name)Transition from state {} to {}-org.apache.kafka.clients.producer.internals.State(name)currentState-org.apache.kafka.clients.producer.internals.State(name)target,o,o,o
Kafka,296,org.apache.kafka.clients.producer.KafkaProducer.close,1160,warn,java.lang.String(name)Overriding close timeout {} ms to 0 ms in order to prevent useless blocking due to self-join. This means you have incorrectly invoked close with a non-zero timeout from the producer call-back.-long(name)timeoutMs,o,o,o
Kafka,283,org.apache.kafka.clients.producer.KafkaProducer.KafkaProducer,428,debug,java.lang.String(name)Kafka producer started,o,o,o
Kafka,381,org.apache.kafka.common.memory.GarbageCollectedMemoryPool.run,108,debug,java.lang.String(name)interrupted-java.lang.InterruptedException(name)e,x,x,x
Kafka,377,org.apache.kafka.common.memory.SimpleMemoryPool.bufferToBeReleased,119,trace,java.lang.String(name)released buffer of size {}-int(name)justReleased.capacity(),o,o,x
Kafka,376,org.apache.kafka.common.memory.SimpleMemoryPool.bufferToBeReturned,114,trace,java.lang.String(name)allocated buffer of size {} -int(name)justAllocated.capacity(),o,o,x
Kafka,390,org.apache.kafka.common.metrics.Metrics.close,658,error,"java.lang.String(name)""Error when closing "" + reporter.getClass().getName()-java.lang.Exception(name)e",o,o,o
Kafka,383,org.apache.kafka.common.metrics.Metrics.sensor,416,debug,java.lang.String(name)Added sensor with name {}-java.lang.String(name)name,o,o,o
Kafka,419,org.apache.kafka.common.network.Selector.close,860,debug,java.lang.String(name)Tracking closing connection {} to process outstanding requests-java.lang.String(name)channel.id(),o,o,o
Kafka,418,org.apache.kafka.common.network.Selector.handleCloseOnAuthenticationFailure,827,error,java.lang.String(name)Exception handling close on authentication failure node {}-java.lang.String(name)channel.id()-java.lang.Exception(name)e,o,o,o
Kafka,408,org.apache.kafka.common.network.SslChannelBuilder.buildChannel,105,info,java.lang.String(name)Failed to create channel due to -java.lang.Exception(name)e,o,o,o
Kafka,435,org.apache.kafka.common.security.authenticator.AbstractLogin.login,61,info,java.lang.String(name)Successfully logged in.,o,x,o
Kafka,441,org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.setSaslState,381,debug,java.lang.String(name)Set SASL server state to {} during {}-org.apache.kafka.common.security.authenticator.SaslState(name)saslState-java.lang.String(name)reauthInfo.authenticationOrReauthenticationText(),o,o,o
Kafka,509,org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin.login,205,info,java.lang.String(name)Successfully logged in.,o,x,o
Kafka,497,org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer.evaluateResponse,97,debug,java.lang.String(name)e.getMessage(),x,x,x
Kafka,498,org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer.process,164,debug,java.lang.String(name)errorMessage,x,x,x
Kafka,474,org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule.identifyExtensions,339,error,java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,x,x
Kafka,472,org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule.identifyToken,318,error,java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,x,x,x
Kafka,471,org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule.login,308,debug,java.lang.String(name)Login succeeded; invoke commit() to commit it; current committed token count={}-int(name)committedTokenCount(),o,o,o
Kafka,534,org.apache.kafka.common.security.ssl.SslEngineBuilder.createSSLContext,157,debug,"java.lang.String(name)Created SSL context with keystore {}, truststore {}-org.apache.kafka.common.security.ssl.SecurityStore(name)keystore-org.apache.kafka.common.security.ssl.SecurityStore(name)truststore",o,o,o
Kafka,584,org.apache.kafka.common.utils.LogContext.error,698,error,java.lang.String(name)addPrefix(message)-java.lang.Object(name)arg,x,x,x
Kafka,594,org.apache.kafka.common.utils.LogContext.info,748,info,java.lang.String(name)addPrefix(message)-java.lang.Object(name)arg,x,x,x
Kafka,595,org.apache.kafka.common.utils.LogContext.info,753,info,java.lang.String(name)addPrefix(message)-java.lang.Object(name)arg1-java.lang.Object(name)arg2,x,x,x
Kafka,596,org.apache.kafka.common.utils.LogContext.info,758,info,java.lang.String(name)addPrefix(message)-Object[](name)args,x,x,x
Kafka,598,org.apache.kafka.common.utils.LogContext.info,768,info,org.apache.kafka.common.utils.Marker(name)marker-java.lang.String(name)addPrefix(msg),x,x,x
Kafka,602,org.apache.kafka.common.utils.LogContext.info,788,info,org.apache.kafka.common.utils.Marker(name)marker-java.lang.String(name)addPrefix(msg)-java.lang.Throwable(name)t,x,x,x
Kafka,542,org.apache.kafka.common.utils.Shell.runCommand,142,warn,java.lang.String(name)Error while closing the input stream-java.io.IOException(name)ioe,o,o,o
Kafka,626,org.apache.kafka.connect.cli.ConnectDistributed.startConnect,128,info,java.lang.String(name)Kafka Connect distributed worker initialization took {}ms-long(name)time.hiResClockMs() - initStart,x,o,o
Kafka,632,org.apache.kafka.connect.cli.ConnectStandalone.main,100,info,java.lang.String(name)Kafka Connect standalone worker initialization took {}ms-long(name)time.hiResClockMs() - initStart,x,o,o
Kafka,608,org.apache.kafka.connect.file.FileStreamSourceTask.poll,96,error,java.lang.String(name)Error while trying to seek to previous offset in file {}: -java.lang.String(name)filename-java.io.IOException(name)e,o,o,o
Kafka,615,org.apache.kafka.connect.file.FileStreamSourceTask.stop,209,trace,java.lang.String(name)Stopping,o,x,o
Kafka,1046,org.apache.kafka.connect.integration.MonitorableSinkConnector.put,127,trace,java.lang.String(name)Task {} obtained record (key='{}' value='{}')-java.lang.String(name)taskId-java.lang.Object(name)rec.key()-java.lang.Object(name)rec.value(),o,o,o
Kafka,1044,org.apache.kafka.connect.integration.MonitorableSinkConnector.start,109,debug,java.lang.String(name)Starting task {}-java.lang.String(name)taskId,o,o,o
Kafka,1043,org.apache.kafka.connect.integration.MonitorableSinkConnector.start,55,info,"java.lang.String(name)Starting connector {}-java.lang.String(name)props.get(""name"")",o,o,o
Kafka,1033,org.apache.kafka.connect.integration.TaskHandle.TaskHandle,46,info,java.lang.String(name)Created task {} for connector {}-java.lang.String(name)taskId-org.apache.kafka.connect.integration.ConnectorHandle(name)connectorHandle,o,o,o
Kafka,2,org.apache.kafka.connect.rest.basic.auth.extension.PropertyFileLoginModule.initialize,73,error,java.lang.String(name)Error loading credentials file -java.io.IOException(name)e,o,o,o
Kafka,640,org.apache.kafka.connect.runtime.ConnectMetrics.ConnectMetrics,83,debug,java.lang.String(name)Registering Connect metrics with JMX for worker '{}'-java.lang.String(name)workerId,o,o,o
Kafka,641,org.apache.kafka.connect.runtime.ConnectMetrics.stop,155,debug,java.lang.String(name)Unregistering Connect metrics with JMX for worker '{}'-java.lang.String(name)workerId,o,o,o
Kafka,796,org.apache.kafka.connect.runtime.distributed.DistributedHerder.connectorConfig,606,trace,java.lang.String(name)Submitting connector config read request {}-java.lang.String(name)connName,o,o,o
Kafka,798,org.apache.kafka.connect.runtime.distributed.DistributedHerder.deleteConnectorConfig,633,trace,java.lang.String(name)Removing connector config {} {}-java.lang.String(name)connName-java.util.Set<String>(name)configState.connectors(),x,o,o
Kafka,820,org.apache.kafka.connect.runtime.distributed.DistributedHerder.getTaskStartingCallable,1053,error,java.lang.String(name)Couldn't instantiate task {} because it has an invalid task configuration. This task will not execute until reconfigured.-org.apache.kafka.connect.util.ConnectorTaskId(name)taskId-java.lang.Throwable(name)t,o,o,o
Kafka,791,org.apache.kafka.connect.runtime.distributed.DistributedHerder.halt,507,info,java.lang.String(name)Stopping connectors and tasks that are still assigned to this worker.,o,o,o
Kafka,811,org.apache.kafka.connect.runtime.distributed.DistributedHerder.handleRebalanceCompleted,909,warn,java.lang.String(name)Catching up to assignment's config offset.,o,o,o
Kafka,831,org.apache.kafka.connect.runtime.distributed.DistributedHerder.onConnectorConfigRemove,1255,info,java.lang.String(name)Connector {} config removed-java.lang.String(name)connector,o,o,o
Kafka,799,org.apache.kafka.connect.runtime.distributed.DistributedHerder.putConnectorConfig,664,trace,java.lang.String(name)Submitting connector config write request {}-java.lang.String(name)connName,o,o,o
Kafka,801,org.apache.kafka.connect.runtime.distributed.DistributedHerder.putConnectorConfig,685,trace,java.lang.String(name)Submitting connector config {} {} {}-java.lang.String(name)connName-boolean(name)allowReplace-java.util.Set<String>(name)configState.connectors(),x,o,o
Kafka,806,org.apache.kafka.connect.runtime.distributed.DistributedHerder.putTaskConfigs,755,trace,java.lang.String(name)Submitting put task configuration request {}-java.lang.String(name)connName,o,o,o
Kafka,815,org.apache.kafka.connect.runtime.distributed.DistributedHerder.readConfigToEnd,974,info,"java.lang.String(name)Finished reading to end of log and updated config snapshot, new config log offset: {}-long(name)configState.offset()",o,o,o
Kafka,825,org.apache.kafka.connect.runtime.distributed.DistributedHerder.reconfigureConnectorTasksWithRetry,1140,error,java.lang.String(name)Unexpected error during connector task reconfiguration: -java.lang.Throwable(name)error,o,o,o
Kafka,802,org.apache.kafka.connect.runtime.distributed.DistributedHerder.requestTaskReconfiguration,703,trace,java.lang.String(name)Submitting connector task reconfiguration request {}-java.lang.String(name)connName,o,o,o
Kafka,782,org.apache.kafka.connect.runtime.distributed.DistributedHerder.run,238,info,java.lang.String(name)Herder starting,o,o,o
Kafka,783,org.apache.kafka.connect.runtime.distributed.DistributedHerder.run,242,info,java.lang.String(name)Herder started,o,o,o
Kafka,805,org.apache.kafka.connect.runtime.distributed.DistributedHerder.taskConfigs,727,trace,java.lang.String(name)Submitting get task configuration request {}-java.lang.String(name)connName,o,o,o
Kafka,835,org.apache.kafka.connect.runtime.distributed.DistributedHerder.updateDeletedConnectorStatus,1373,debug,java.lang.String(name)Cleaning status information for connector {}-java.lang.String(name)connector,o,o,o
Kafka,869,org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeAssignor.handleLostAssignments,350,debug,"java.lang.String(name)""Found the following connectors and tasks missing from previous assignments: "" + lostAssignments",o,o,o
Kafka,854,org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeAssignor.performTaskAssignment,190,debug,java.lang.String(name)Remaining (excluding deleted) active assignments: {}-org.apache.kafka.connect.runtime.distributed.ConnectorsAndTasks(name)remainingActive,o,o,o
Kafka,857,org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeAssignor.performTaskAssignment,204,debug,java.lang.String(name)Complete (ignoring deletions) worker assignments: {}-java.util.List<WorkerLoad>(name)completeWorkerAssignment,o,o,o
Kafka,858,org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeAssignor.performTaskAssignment,209,debug,"java.lang.String(name)Complete (ignoring deletions) connector assignments: {}-java.util.Map<String,Collection<String>>(name)connectorAssignments",o,o,o
Kafka,859,org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeAssignor.performTaskAssignment,214,debug,"java.lang.String(name)Complete (ignoring deletions) task assignments: {}-java.util.Map<String,Collection<ConnectorTaskId>>(name)taskAssignments",o,o,o
Kafka,777,org.apache.kafka.connect.runtime.distributed.WorkerCoordinator.onJoinComplete,184,debug,java.lang.String(name)After revocations snapshot of assignment: {}-org.apache.kafka.connect.runtime.distributed.ExtendedAssignment(name)assignmentSnapshot,o,o,o
Kafka,779,org.apache.kafka.connect.runtime.distributed.WorkerCoordinator.onJoinPrepare,203,info,java.lang.String(name)Rebalance started,o,o,o
Kafka,883,org.apache.kafka.connect.runtime.distributed.WorkerGroupMember.stop,218,debug,java.lang.String(name)The Connect group member has stopped.,o,o,o
Kafka,886,org.apache.kafka.connect.runtime.errors.LogReporter.report,62,error,java.lang.String(name)message(context)-java.lang.Throwable(name)context.error(),x,x,x
Kafka,907,org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.addAliases,394,info,java.lang.String(name)Added alias '{}' to plugin '{}'-java.lang.String(name)simple-java.lang.String(name)plugin.className(),o,o,o
Kafka,895,org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.connectorLoader,146,error,java.lang.String(name)Plugin class loader for connector: '{}' was not found. Returning: {}-java.lang.String(name)connectorClassOrAlias-org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader(name)this,o,o,o
Kafka,906,org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.loadClass,369,trace,java.lang.String(name)Retrieving loaded class '{}' from '{}'-java.lang.String(name)fullName-java.lang.ClassLoader(name)pluginLoader,o,o,o
Kafka,909,org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scan,423,warn,java.lang.String(name)could not create Vfs.Dir from url. ignoring the exception and continuing-org.apache.kafka.connect.runtime.isolation.ReflectionsException(name)e,o,o,x
Kafka,893,org.apache.kafka.connect.runtime.isolation.Plugins.newHeaderConverter,323,debug,java.lang.String(name)Configuring the header converter with configuration keys:{}{}-java.lang.String(name)System.lineSeparator()-java.util.Set<String>(name)converterConfig.keySet(),x,o,o
Kafka,919,org.apache.kafka.connect.runtime.rest.ConnectRestConfigurable.allowedToRegister,125,warn,java.lang.String(name)The resource {} is already registered-java.lang.Object(name)component,o,o,o
Kafka,920,org.apache.kafka.connect.runtime.rest.ConnectRestConfigurable.allowedToRegister,133,warn,java.lang.String(name)The resource {} is already registered-java.lang.Class<>(name)componentClass,o,o,o
Kafka,928,org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.completeOrForwardRequest,308,debug,java.lang.String(name)Forwarding request {} {} {}-java.lang.String(name)forwardUrl-java.lang.String(name)method-java.lang.Object(name)body,x,o,o
Kafka,934,org.apache.kafka.connect.runtime.standalone.StandaloneHerder.updateConnectorTasks,325,info,java.lang.String(name)Skipping update of connector {} since it is not running-java.lang.String(name)connName,o,o,o
Kafka,738,org.apache.kafka.connect.runtime.Worker.awaitStopTask,718,debug,java.lang.String(name)Graceful stop of task {} succeeded.-org.apache.kafka.connect.util.ConnectorTaskId(name)task.id(),o,o,o
Kafka,737,org.apache.kafka.connect.runtime.Worker.awaitStopTask,715,error,java.lang.String(name)Graceful stop of task {} failed.-org.apache.kafka.connect.util.ConnectorTaskId(name)task.id(),o,o,o
Kafka,731,org.apache.kafka.connect.runtime.Worker.buildWorkerTask,507,info,java.lang.String(name)Initializing: {}-org.apache.kafka.connect.runtime.TransformationChain<SourceRecord>(name)transformationChain,o,o,o
Kafka,732,org.apache.kafka.connect.runtime.Worker.buildWorkerTask,522,info,java.lang.String(name)Initializing: {}-org.apache.kafka.connect.runtime.TransformationChain<SinkRecord>(name)transformationChain,o,o,o
Kafka,739,org.apache.kafka.connect.runtime.Worker.setTargetState,791,info,java.lang.String(name)Setting connector {} state to {}-java.lang.String(name)connName-org.apache.kafka.connect.runtime.TargetState(name)state,o,o,o
Kafka,714,org.apache.kafka.connect.runtime.Worker.startConnector,246,info,java.lang.String(name)Creating connector {} of type {}-java.lang.String(name)connName-java.lang.String(name)connClass,o,o,o
Kafka,665,org.apache.kafka.connect.runtime.WorkerConfig.logDeprecatedProperty,332,info,"java.lang.String(name)Worker configuration property '{}'{} is deprecated and may be removed in an upcoming release. The specified value '{}' matches the default, so this property can be safely removed from the worker configuration.-java.lang.String(name)propName-java.lang.String(name)prefixNotice-java.lang.String(name)propValue",o,o,o
Kafka,770,org.apache.kafka.connect.runtime.WorkerConnector.doStart,119,error,java.lang.String(name){} Error while starting connector-org.apache.kafka.connect.runtime.WorkerConnector(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,768,org.apache.kafka.connect.runtime.WorkerConnector.initialize,92,error,java.lang.String(name){} Connector raised an error-org.apache.kafka.connect.runtime.WorkerConnector(name)WorkerConnector.this-java.lang.Exception(name)e,o,o,o
Kafka,769,org.apache.kafka.connect.runtime.WorkerConnector.initialize,98,error,java.lang.String(name){} Error initializing connector-org.apache.kafka.connect.runtime.WorkerConnector(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,771,org.apache.kafka.connect.runtime.WorkerConnector.pause,164,error,java.lang.String(name){} Error while shutting down connector-org.apache.kafka.connect.runtime.WorkerConnector(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,772,org.apache.kafka.connect.runtime.WorkerConnector.shutdown,177,error,java.lang.String(name){} Error while shutting down connector-org.apache.kafka.connect.runtime.WorkerConnector(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,659,org.apache.kafka.connect.runtime.WorkerInfo.logAll,71,info,java.lang.String(name)b.toString(),x,x,x
Kafka,669,org.apache.kafka.connect.runtime.WorkerSinkTask.close,160,warn,java.lang.String(name)Could not stop task-java.lang.Throwable(name)t,o,o,o
Kafka,670,org.apache.kafka.connect.runtime.WorkerSinkTask.close,166,warn,java.lang.String(name)Could not close consumer-java.lang.Throwable(name)t,o,o,o
Kafka,695,org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages,473,trace,"java.lang.String(name){} Converters and transformations returned null, possibly because of too many retries, so dropping record in topic '{}' partition {} at offset {}-org.apache.kafka.connect.runtime.WorkerSinkTask(name)this-java.lang.String(name)msg.topic()-int(name)msg.partition()-long(name)msg.offset()",o,o,o
Kafka,681,org.apache.kafka.connect.runtime.WorkerSinkTask.poll,315,trace,java.lang.String(name){} Polling consumer with timeout {} ms-org.apache.kafka.connect.runtime.WorkerSinkTask(name)this-long(name)timeoutMs,o,o,o
Kafka,701,org.apache.kafka.connect.runtime.WorkerSinkTask.rewind,578,warn,java.lang.String(name){} Cannot rewind {} to null offset-org.apache.kafka.connect.runtime.WorkerSinkTask(name)this-org.apache.kafka.common.TopicPartition(name)tp,o,o,o
Kafka,645,org.apache.kafka.connect.runtime.WorkerSinkTaskContext.pause,115,debug,"java.lang.String(name){} Connector is paused, so not pausing consumer's partitions {}-org.apache.kafka.connect.runtime.WorkerSinkTaskContext(name)this-TopicPartition[](name)partitions",o,o,o
Kafka,647,org.apache.kafka.connect.runtime.WorkerSinkTaskContext.resume,133,debug,"java.lang.String(name){} Connector is paused, so not resuming consumer's partitions {}-org.apache.kafka.connect.runtime.WorkerSinkTaskContext(name)this-TopicPartition[](name)partitions",o,o,o
Kafka,644,org.apache.kafka.connect.runtime.WorkerSinkTaskContext.timeout,87,debug,java.lang.String(name){} Setting timeout to {} ms-org.apache.kafka.connect.runtime.WorkerSinkTaskContext(name)this-long(name)timeoutMs,o,o,o
Kafka,761,org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets,460,trace,java.lang.String(name){} Finished flushing offsets to storage-org.apache.kafka.connect.runtime.WorkerSourceTask(name)WorkerSourceTask.this,o,o,o
Kafka,762,org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets,478,warn,"java.lang.String(name){} Flush of offsets interrupted, cancelling-org.apache.kafka.connect.runtime.WorkerSourceTask(name)this",o,o,o
Kafka,766,org.apache.kafka.connect.runtime.WorkerSourceTask.commitSourceTask,509,error,java.lang.String(name){} Exception thrown while calling task.commit()-org.apache.kafka.connect.runtime.WorkerSourceTask(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,753,org.apache.kafka.connect.runtime.WorkerSourceTask.commitTaskRecord,377,error,java.lang.String(name){} Exception thrown while calling task.commitRecord()-org.apache.kafka.connect.runtime.WorkerSourceTask(name)this-java.lang.Throwable(name)t,o,o,o
Kafka,751,org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords,333,trace,java.lang.String(name){} Wrote record successfully: topic {} partition {} offset {}-org.apache.kafka.connect.runtime.WorkerSourceTask(name)WorkerSourceTask.this-java.lang.String(name)recordMetadata.topic()-int(name)recordMetadata.partition()-long(name)recordMetadata.offset(),o,o,o
Kafka,981,org.apache.kafka.connect.storage.KafkaConfigBackingStore.onCompletion,618,debug,java.lang.String(name)We have an incomplete set of task configs for connector '{}' probably due to compaction. So we are not doing anything with the new configuration.-java.lang.String(name)connectorName,o,o,o
Kafka,966,org.apache.kafka.connect.storage.KafkaConfigBackingStore.onCompletion,454,error,java.lang.String(name)Unexpected in consumer callback for KafkaConfigBackingStore: -java.lang.Throwable(name)error,o,o,o
Kafka,972,org.apache.kafka.connect.storage.KafkaConfigBackingStore.onCompletion,517,info,java.lang.String(name)Successfully processed removal of connector '{}'-java.lang.String(name)connectorName,o,o,o
Kafka,956,org.apache.kafka.connect.storage.KafkaConfigBackingStore.putConnectorConfig,299,debug,java.lang.String(name)Writing connector configuration for connector '{}'-java.lang.String(name)connector,o,o,o
Kafka,962,org.apache.kafka.connect.storage.KafkaConfigBackingStore.putTaskConfigs,383,debug,java.lang.String(name)Writing commit for connector '{}' with {} tasks.-java.lang.String(name)connector-int(name)taskCount,o,o,o
Kafka,957,org.apache.kafka.connect.storage.KafkaConfigBackingStore.removeConnectorConfig,312,debug,java.lang.String(name)Removing connector configuration for connector '{}'-java.lang.String(name)connector,o,o,o
Kafka,951,org.apache.kafka.connect.storage.KafkaStatusBackingStore.read,445,warn,java.lang.String(name)Discarding record with invalid key {}-java.lang.String(name)key,o,o,o
Kafka,946,org.apache.kafka.connect.storage.KafkaStatusBackingStore.readConnectorStatus,405,trace,java.lang.String(name)Received connector {} status update {}-java.lang.String(name)connector-org.apache.kafka.connect.runtime.ConnectorStatus(name)status,o,o,o
Kafka,945,org.apache.kafka.connect.storage.KafkaStatusBackingStore.readConnectorStatus,395,trace,java.lang.String(name)Removing status for connector {}-java.lang.String(name)connector,o,o,o
Kafka,950,org.apache.kafka.connect.storage.KafkaStatusBackingStore.readTaskStatus,431,trace,java.lang.String(name)Received task {} status update {}-org.apache.kafka.connect.util.ConnectorTaskId(name)id-org.apache.kafka.connect.runtime.TaskStatus(name)status,o,o,o
Kafka,948,org.apache.kafka.connect.storage.KafkaStatusBackingStore.readTaskStatus,419,trace,java.lang.String(name)Removing task status for {}-org.apache.kafka.connect.util.ConnectorTaskId(name)id,o,o,o
Kafka,949,org.apache.kafka.connect.storage.KafkaStatusBackingStore.readTaskStatus,426,warn,java.lang.String(name)Failed to parse task status with key {}-java.lang.String(name)key,o,o,o
Kafka,986,org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets,102,error,"java.lang.String(name)CRITICAL: Failed to deserialize offset data when getting offsets for task with namespace {}. No value for this data will be returned, which may break the task or cause it to skip some data. This could either be due to an error in the connector implementation or incompatible schema.-java.lang.String(name)namespace-java.lang.Throwable(name)t",o,o,o
Kafka,985,org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets,91,error,"java.lang.String(name)Should be able to map {} back to a requested partition-offset key, backing store may have returned invalid data-java.nio.ByteBuffer(name)rawEntry.getKey()",o,o,o
Kafka,1058,org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus,331,error,java.lang.String(name)Could not read connector state-java.io.IOException(name)e,o,o,o
Kafka,1066,org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.stop,149,error,java.lang.String(name)msg-java.lang.Throwable(name)t,x,x,x
Kafka,1013,org.apache.kafka.connect.util.KafkaBasedLog.poll,268,error,"java.lang.String(name)""Error polling: "" + e",o,o,o
Kafka,1014,org.apache.kafka.connect.util.KafkaBasedLog.readToLogEnd,273,trace,java.lang.String(name)Reading to end of offset log,o,o,o
Kafka,1029,org.apache.kafka.connect.util.ShutdownableThread.run,83,error,java.lang.String(name)Thread {} exiting with uncaught exception: -java.lang.String(name)getName()-java.lang.Throwable(name)e,o,o,o
Kafka,1030,org.apache.kafka.connect.util.ShutdownableThread.startGracefulShutdown,118,info,java.lang.String(name)Starting graceful shutdown of thread {}-java.lang.String(name)getName(),o,o,o
Kafka,1021,org.apache.kafka.connect.util.TopicAdmin.createTopics,239,debug,java.lang.String(name)Unable to create topic(s) '{}' since the brokers at {} do not support the CreateTopics API.-java.lang.String(name) Falling back to assume topic(s) exist or will be auto-created by the broker.-java.lang.String(name)topicNameList-java.lang.String(name)bootstrapServers,o,o,o
Kafka,1019,org.apache.kafka.connect.util.TopicAdmin.createTopics,230,info,java.lang.String(name)Created topic {} on brokers at {}-org.apache.kafka.clients.admin.NewTopic(name)topicsByName.get(topic)-java.lang.String(name)bootstrapServers,o,o,o
Kafka,620,org.apache.kafka.log4jappender.KafkaLog4jAppender.append,308,debug,"java.lang.String(name)""["" + new Date(event.getTimeStamp()) + ""]""+ message",x,x,x
Kafka,1380,org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster.start,88,debug,java.lang.String(name)ZooKeeper instance is running at {}-java.lang.String(name)zKConnectString(),o,o,o
Kafka,1071,org.apache.kafka.streams.KafkaStreams.waitOnState,227,debug,java.lang.String(name)Cannot transit to {} within {}ms-org.apache.kafka.streams.State(name)targetState-long(name)waitMs,x,o,o
Kafka,1111,org.apache.kafka.streams.processor.FailOnInvalidTimestamp.onInvalidTimestamp,72,error,java.lang.String(name)message,o,x,x
Kafka,1184,org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.closeAllRestoringTasks,78,error,java.lang.String(name)Failed to remove restoring task {} due to the following error:-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)e,o,o,o
Kafka,1191,org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.process,203,info,java.lang.String(name)Failed to process stream task {} since it got migrated to another thread already. Closing it as zombie before triggering a new rebalance.-org.apache.kafka.streams.processor.TaskId(name)task.id(),o,o,o
Kafka,1193,org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.punctuate,236,info,java.lang.String(name)Failed to punctuate stream task {} since it got migrated to another thread already. Closing it as zombie before triggering a new rebalance.-org.apache.kafka.streams.processor.TaskId(name)task.id(),o,o,o
Kafka,1258,org.apache.kafka.streams.processor.internals.AssignedTasks.close,343,error,java.lang.String(name)Failed while closing {} {} due to the following error:-java.lang.String(name)task.getClass().getSimpleName()-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)t,x,o,o
Kafka,1257,org.apache.kafka.streams.processor.internals.AssignedTasks.close,339,info,java.lang.String(name)Failed to close {} {} since it got migrated to another thread already. Closing it as zombie and move on.-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1255,org.apache.kafka.streams.processor.internals.AssignedTasks.closeNonAssignedSuspendedTasks,321,debug,java.lang.String(name)Closing suspended and not re-assigned {} {}-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)suspendedTask.id(),x,o,o
Kafka,1256,org.apache.kafka.streams.processor.internals.AssignedTasks.closeNonAssignedSuspendedTasks,325,error,java.lang.String(name)Failed to remove suspended {} {} due to the following error:-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)suspendedTask.id()-java.lang.Exception(name)e,x,o,o
Kafka,1260,org.apache.kafka.streams.processor.internals.AssignedTasks.closeUnclean,370,error,java.lang.String(name)Failed while closing {} {} due to the following error:-java.lang.String(name)task.getClass().getSimpleName()-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)fatalException,x,o,o
Kafka,1259,org.apache.kafka.streams.processor.internals.AssignedTasks.closeUnclean,366,info,java.lang.String(name)Try to close {} {} unclean.-java.lang.String(name)task.getClass().getSimpleName()-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1247,org.apache.kafka.streams.processor.internals.AssignedTasks.closeZombieTask,153,warn,java.lang.String(name)Failed to close zombie {} {} due to {}; ignore and proceed.-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.String(name)e.toString(),x,o,o
Kafka,1254,org.apache.kafka.streams.processor.internals.AssignedTasks.commit,299,error,java.lang.String(name)Failed to commit {} {} due to the following error:-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)t,x,o,o
Kafka,1253,org.apache.kafka.streams.processor.internals.AssignedTasks.commit,290,info,java.lang.String(name)Failed to commit {} {} since it got migrated to another thread already. Closing it as zombie before triggering a new rebalance.-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1239,org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks,73,debug,java.lang.String(name)Transitioning {} {} to restoring-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)entry.getKey(),x,o,o
Kafka,1240,org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks,81,trace,java.lang.String(name)Could not create {} {} due to {}; will retry-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)entry.getKey()-java.lang.String(name)e.toString(),x,o,o
Kafka,1249,org.apache.kafka.streams.processor.internals.AssignedTasks.maybeResumeSuspendedTask,178,info,java.lang.String(name)Failed to resume {} {} since it got migrated to another thread already. Closing it as zombie before triggering a new rebalance.-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1248,org.apache.kafka.streams.processor.internals.AssignedTasks.maybeResumeSuspendedTask,169,trace,java.lang.String(name)Found suspended {} {}-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)taskId,x,o,o
Kafka,1250,org.apache.kafka.streams.processor.internals.AssignedTasks.maybeResumeSuspendedTask,187,trace,java.lang.String(name)Resuming suspended {} {}-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1242,org.apache.kafka.streams.processor.internals.AssignedTasks.suspend,98,trace,java.lang.String(name)Close created {} {}-java.lang.String(name)taskTypeName-java.util.Set<TaskId>(name)created.keySet(),x,o,o
Kafka,1241,org.apache.kafka.streams.processor.internals.AssignedTasks.suspend,96,trace,java.lang.String(name)Suspending running {} {}-java.lang.String(name)taskTypeName-java.util.Set<TaskId>(name)runningTaskIds(),x,o,o
Kafka,1246,org.apache.kafka.streams.processor.internals.AssignedTasks.suspendTasks,142,error,"java.lang.String(name)After suspending failed, closing the same {} {} failed again due to the following error:-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)f",x,o,o
Kafka,1245,org.apache.kafka.streams.processor.internals.AssignedTasks.suspendTasks,137,error,java.lang.String(name)Suspending {} {} failed due to the following error:-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id()-java.lang.RuntimeException(name)e,x,o,o
Kafka,1244,org.apache.kafka.streams.processor.internals.AssignedTasks.suspendTasks,132,info,java.lang.String(name)Failed to suspend {} {} since it got migrated to another thread already. Closing it as zombie and move on.-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1252,org.apache.kafka.streams.processor.internals.AssignedTasks.transitionToRunning,200,debug,java.lang.String(name)Transitioning {} {} to running-java.lang.String(name)taskTypeName-org.apache.kafka.streams.processor.TaskId(name)task.id(),x,o,o
Kafka,1349,org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo.decode,260,error,java.lang.String(name)fatalException.getMessage()-org.apache.kafka.streams.errors.TaskAssignmentException(name)fatalException,x,x,x
Kafka,1350,org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor.assignStandby,61,warn,java.lang.String(name)Unable to assign {} of {} standby tasks for task [{}]. There is not enough available capacity. You should increase the number of threads and/or application instances to maintain the requested number of standby replicas.-int(name)numStandbyReplicas - i-int(name)numStandbyReplicas-org.apache.kafka.streams.processor.TaskId(name)taskId,o,o,o
Kafka,1178,org.apache.kafka.streams.processor.internals.GlobalStateManagerImpl.close,338,debug,java.lang.String(name)Closing global storage engine {}-java.lang.String(name)entry.getKey(),o,o,o
Kafka,1169,org.apache.kafka.streams.processor.internals.GlobalStateManagerImpl.initialize,115,error,java.lang.String(name)Failed to unlock the global state directory-java.io.IOException(name)e,o,o,o
Kafka,1131,org.apache.kafka.streams.processor.internals.GlobalStreamThread.close,261,error,java.lang.String(name)Failed to close global consumer due to the following error:-java.lang.RuntimeException(name)e,o,o,o
Kafka,1134,org.apache.kafka.streams.processor.internals.GlobalStreamThread.run,303,error,java.lang.String(name)Failed to close state maintainer due to the following error:-java.io.IOException(name)e,o,o,o
Kafka,1133,org.apache.kafka.streams.processor.internals.GlobalStreamThread.run,298,info,java.lang.String(name)Shutting down,o,x,o
Kafka,1135,org.apache.kafka.streams.processor.internals.GlobalStreamThread.run,310,info,java.lang.String(name)Shutdown complete,o,x,o
Kafka,1128,org.apache.kafka.streams.processor.internals.GlobalStreamThread.setState,157,error,java.lang.String(name)Unexpected state transition from {} to {}-org.apache.kafka.streams.processor.internals.State(name)oldState-org.apache.kafka.streams.processor.internals.State(name)newState,o,o,o
Kafka,1129,org.apache.kafka.streams.processor.internals.GlobalStreamThread.setState,160,info,java.lang.String(name)State transition from {} to {}-org.apache.kafka.streams.processor.internals.State(name)oldState-org.apache.kafka.streams.processor.internals.State(name)newState,o,o,o
Kafka,1343,org.apache.kafka.streams.processor.internals.InternalTopicManager.makeReady,168,error,java.lang.String(name)timeoutAndRetryError,x,x,x
Kafka,1340,org.apache.kafka.streams.processor.internals.InternalTopicManager.makeReady,142,info,java.lang.String(name)Could not create topic {}. Topic is probably marked for deletion (number of partitions is unknown). Will retry to create this topic in {} ms (to let broker finish async delete operation first). Error message was: {}-java.lang.String(name)topicName-long(name)retryBackOffMs-java.lang.String(name)cause.toString(),o,o,o
Kafka,1348,org.apache.kafka.streams.processor.internals.InternalTopicManager.validateTopics,233,error,java.lang.String(name)errorMsg,x,x,x
Kafka,1160,org.apache.kafka.streams.processor.internals.ProcessorStateManager.close,277,debug,java.lang.String(name)Closing its state manager and all the registered state stores,o,o,o
Kafka,1162,org.apache.kafka.streams.processor.internals.ProcessorStateManager.close,289,error,java.lang.String(name)Failed to close state store {}: -java.lang.String(name)store.name()-java.lang.Exception(name)e,o,o,o
Kafka,1159,org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush,253,error,java.lang.String(name)Failed to flush state store {}: -java.lang.String(name)store.name()-java.lang.Exception(name)e,o,o,o
Kafka,1336,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.close,272,debug,java.lang.String(name)Closing producer,o,o,o
Kafka,1335,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.flush,265,debug,java.lang.String(name)Flushing producer,o,o,o
Kafka,1328,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError,136,error,java.lang.String(name)errorLogMessage-java.lang.String(name)topic-java.lang.String(name)exception.getMessage()-java.lang.Exception(name)exception,x,x,x
Kafka,1334,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send,221,error,"java.lang.String(name)Timeout exception caught when sending record to topic {}. This might happen if the producer cannot send data to the Kafka cluster and thus, its internal buffer fills up. This can also happen if the broker is slow to respond, if the network connection to the broker was interrupted, or if similar circumstances arise. You can increase producer parameter `max.block.ms` to increase this timeout.-java.lang.String(name)topic-org.apache.kafka.common.errors.TimeoutException(name)e",o,o,o
Kafka,1330,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send,182,warn,java.lang.String(name)Error sending record to topic {} due to {}; No more records will be sent and no more offsets will be recorded for this task. Enable TRACE logging to view failed record key and value.-java.lang.String(name)topic-java.lang.String(name)exception.getMessage()-java.lang.Exception(name)exception,o,o,o
Kafka,1332,org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send,203,warn,java.lang.String(name)Error sending records topic=[{}] and partition=[{}]; The exception handler chose to CONTINUE processing in spite of this error. Enable TRACE logging to view failed messages key and value.-java.lang.String(name)topic-java.lang.Integer(name)partition-java.lang.Exception(name)exception,o,o,o
Kafka,1274,org.apache.kafka.streams.processor.internals.StandbyTask.close,142,debug,java.lang.String(name)Closing,o,x,o
Kafka,1272,org.apache.kafka.streams.processor.internals.StandbyTask.commit,103,trace,java.lang.String(name)Committing,o,x,o
Kafka,1271,org.apache.kafka.streams.processor.internals.StandbyTask.resume,90,debug,java.lang.String(name)Resuming,o,x,o
Kafka,1273,org.apache.kafka.streams.processor.internals.StandbyTask.suspend,119,debug,java.lang.String(name)Suspending,o,x,o
Kafka,1286,org.apache.kafka.streams.processor.internals.StateDirectory.cleanRemovedTasks,311,error,java.lang.String(name){} Failed to delete the state directory.-java.lang.String(name)logPrefix()-java.io.IOException(name)e,o,o,o
Kafka,1283,org.apache.kafka.streams.processor.internals.StateDirectory.cleanRemovedTasks,287,info,java.lang.String(name){} Deleting obsolete state directory {} for task {} as {}ms has elapsed (cleanup delay is {}ms).-java.lang.String(name)logPrefix()-java.lang.String(name)dirName-org.apache.kafka.streams.processor.TaskId(name)id-long(name)now - lastModifiedMs-long(name)cleanupDelayMs,x,o,o
Kafka,1120,org.apache.kafka.streams.processor.internals.StoreChangelogReader.startRestoration,221,info,java.lang.String(name)No checkpoint found for task {} state store {} changelog {} with EOS turned on. Reinitializing the task and restore its state from the beginning.-org.apache.kafka.streams.processor.TaskId(name)task.id-java.lang.String(name)restorer.storeName()-org.apache.kafka.common.TopicPartition(name)partition,o,o,x
Kafka,1210,org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign,593,debug,java.lang.String(name)No tasks found for topic group {}-int(name)topicGroupId,o,o,o
Kafka,1213,org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign,616,info,"java.lang.String(name)Assigned tasks to clients as {}.-java.util.Map<UUID,ClientState>(name)states",o,o,o
Kafka,1197,org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.configure,251,error,java.lang.String(name)fatalException.getMessage()-org.apache.kafka.common.KafkaException(name)fatalException,x,x,x
Kafka,1216,org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment,837,info,java.lang.String(name)Sent a version {} subscription and group leader's latest supported version is {}. Upgrading subscription metadata version to {} for next rebalance.-int(name)usedSubscriptionMetadataVersion-int(name)leaderSupportedVersion-int(name)leaderSupportedVersion,o,o,o
Kafka,1220,org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.validate,983,error,java.lang.String(name)str,x,x,x
Kafka,1234,org.apache.kafka.streams.processor.internals.StreamTask.close,704,debug,java.lang.String(name)Closing,o,x,o
Kafka,1227,org.apache.kafka.streams.processor.internals.StreamTask.commit,453,debug,java.lang.String(name)Committing,o,x,o
Kafka,1222,org.apache.kafka.streams.processor.internals.StreamTask.resume,280,debug,java.lang.String(name)Resuming,o,x,o
Kafka,1230,org.apache.kafka.streams.processor.internals.StreamTask.suspend,550,debug,java.lang.String(name)Suspending,o,x,o
Kafka,1313,org.apache.kafka.streams.processor.internals.StreamThread.addRecordsToTasks,994,info,"java.lang.String(name)Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. Notifying the thread to trigger a new rebalance immediately.-org.apache.kafka.streams.processor.TaskId(name)task.id()",o,o,o
Kafka,1311,org.apache.kafka.streams.processor.internals.StreamThread.addToResetList,971,info,java.lang.String(name)logMessage-java.lang.String(name)topic-java.lang.String(name)resetPolicy,x,x,x
Kafka,1323,org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown,1182,info,java.lang.String(name)Shutting down,o,x,o
Kafka,1327,org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown,1202,info,java.lang.String(name)Shutdown complete,o,x,o
Kafka,1315,org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit,1042,debug,java.lang.String(name)Committed all active tasks {} and standby tasks {} in {}ms-java.util.Set<TaskId>(name)taskManager.activeTaskIds()-java.util.Set<TaskId>(name)taskManager.standbyTaskIds()-long(name)intervalCommitLatency,x,o,o
Kafka,1314,org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit,1029,trace,java.lang.String(name)Committing all active tasks {} and standby tasks {} since {}ms has elapsed (commit interval is {}ms)-java.util.Set<TaskId>(name)taskManager.activeTaskIds()-java.util.Set<TaskId>(name)taskManager.standbyTaskIds()-long(name)now - lastCommitMs-long(name)commitTimeMs,x,o,o
Kafka,1317,org.apache.kafka.streams.processor.internals.StreamThread.maybeUpdateStandbyTasks,1091,debug,java.lang.String(name)Updated standby tasks {} in {}ms-java.util.Set<TaskId>(name)taskManager.standbyTaskIds()-long(name)time.milliseconds() - now,x,o,o
Kafka,1295,org.apache.kafka.streams.processor.internals.StreamThread.onPartitionsRevoked,315,error,"java.lang.String(name)Error caught during partition revocation, will abort the current process and re-throw at the end of rebalance: {}-java.lang.Throwable(name)t",o,o,o
Kafka,1304,org.apache.kafka.streams.processor.internals.StreamThread.run,743,info,java.lang.String(name)Starting,o,x,o
Kafka,1309,org.apache.kafka.streams.processor.internals.StreamThread.runLoop,787,warn,"java.lang.String(name)Detected task {} that got migrated to another thread. This implies that this thread missed a rebalance and dropped out of the consumer group. Will try to rejoin the consumer group. Below is the detailed description of the task: {}-org.apache.kafka.streams.processor.TaskId(name)ignoreAndRejoinGroup.migratedTask().id()-java.lang.String(name)ignoreAndRejoinGroup.migratedTask().toString("">"")",o,x,o
Kafka,1145,org.apache.kafka.streams.processor.internals.TaskManager.addStandbyTasks,175,trace,"java.lang.String(name)New standby tasks to be created: {}-java.util.Map<TaskId,Set<TopicPartition>>(name)newStandbyTasks",o,o,o
Kafka,1140,org.apache.kafka.streams.processor.internals.TaskManager.addStreamTasks,120,debug,"java.lang.String(name)Adding assigned tasks as active: {}-java.util.Map<TaskId,Set<TopicPartition>>(name)assignedActiveTasks",o,o,o
Kafka,1143,org.apache.kafka.streams.processor.internals.TaskManager.addStreamTasks,146,trace,"java.lang.String(name)New active tasks to be created: {}-java.util.Map<TaskId,Set<TopicPartition>>(name)newTasks",o,o,o
Kafka,1363,org.apache.kafka.streams.state.internals.AbstractSegments.cleanupEarlierThan,179,error,java.lang.String(name)Error destroying {}-S(name)segment-java.io.IOException(name)e,o,o,o
Kafka,1370,org.apache.kafka.streams.state.internals.InMemoryWindowStore.close,259,warn,java.lang.String(name)Closing {} open iterators for store {}-int(name)openIterators.size()-java.lang.String(name)name,o,o,o
Kafka,1084,org.apache.kafka.streams.StreamsConfig.checkIfUnexpectedUserSpecifiedConsumerConfig,922,warn,"java.lang.String(name)String.format(nonConfigurableConfigMessage,""consumer"",config,eosMessage,clientProvidedProps.get(config),CONSUMER_EOS_OVERRIDES.get(config))",o,x,x
Kafka,1085,org.apache.kafka.streams.StreamsConfig.checkIfUnexpectedUserSpecifiedConsumerConfig,928,warn,"java.lang.String(name)String.format(nonConfigurableConfigMessage,""producer"",config,eosMessage,clientProvidedProps.get(config),PRODUCER_EOS_OVERRIDES.get(config))",o,x,x
Kafka,1395,org.apache.kafka.tools.PushHttpMetricsReporter.run,208,error,"java.lang.String(name)Error reporting metrics, {}: {}-int(name)responseCode-java.lang.String(name)msg",o,o,o
Kafka,1398,org.apache.kafka.tools.PushHttpMetricsReporter.run,215,error,java.lang.String(name)Error reporting metrics-java.lang.Throwable(name)t,o,o,o
Kafka,1394,org.apache.kafka.tools.PushHttpMetricsReporter.run,183,trace,java.lang.String(name)Reporting {} metrics to {}-int(name)samples.size()-java.net.URL(name)url,o,o,o
Kafka,1400,org.apache.kafka.tools.VerifiableConsumer.run,246,error,"java.lang.String(name)Error during processing, terminating consumer process: -java.lang.Throwable(name)t",o,o,o
Kafka,1389,org.apache.kafka.tools.VerifiableLog4jAppender.append,259,info,java.lang.String(name)msg,x,x,x
Kafka,1401,org.apache.kafka.trogdor.agent.Agent.main,249,info,java.lang.String(name)Starting agent process.,o,o,o
Kafka,1411,org.apache.kafka.trogdor.agent.WorkerManager.call,466,info,java.lang.String(name){}: Worker {} {} during startup. Transitioning to DONE.-java.lang.String(name)nodeName-org.apache.kafka.trogdor.agent.Worker(name)worker-java.lang.String(name)verb,x,o,o
Kafka,1412,org.apache.kafka.trogdor.agent.WorkerManager.call,470,info,java.lang.String(name){}: Worker {} {} during startup. Transitioning to CANCELLING.-java.lang.String(name)nodeName-org.apache.kafka.trogdor.agent.Worker(name)worker-java.lang.String(name)verb,x,o,o
Kafka,1414,org.apache.kafka.trogdor.agent.WorkerManager.call,480,info,java.lang.String(name){}: Running worker {} {}. Transitioning to STOPPING.-java.lang.String(name)nodeName-org.apache.kafka.trogdor.agent.Worker(name)worker-java.lang.String(name)verb,x,o,o
Kafka,1413,org.apache.kafka.trogdor.agent.WorkerManager.call,476,info,java.lang.String(name){}: Cancelling worker {} {}. -java.lang.String(name)nodeName-org.apache.kafka.trogdor.agent.Worker(name)worker-java.lang.String(name)verb,x,o,o
Kafka,1415,org.apache.kafka.trogdor.agent.WorkerManager.call,485,info,java.lang.String(name){}: Stopping worker {} {}.-java.lang.String(name)nodeName-org.apache.kafka.trogdor.agent.Worker(name)worker-java.lang.String(name)verb,x,o,o
Kafka,1406,org.apache.kafka.trogdor.agent.WorkerManager.createWorker,362,info,java.lang.String(name){}: Error creating worker {} for task {} with spec {}-java.lang.String(name)nodeName-long(name)workerId-java.lang.String(name)taskId-org.apache.kafka.trogdor.task.TaskSpec(name)spec-java.util.concurrent.ExecutionException(name)e,o,o,o
Kafka,1558,org.apache.kafka.trogdor.common.CapturingCommandRunner.run,53,debug,"java.lang.String(name)RAN {}: {}-org.apache.kafka.trogdor.common.Node(name)curNode-java.lang.String(name)Utils.join(command,"" "")",o,x,x
Kafka,1560,org.apache.kafka.trogdor.common.MiniTrogdorCluster.close,277,info,java.lang.String(name)Closing MiniTrogdorCluster.,o,o,o
Kafka,1444,org.apache.kafka.trogdor.common.WorkerUtils.createTopics,215,info,java.lang.String(name)Topic {} already exists.-java.lang.String(name)topicName,o,o,o
Kafka,1440,org.apache.kafka.trogdor.common.WorkerUtils.createTopics,161,warn,java.lang.String(name)Topic(s) {} already exist.-java.util.Collection<String>(name)topicsExists,o,o,o
Kafka,1446,org.apache.kafka.trogdor.common.WorkerUtils.createTopics,229,warn,java.lang.String(name)str,x,x,x
Kafka,1486,org.apache.kafka.trogdor.coordinator.Coordinator.main,170,info,java.lang.String(name)Starting coordinator process.,o,o,o
Kafka,1468,org.apache.kafka.trogdor.coordinator.TaskManager.call,367,info,"java.lang.String(name)Created a new task {} with spec {}, scheduled to start {} ms from now.-java.lang.String(name)id-org.apache.kafka.trogdor.task.TaskSpec(name)spec-long(name)delayMs",o,o,o
Kafka,1471,org.apache.kafka.trogdor.coordinator.TaskManager.call,401,info,"java.lang.String(name)Running task {} on node(s): {}-java.lang.String(name)task.id-java.lang.String(name)Utils.join(nodeNames,"", "")",o,o,o
Kafka,1477,org.apache.kafka.trogdor.coordinator.TaskManager.call,471,info,"java.lang.String(name)Cancelling task {} with worker(s) {}-java.lang.String(name)id-java.lang.String(name)Utils.mkString(activeWorkerIds,"""","""","" = "","", "")",o,o,o
Kafka,1494,org.apache.kafka.trogdor.fault.KiboshFaultWorker.start,49,info,java.lang.String(name)Activating {} {}: {}.-java.lang.String(name)spec.getClass().getSimpleName()-java.lang.String(name)id-org.apache.kafka.trogdor.fault.KiboshFaultSpec(name)spec,x,o,o
Kafka,1495,org.apache.kafka.trogdor.fault.KiboshFaultWorker.stop,58,info,java.lang.String(name)Deactivating {} {}: {}.-java.lang.String(name)spec.getClass().getSimpleName()-java.lang.String(name)id-org.apache.kafka.trogdor.fault.KiboshFaultSpec(name)spec,x,o,o
Kafka,1496,org.apache.kafka.trogdor.fault.NetworkPartitionFaultWorker.start,52,info,java.lang.String(name)Activating NetworkPartitionFault {}.-java.lang.String(name)id,o,o,o
Kafka,1497,org.apache.kafka.trogdor.fault.NetworkPartitionFaultWorker.stop,61,info,java.lang.String(name)Deactivating NetworkPartitionFault {}.-java.lang.String(name)id,o,o,o
Kafka,1503,org.apache.kafka.trogdor.rest.JsonRestServer.httpRequest,267,info,java.lang.String(name){} {}: error: {}-java.lang.String(name)method-java.lang.String(name)url-java.lang.String(name)e.getMessage(),x,o,o
Kafka,1500,org.apache.kafka.trogdor.rest.JsonRestServer.start,90,info,java.lang.String(name)Starting REST server,o,o,o
Kafka,1501,org.apache.kafka.trogdor.rest.JsonRestServer.start,95,info,java.lang.String(name)Registered resource {}-java.lang.Object(name)resource,o,o,o
Kafka,1504,org.apache.kafka.trogdor.task.NoOpTaskWorker.start,40,info,java.lang.String(name){}: Activating NoOpTask.-java.lang.String(name)id,o,o,o
Kafka,1505,org.apache.kafka.trogdor.task.NoOpTaskWorker.stop,47,info,java.lang.String(name){}: Deactivating NoOpTask.-java.lang.String(name)id,o,o,o
Kafka,1519,org.apache.kafka.trogdor.workload.ConnectionStressWorker.stop,305,info,java.lang.String(name){}: Deactivating ConnectionStressWorker.-java.lang.String(name)id,o,o,o
Kafka,1533,org.apache.kafka.trogdor.workload.ConsumeBenchWorker.stop,448,info,java.lang.String(name){}: Deactivating ConsumeBenchWorker.-java.lang.String(name)id,o,o,o
Kafka,1532,org.apache.kafka.trogdor.workload.ConsumeBenchWorker.update,363,info,java.lang.String(name)Status={}-java.lang.String(name)JsonUtil.toJsonString(statusData),o,x,o
Kafka,1539,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,219,error,java.lang.String(name){}: error: {}-java.lang.String(name)id-java.lang.String(name)error,o,x,o
Kafka,1556,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,375,error,java.lang.String(name){}: Terminator error-java.lang.String(name)id-java.lang.Throwable(name)e,o,o,o
Kafka,1552,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,336,error,java.lang.String(name){}: ExitMonitor error-java.lang.String(name)id-java.lang.Throwable(name)e,o,o,o
Kafka,1543,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,252,error,java.lang.String(name){}: (stderr):{}-java.lang.String(name)id-java.lang.String(name)line,x,x,x
Kafka,1553,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,358,info,java.lang.String(name){}: destroying process-java.lang.String(name)id,o,o,o
Kafka,1546,org.apache.kafka.trogdor.workload.ExternalCommandWorker.run,276,trace,java.lang.String(name){}: StdinWriter terminating.-java.lang.String(name)id,o,o,o
Kafka,1557,org.apache.kafka.trogdor.workload.ExternalCommandWorker.stop,386,info,java.lang.String(name){}: Deactivating ExternalCommandWorker.-java.lang.String(name)id,o,o,o
Kafka,1526,org.apache.kafka.trogdor.workload.ProduceBenchWorker.stop,409,info,java.lang.String(name){}: Deactivating ProduceBenchWorker.-java.lang.String(name)id,o,o,o
Kafka,1523,org.apache.kafka.trogdor.workload.ProduceBenchWorker.takeTransactionAction,272,debug,java.lang.String(name)Beginning transaction.,o,o,o
Kafka,1524,org.apache.kafka.trogdor.workload.ProduceBenchWorker.takeTransactionAction,276,debug,java.lang.String(name)Committing transaction.,o,o,o
Kafka,1525,org.apache.kafka.trogdor.workload.ProduceBenchWorker.takeTransactionAction,281,debug,java.lang.String(name)Aborting transaction.,o,o,o
Kafka,1507,org.apache.kafka.trogdor.workload.RoundTripWorker.run,232,debug,java.lang.String(name){}: Starting RoundTripWorker#ProducerRunnable.-java.lang.String(name)id,o,o,o
Kafka,1510,org.apache.kafka.trogdor.workload.RoundTripWorker.run,348,debug,java.lang.String(name){}: Starting RoundTripWorker#ConsumerRunnable.-java.lang.String(name)id,o,o,o
Kafka,1506,org.apache.kafka.trogdor.workload.RoundTripWorker.start,116,info,java.lang.String(name){}: Activating RoundTripWorker.-java.lang.String(name)id,o,o,o
Kafka,1516,org.apache.kafka.trogdor.workload.RoundTripWorker.stop,445,info,java.lang.String(name){}: Deactivating RoundTripWorker.-java.lang.String(name)id,o,o,o
Kafka,1517,org.apache.kafka.trogdor.workload.RoundTripWorker.stop,456,info,java.lang.String(name){}: Deactivated RoundTripWorker.-java.lang.String(name)id,o,o,o
Karaf,663,org.apache.felix.webconsole.internal.servlet.JaasSecurityProvider.authenticate,236,warn,java.lang.String(name)Error during authentication-java.lang.Exception(name)e,o,o,o
Karaf,660,org.apache.felix.webconsole.internal.servlet.JaasSecurityProvider.doAuthenticate,142,debug,java.lang.String(name)Login failed-javax.security.auth.login.FailedLoginException(name)e,o,o,o
Karaf,661,org.apache.felix.webconsole.internal.servlet.JaasSecurityProvider.doAuthenticate,145,warn,java.lang.String(name)Account failure-javax.security.auth.login.AccountException(name)e,o,x,o
Karaf,113,org.apache.karaf.audit.Activator.createLayout,207,error,"java.lang.String(name)""Error creating layout: "" + type + "". Using a simple layout.""-java.lang.Exception(name)e",o,o,o
Karaf,120,org.apache.karaf.bundle.command.ShowBundleTree.createNode,236,debug,"java.lang.String(name)format(""Skipping %s (already exists in the current branch)"",exporter)",o,o,o
Karaf,119,org.apache.karaf.bundle.command.ShowBundleTree.doExecute,71,debug,"java.lang.String(name)format(""Dependency tree calculated in %d ms"",System.currentTimeMillis() - start)",o,o,o
Karaf,124,org.apache.karaf.bundle.core.internal.BundleServiceImpl.disableDynamicImports,264,debug,java.lang.String(name)No additional packages have been wired since dynamic import was enabled,o,o,o
Karaf,125,org.apache.karaf.bundle.core.internal.BundleServiceImpl.disableDynamicImports,266,debug,java.lang.String(name)Additional packages wired since dynamic import was enabled,o,o,o
Karaf,126,org.apache.karaf.bundle.core.internal.BundleServiceImpl.disableDynamicImports,268,debug,"java.lang.String(name)""- "" + pkg",x,x,x
Karaf,138,org.apache.karaf.bundle.core.internal.BundlesMBeanImpl.getBundles,88,error,java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,x,x,x
Karaf,137,org.apache.karaf.bundle.core.internal.BundleWatcherImpl.getBundleExternalLocation,214,error,"java.lang.String(name)""Could not parse artifact path for bundle"" + bundle.getSymbolicName()-java.net.MalformedURLException(name)e",o,o,o
Karaf,132,org.apache.karaf.bundle.core.internal.BundleWatcherImpl.run,119,info,"java.lang.String(name)[Watch] Bundle {} is a fragment, so it's not started-java.lang.String(name)bundle.getSymbolicName()",o,o,o
Karaf,136,org.apache.karaf.bundle.core.internal.BundleWatcherImpl.updateBundleIfNecessary,157,info,"java.lang.String(name)[Watch] Bundle {} is a fragment, so it's not stopped-java.lang.String(name)bundle.getSymbolicName()",o,o,o
Karaf,118,org.apache.karaf.bundle.state.blueprint.internal.BlueprintStateService.blueprintEvent,103,debug,"java.lang.String(name)""Blueprint app state changed to "" + state + "" for bundle ""+ blueprintEvent.getBundle().getBundleId()",o,o,o
Karaf,139,org.apache.karaf.bundle.state.spring.internal.SpringStateService.onOsgiApplicationEvent,116,debug,"java.lang.String(name)""Spring app state changed to "" + state + "" for bundle ""+ event.getBundle().getBundleId()",o,o,o
Karaf,147,org.apache.karaf.deployer.blueprint.BlueprintDeploymentListener.transform,61,error,java.lang.String(name)Unable to build blueprint application bundle-java.lang.Exception(name)e,o,o,o
Karaf,152,org.apache.karaf.deployer.features.FeatureDeploymentListener.bundleChanged,254,error,"java.lang.String(name)""Unable to update deployed features for bundle: "" + bundle.getSymbolicName() + "" - ""+ bundle.getVersion()-java.lang.Exception(name)e",o,o,o
Karaf,156,org.apache.karaf.deployer.features.osgi.Activator.deploymentEvent,90,error,java.lang.String(name)ex.getMessage()-java.lang.Exception(name)ex,x,x,x
Karaf,162,org.apache.karaf.deployer.kar.KarArtifactInstaller.canHandle,81,debug,java.lang.String(name)Found a .zip file to deploy; checking contents to see if it's a Karaf archive.,o,o,o
Karaf,163,org.apache.karaf.deployer.kar.KarArtifactInstaller.canHandle,86,info,java.lang.String(name)Found a Karaf archive with .zip prefix; will deploy.,o,o,o
Karaf,161,org.apache.karaf.deployer.kar.KarArtifactInstaller.canHandle,75,info,java.lang.String(name)Found a .kar file to deploy.,o,o,o
Karaf,164,org.apache.karaf.deployer.kar.KarArtifactInstaller.canHandle,90,warn,java.lang.String(name)Problem extracting zip file '{}'; ignoring.-java.lang.String(name)file.getName()-java.lang.Exception(name)e,o,o,o
Karaf,165,org.apache.karaf.deployer.kar.KarArtifactInstaller.canHandle,97,warn,java.lang.String(name)Problem closing zip file '{}'; ignoring.-java.lang.String(name)file.getName()-java.io.IOException(name)e,o,o,o
Karaf,157,org.apache.karaf.deployer.kar.KarArtifactInstaller.install,44,info,java.lang.String(name)KAR {} is already installed. Please uninstall it first.-java.lang.String(name)file.getName(),o,o,o
Karaf,160,org.apache.karaf.deployer.kar.KarArtifactInstaller.update,60,warn,java.lang.String(name)Karaf archive {}' has been updated; redeploying.-java.io.File(name)file,o,o,o
Karaf,166,org.apache.karaf.deployer.spring.SpringDeploymentListener.canHandle,54,error,"java.lang.String(name)""Unable to parse deployed file "" + artifact.getAbsolutePath()-java.lang.Exception(name)e",o,o,o
Karaf,171,org.apache.karaf.docker.DockerClient.pull,250,debug,java.lang.String(name)line,x,x,x
Karaf,13,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.add,209,debug,java.lang.String(name)Booking created with id = {}-int(name)newId,o,o,o
Karaf,14,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.add,213,error,java.lang.String(name)Can't insert booking with customer {}-java.lang.String(name)booking.getCustomer()-java.sql.SQLException(name)exception,o,o,o
Karaf,5,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.createTables,117,info,java.lang.String(name)Schema and tables has been created,o,o,o
Karaf,11,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.get,175,error,java.lang.String(name)Can't find booking with id {}-java.lang.Long(name)id-java.sql.SQLException(name)exception,o,o,o
Karaf,2,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.open,84,debug,java.lang.String(name)Datasource {} -javax.sql.DataSource(name)this.dataSource,o,o,o
Karaf,16,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.remove,235,debug,java.lang.String(name)Service deleted with id = {}-java.lang.Long(name)id,o,o,o
Karaf,17,org.apache.karaf.examples.jdbc.provider.BookingServiceJdbcImpl.remove,238,error,java.lang.String(name)Can't delete service with id {}-java.lang.Long(name)id-java.sql.SQLException(name)exception,o,o,o
Karaf,176,org.apache.karaf.features.internal.download.impl.AbstractRetryableDownloadTask.run,73,debug,"java.lang.String(name)""Error downloading "" + url + "": ""+ e.getMessage()+ "". ""+ retry+ "" in approx ""+ delay+ "" ms.""",o,o,o
Karaf,177,org.apache.karaf.features.internal.model.JaxbUtil.unmarshalValidate,120,warn,java.lang.String(name)Old style feature file without namespace found (URI: {}). This format is deprecated and support for it will soon be removed-java.lang.String(name)uri,o,o,o
Karaf,184,org.apache.karaf.features.internal.model.processing.FeaturesProcessing.calculateOverridenURI,290,warn,"java.lang.String(name)""Problem parsing override URI \"""" + replacement + ""\"": ""+ e.getMessage()+ "". Ignoring.""",o,o,o
Karaf,182,org.apache.karaf.features.internal.model.processing.FeaturesProcessing.parseOverridesClauses,241,warn,"java.lang.String(name)""Can't parse override URL location pattern: "" + originalUri + "". Ignoring.""",o,o,o
Karaf,179,org.apache.karaf.features.internal.model.processing.FeaturesProcessing.postUnmarshall,195,warn,java.lang.String(name)Can't override bundle in maven mode without explicit original URL. Switching to osgi mode.,o,o,o
Karaf,178,org.apache.karaf.features.internal.model.processing.FeaturesProcessing.postUnmarshall,167,warn,"java.lang.String(name)""Can't parse blacklisted repository location pattern: "" + repositoryURI + "". Ignoring.""",o,o,o
Karaf,195,org.apache.karaf.features.internal.service.Blacklist.Blacklist,81,debug,java.lang.String(name)Unable to load blacklist bundles list {}-java.lang.String(name)e.toString(),o,o,o
Karaf,196,org.apache.karaf.features.internal.service.Blacklist.Blacklist,83,debug,java.lang.String(name)Unable to load blacklist bundles list-java.lang.Exception(name)e,o,o,o
Karaf,197,org.apache.karaf.features.internal.service.Blacklist.compileClauses,115,warn,java.lang.String(name)Repository blacklist URI is empty. Ignoring.,o,o,o
Karaf,200,org.apache.karaf.features.internal.service.Blacklist.compileClauses,138,warn,java.lang.String(name)Bundle blacklist URI is empty. Ignoring.,o,o,o
Karaf,218,org.apache.karaf.features.internal.service.BootFeaturesInstaller.separatorsToUnix,191,debug,java.lang.String(name)Converted path to unix separators: {}-java.lang.String(name)path,o,o,o
Karaf,220,org.apache.karaf.features.internal.service.Deployer.computeDeployment,1453,debug,java.lang.String(name)Error calculating checksum for bundle: {}-org.osgi.framework.Bundle(name)bundle-java.lang.Throwable(name)t,o,o,o
Karaf,223,org.apache.karaf.features.internal.service.Deployer.getBundlesToStop,1594,debug,java.lang.String(name)Selected bundles {} for destroy (no services in use)-java.util.List<Bundle>(name)bundlesToDestroy,o,o,o
Karaf,224,org.apache.karaf.features.internal.service.Deployer.getBundlesToStop,1604,debug,java.lang.String(name)Currently selecting bundle {} for destroy (with reference {})-org.osgi.framework.Bundle(name)bundle-org.osgi.framework.ServiceReference(name)reference,o,o,o
Karaf,225,org.apache.karaf.features.internal.service.Deployer.getBundlesToStop,1612,debug,java.lang.String(name)Selected bundle {} for destroy (lowest ranking service)-java.util.List<Bundle>(name)bundlesToDestroy,o,o,o
Karaf,205,org.apache.karaf.features.internal.service.EventAdminListener.featureEvent,71,warn,java.lang.String(name)Unable to post event to EventAdmin-java.lang.IllegalStateException(name)e,o,o,o
Karaf,211,org.apache.karaf.features.internal.service.FeatureConfigInstaller.installConfigurationFile,261,debug,"java.lang.String(name)Configuration file {} already exist, don't override it-java.lang.String(name)finalname",o,o,o
Karaf,212,org.apache.karaf.features.internal.service.FeatureConfigInstaller.installConfigurationFile,264,info,"java.lang.String(name)Configuration file {} already exist, overriding it-java.lang.String(name)finalname",o,o,o
Karaf,208,org.apache.karaf.features.internal.service.FeatureConfigInstaller.installFeatureConfigs,137,info,java.lang.String(name)Skipping configuration {} - file already exists-java.io.File(name)cfgFile,o,o,o
Karaf,194,org.apache.karaf.features.internal.service.FeaturesProcessingSerializer.resolve,312,warn,"java.lang.String(name)Value {} has unresolved properties, please check configuration.-java.lang.String(name)value",o,o,o
Karaf,226,org.apache.karaf.features.internal.service.FeaturesProcessorImpl.FeaturesProcessorImpl,85,debug,"java.lang.String(name)""Can't find feature processing file ("" + featureModificationsURI + ""), skipping""",o,o,o
Karaf,234,org.apache.karaf.features.internal.service.FeaturesServiceImpl.callListeners,323,warn,java.lang.String(name)DeploymentListener {} failed to process event {}-org.apache.karaf.features.DeploymentListener(name)listener-org.apache.karaf.features.DeploymentEvent(name)event-java.lang.Exception(name)e,o,o,o
Karaf,236,org.apache.karaf.features.internal.service.FeaturesServiceImpl.logInstalledOrUpdated,843,info,java.lang.String(name)The specified feature: '{}' version '{}' {}-java.lang.String(name)f.getName()-java.lang.String(name)f.getVersion()-java.lang.String(name)msg,o,o,o
Karaf,233,org.apache.karaf.features.internal.service.FeaturesServiceImpl.registerListener,283,error,java.lang.String(name)Error notifying listener about the current state-java.lang.Exception(name)e,o,o,o
Karaf,203,org.apache.karaf.features.internal.service.Overrides.loadOverrides,163,debug,java.lang.String(name)Unable to load overrides bundles list {}-java.lang.String(name)e.toString(),o,o,o
Karaf,204,org.apache.karaf.features.internal.service.Overrides.loadOverrides,165,debug,java.lang.String(name)Unable to load overrides bundles list-java.lang.Exception(name)e,o,o,o
Karaf,174,org.apache.karaf.features.LocationPattern.matches,174,warn,"java.lang.String(name)""Matched URI can't use version ranges: "" + otherUri",o,o,o
Karaf,242,org.apache.karaf.http.core.internal.ProxyServiceImpl.addProxyInternal,120,debug,java.lang.String(name)Adding {} proxy to {}-java.lang.String(name)proxy.getUrl()-java.lang.String(name)proxy.getProxyTo(),o,o,o
Karaf,243,org.apache.karaf.http.core.internal.ProxyServiceImpl.addProxyInternal,135,error,java.lang.String(name)Can't add {} proxy to {}-java.lang.String(name)proxy.getUrl()-java.lang.String(name)proxy.getProxyTo()-java.lang.Exception(name)e,o,o,o
Karaf,244,org.apache.karaf.http.core.internal.ProxyServiceImpl.updateConfiguration,157,error,java.lang.String(name)unable to update http proxy from configuration-java.lang.Exception(name)e,o,o,x
Karaf,251,org.apache.karaf.instance.core.internal.InstanceServiceImpl.cleanShutdown,867,debug,"java.lang.String(name)""Unable to cleanly shutdown instance "" + instance.name-java.lang.Exception(name)e",o,o,o
Karaf,249,org.apache.karaf.instance.core.internal.InstanceServiceImpl.doStart,560,debug,"java.lang.String(name)""Starting instance "" + name + "" with command: ""+ command",o,o,o
Karaf,247,org.apache.karaf.instance.core.internal.InstanceServiceImpl.logInfo,273,info,java.lang.String(name)formatted,x,x,x
Karaf,257,org.apache.karaf.jaas.config.impl.OsgiConfiguration.init,38,warn,java.lang.String(name)Unable to retrieve default configuration-java.lang.Throwable(name)ex,o,o,o
Karaf,259,org.apache.karaf.jaas.config.impl.ResourceKeystoreInstance.getCertificateAlias,154,error,java.lang.String(name)Unable to read retrieve alias for given certificate from keystore-java.security.KeyStoreException(name)e,o,o,o
Karaf,260,org.apache.karaf.jaas.config.impl.ResourceKeystoreInstance.getCertificateChain,166,error,java.lang.String(name)Unable to read certificate chain from keystore-java.security.KeyStoreException(name)e,o,o,o
Karaf,261,org.apache.karaf.jaas.config.impl.ResourceKeystoreInstance.getPrivateKey,197,error,java.lang.String(name)Unable to read private key from keystore-java.security.KeyStoreException(name)e,o,o,o
Karaf,264,org.apache.karaf.jaas.config.impl.ResourceKeystoreInstance.loadKeystoreData,268,error,java.lang.String(name)Unable to open keystore with provided password-java.security.KeyStoreException(name)e,o,o,o
Karaf,268,org.apache.karaf.jaas.modules.AbstractKarafLoginModule.abort,82,debug,java.lang.String(name)abort,o,x,x
Karaf,269,org.apache.karaf.jaas.modules.AbstractKarafLoginModule.logout,100,debug,java.lang.String(name)logout,o,x,x
Karaf,270,org.apache.karaf.jaas.modules.audit.EventAdminAuditLoginModule.audit,60,warn,"java.lang.String(name)""Unable to send security auditing EventAdmin events: "" + t",o,o,o
Karaf,271,org.apache.karaf.jaas.modules.audit.LogAuditLoginModule.audit,50,debug,java.lang.String(name){} - {} - {}-java.lang.String(name)actionStr-java.lang.String(name)username-java.lang.String(name)getPrincipalInfo(),x,x,x
Karaf,274,org.apache.karaf.jaas.modules.audit.LogAuditLoginModule.audit,56,error,java.lang.String(name){} - {} - {}-java.lang.String(name)actionStr-java.lang.String(name)username-java.lang.String(name)getPrincipalInfo(),x,x,x
Karaf,275,org.apache.karaf.jaas.modules.audit.LogAuditLoginModule.audit,58,info,java.lang.String(name){} - {} - {}-java.lang.String(name)actionStr-java.lang.String(name)username-java.lang.String(name)getPrincipalInfo(),x,x,x
Karaf,272,org.apache.karaf.jaas.modules.audit.LogAuditLoginModule.audit,52,trace,java.lang.String(name){} - {} - {}-java.lang.String(name)actionStr-java.lang.String(name)username-java.lang.String(name)getPrincipalInfo(),x,x,x
Karaf,273,org.apache.karaf.jaas.modules.audit.LogAuditLoginModule.audit,54,warn,java.lang.String(name){} - {} - {}-java.lang.String(name)actionStr-java.lang.String(name)username-java.lang.String(name)getPrincipalInfo(),x,x,x
Karaf,279,org.apache.karaf.jaas.modules.encryption.BasicEncryption.BasicEncryption,53,error,"java.lang.String(name)""Initialization failed. Digest algorithm "" + algorithm + "" is not available.""-java.security.NoSuchAlgorithmException(name)e",o,o,o
Karaf,280,org.apache.karaf.jaas.modules.encryption.BasicEncryption.BasicEncryption,59,error,"java.lang.String(name)""Initialization failed. Digest encoding "" + encoding + "" is not supported.""",o,o,o
Karaf,277,org.apache.karaf.jaas.modules.encryption.EncryptionSupport.logOptions,94,debug,"java.lang.String(name)""Encryption is enabled. Using service "" + name + "" with options ""+ encOpts",o,o,o
Karaf,278,org.apache.karaf.jaas.modules.encryption.EncryptionSupport.logOptions,96,debug,"java.lang.String(name)""Encryption is enabled. Using options "" + encOpts",o,o,o
Karaf,287,org.apache.karaf.jaas.modules.jdbc.JDBCLoginModule.login,127,debug,java.lang.String(name)No roleQuery specified so no roles have been retrieved for the authenticated user,o,o,o
Karaf,326,org.apache.karaf.jaas.modules.ldap.GSSAPILdapLoginModule.doLogin,89,error,java.lang.String(name)error with callback handler-java.io.IOException(name)ioException,o,o,x
Karaf,329,org.apache.karaf.jaas.modules.ldap.GSSAPILdapLoginModule.doLogin,108,warn,java.lang.String(name)Can't connect to the LDAP server: {}-java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,o,o,o
Karaf,325,org.apache.karaf.jaas.modules.ldap.GSSAPILdapLoginModule.login,68,error,java.lang.String(name)error with delegated authentication-java.security.PrivilegedActionException(name)pExcp,o,o,x
Karaf,299,org.apache.karaf.jaas.modules.ldap.LDAPBackingEngine.listUsers,116,debug,java.lang.String(name)Looking for the users in LDAP with,o,o,o
Karaf,301,org.apache.karaf.jaas.modules.ldap.LDAPBackingEngine.listUsers,118,debug,"java.lang.String(name)"" filter: "" + filter",o,x,x
Karaf,296,org.apache.karaf.jaas.modules.ldap.LDAPBackingEngine.lookupUser,82,debug,java.lang.String(name)Looking for user {} in LDAP with-java.lang.String(name)username,o,o,o
Karaf,298,org.apache.karaf.jaas.modules.ldap.LDAPBackingEngine.lookupUser,84,debug,java.lang.String(name) filter: {}-java.lang.String(name)filter,o,x,o
Karaf,310,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserDnAndNamespace,167,debug,java.lang.String(name)Looking for the user in LDAP with,o,o,o
Karaf,312,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserDnAndNamespace,169,debug,"java.lang.String(name)"" filter: "" + filter",o,x,x
Karaf,323,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserPubkeys,339,debug,java.lang.String(name)The user public key attribute is null so no keys were retrieved,o,o,o
Karaf,322,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserPubkeys,323,debug,java.lang.String(name)Looking for public keys of user {} in attribute {}-java.lang.String(name)userDn-java.lang.String(name)userPubkeyAttribute,o,o,o
Karaf,321,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserRoles,313,debug,java.lang.String(name)The user role filter is null so no roles are retrieved,o,o,o
Karaf,316,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserRoles,264,debug,java.lang.String(name)Looking for the user roles in LDAP with,o,o,o
Karaf,319,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserRoles,279,debug,java.lang.String(name)User {} is a member of role {}-java.lang.String(name)user-java.lang.String(name)role,o,o,o
Karaf,317,org.apache.karaf.jaas.modules.ldap.LDAPCache.doGetUserRoles,265,debug,java.lang.String(name) base DN: {}-java.lang.String(name)options.getRoleBaseDn(),o,o,x
Karaf,315,org.apache.karaf.jaas.modules.ldap.LDAPCache.tryMappingRole,239,debug,java.lang.String(name)LDAP role {} is mapped to Karaf role {}-java.lang.String(name)role-java.lang.String(name)karafRole,o,o,o
Karaf,288,org.apache.karaf.jaas.modules.ldap.LDAPLoginModule.doLogin,91,debug,java.lang.String(name)Changing from authentication = none to simple since user or password was specified.,o,o,o
Karaf,290,org.apache.karaf.jaas.modules.ldap.LDAPLoginModule.doLogin,121,warn,java.lang.String(name)Can't connect to the LDAP server: {}-java.lang.String(name)e.getMessage()-java.lang.Exception(name)e,o,o,o
Karaf,309,org.apache.karaf.jaas.modules.ldap.LDAPPubkeyLoginModule.doLogin,112,warn,java.lang.String(name)Public key authentication failed for user {}: {}-java.lang.String(name)user-java.lang.String(name)e.getMessage()-javax.security.auth.login.FailedLoginException(name)e,o,o,o
Karaf,308,org.apache.karaf.jaas.modules.ldap.LDAPPubkeyLoginModule.doLogin,106,warn,java.lang.String(name)Can't connect to the LDAP server: {}-java.lang.String(name)e.getMessage()-javax.naming.NamingException(name)e,o,o,o
Karaf,340,org.apache.karaf.jaas.modules.properties.AutoEncryptionSupport.encryptedPassword,140,debug,"java.lang.String(name)The password isn't flagged as encrypted, encrypt it.",o,o,o
Karaf,334,org.apache.karaf.jaas.modules.properties.PropertiesBackingEngine.addUserInternal,83,error,"java.lang.String(name)Cannot update users file,-java.lang.Exception(name)ex",o,o,o
Karaf,341,org.apache.karaf.jaas.modules.properties.PropertiesBackingEngineFactory.build,51,warn,java.lang.String(name)Cannot open users file: {}-java.lang.String(name)usersFile,o,o,o
Karaf,345,org.apache.karaf.jaas.modules.properties.PropertiesLoginModule.login,157,debug,java.lang.String(name)Successfully logged in {}-java.lang.String(name)user,o,o,o
Karaf,348,org.apache.karaf.jaas.modules.publickey.PublickeyLoginModule.login,176,debug,"java.lang.String(name)""Successfully logged in "" + user",o,o,o
Karaf,364,org.apache.karaf.jaas.modules.syncope.SyncopeBackingEngineFactory.build,40,error,java.lang.String(name)Error creating the Syncope backing engine-java.lang.Exception(name)e,o,o,o
Karaf,353,org.apache.karaf.jaas.modules.syncope.SyncopeLoginModule.login,88,debug,java.lang.String(name)Authenticate user {} on Syncope located {}-java.lang.String(name)user-java.lang.String(name)address,o,o,o
Karaf,354,org.apache.karaf.jaas.modules.syncope.SyncopeLoginModule.login,103,debug,java.lang.String(name)Syncope HTTP response status code: {}-int(name)response.getStatusLine().getStatusCode(),o,o,o
Karaf,356,org.apache.karaf.jaas.modules.syncope.SyncopeLoginModule.login,108,debug,java.lang.String(name)User {} authenticated-java.lang.String(name)user,o,o,o
Karaf,365,org.apache.karaf.jdbc.internal.JdbcServiceImpl.info,205,error,java.lang.String(name)Can't get information about datasource {}-java.lang.String(name)datasource-java.lang.Exception(name)e,o,o,o
Karaf,366,org.apache.karaf.jdbc.internal.JdbcServiceImpl.lookupDataSource,226,warn,"java.lang.String(name)""Multiple JDBC datasources found with the same service ranking for "" + name",o,o,o
Karaf,367,org.apache.karaf.kar.internal.FeatureDetector.isFeaturesRepository,69,debug,java.lang.String(name)File '{}' is not a features file.-java.lang.String(name)artifact.getName()-java.lang.Exception(name)e,o,o,o
Karaf,368,org.apache.karaf.kar.internal.Kar.extract,95,debug,java.lang.String(name)Uncompress the KAR file {} into directory {}-java.net.URI(name)karUri-java.io.File(name)repoDir,o,o,o
Karaf,372,org.apache.karaf.kar.internal.KarServiceImpl.install,92,debug,java.lang.String(name)Installing KAR {} from {}-java.lang.String(name)karName-java.net.URI(name)karUri,o,o,o
Karaf,374,org.apache.karaf.kar.internal.KarServiceImpl.install,132,info,java.lang.String(name)Dependencies of kar {} are now satisfied. Installing-java.lang.String(name)delayedKar.getKarName(),o,o,o
Karaf,373,org.apache.karaf.kar.internal.KarServiceImpl.install,120,warn,java.lang.String(name)Feature dependency {} is not available. Kar deployment postponed to see if it is about to be deployed-org.apache.karaf.features.Dependency(name)missingDependency,o,o,o
Karaf,387,org.apache.karaf.kar.internal.KarServiceImpl.installDelayedKars,530,error,"java.lang.String(name)""Delayed deployment of kar "" + kar.getKarName() + "" failed""-java.lang.Exception(name)e",o,o,o
Karaf,380,org.apache.karaf.kar.internal.KarServiceImpl.installFeatures,301,warn,"java.lang.String(name)Unable to install Kar feature {}-java.lang.String(name)feature.getName() + ""/"" + feature.getVersion()-java.lang.Exception(name)e",o,o,o
Karaf,382,org.apache.karaf.kar.internal.KarServiceImpl.installFeatures,308,warn,java.lang.String(name)Can't get features for KAR {}-java.net.URI(name)karFeatureRepoUri-java.lang.Exception(name)e,o,o,o
Karaf,385,org.apache.karaf.kar.internal.KarServiceImpl.uninstallFeatures,465,warn,"java.lang.String(name)Unable to uninstall Kar feature {}-java.lang.String(name)feature.getName() + ""/"" + feature.getVersion()-java.lang.Exception(name)e",o,o,x
Karaf,386,org.apache.karaf.kar.internal.KarServiceImpl.uninstallFeatures,470,warn,java.lang.String(name)Can't get features for KAR {}-java.net.URI(name)karFeatureRepoUri-java.lang.Exception(name)e,o,o,o
Karaf,402,org.apache.karaf.main.lock.OracleJDBCLock.lockAcquiredOnNonEmptySelection,94,warning,java.lang.String(name)Failed to acquire database lock. Missing database lock record.,o,o,o
Karaf,390,org.apache.karaf.main.Main.doMonitor,401,info,"java.lang.String(name)Data directory does not exist anymore, halting",o,o,o
Karaf,389,org.apache.karaf.main.Main.launch,274,info,java.lang.String(name)All initial bundles installed and set to start,o,o,o
Karaf,407,org.apache.karaf.management.internal.Activator.doStart,195,error,"java.lang.String(name)""Can't re-init JMXConnectorServer with SSL enabled when register a keystore:"" + e.getMessage()",o,o,o
Karaf,408,org.apache.karaf.management.internal.Activator.doStart,209,error,"java.lang.String(name)""Can't re-init JMXConnectorServer with SSL enabled when unregister a keystore: "" + e.getMessage()",o,o,o
Karaf,411,org.apache.karaf.management.internal.JMXSecurityMBeanImpl.canInvoke,111,warn,"java.lang.String(name){} (objectName = ""{}"", method = ""{}"")-javax.management.openmbean.KeyAlreadyExistsException(name)e-Object[](name)new Object[]{objectName,method}",x,x,o
Karaf,405,org.apache.karaf.management.KarafMBeanServerGuard.printDetailedMessage,421,debug,"java.lang.String(name)""The current roles are \'"" + currentRoles + ""\', however the expected roles are \'""+ expectedRoles+ ""\'. To make the call pass RBAC check, please add current role into entry \'""+ operationName+ ""\' of file ""+ matchedPid+ "".cfg""",o,o,o
Karaf,423,org.apache.karaf.maven.core.MavenRepositoryURL.MavenRepositoryURL,220,warn,java.lang.String(name)warn,x,x,x
Karaf,426,org.apache.karaf.packages.core.internal.PackagesMBeanImpl.getExports,88,error,java.lang.String(name)e.getMessage()-javax.management.openmbean.OpenDataException(name)e,x,x,x
Karaf,427,org.apache.karaf.packages.core.internal.PackagesMBeanImpl.getImports,126,error,java.lang.String(name)e.getMessage()-java.lang.RuntimeException(name)e,x,x,x
Karaf,428,org.apache.karaf.packages.core.internal.PackagesMBeanImpl.getImports,129,error,java.lang.String(name)e.getMessage()-javax.management.openmbean.OpenDataException(name)e,x,x,x
Karaf,430,org.apache.karaf.profile.assembly.ArtifactInstaller.installArtifact,64,info,"java.lang.String(name)"" adding overriden maven artifact: "" + bundle.getLocation() + "" (original location: ""+ bundle.getOriginalLocation()+ "")""",o,o,x
Karaf,491,org.apache.karaf.profile.assembly.AssemblyDeployCallback.bundleBlacklisted,309,info,java.lang.String(name) skipping blacklisted bundle: {}-java.lang.String(name)bundleInfo.getLocation(),o,o,x
Karaf,490,org.apache.karaf.profile.assembly.AssemblyDeployCallback.installBundle,263,info,"java.lang.String(name)"" adding maven artifact: "" + uri",o,o,x
Karaf,473,org.apache.karaf.profile.assembly.Builder.bootStage,1768,info,"java.lang.String(name)"" Feature "" + feature.getId() + "" is blacklisted, ignoring""",o,o,o
Karaf,475,org.apache.karaf.profile.assembly.Builder.bootStage,1781,info,"java.lang.String(name)"" Conditionial "" + cond.getConditionId() + "" is blacklisted, ignoring""",o,o,o
Karaf,470,org.apache.karaf.profile.assembly.Builder.bootStage,1707,info,java.lang.String(name) Loading boot repositories,o,o,o
Karaf,447,org.apache.karaf.profile.assembly.Builder.doGenerateAssembly,1034,info,java.lang.String(name)Adding profiles to {}-java.nio.file.Path(name)homeDirectory.relativize(profiles),o,o,o
Karaf,437,org.apache.karaf.profile.assembly.Builder.doGenerateAssembly,908,info,"java.lang.String(name)"" processing KAR: "" + karUri",o,o,x
Karaf,438,org.apache.karaf.profile.assembly.Builder.doGenerateAssembly,913,info,"java.lang.String(name)"" found repository: "" + repositoryUri",o,x,x
Karaf,456,org.apache.karaf.profile.assembly.Builder.doGenerateAssembly,1139,warn,java.lang.String(name)Can't generate consistency report into {} - it's not a directory-java.lang.String(name)generateConsistencyReport,o,o,o
Karaf,485,org.apache.karaf.profile.assembly.Builder.generateProfile,2095,info,java.lang.String(name)Generating {} profile-java.lang.String(name)name,o,o,o
Karaf,467,org.apache.karaf.profile.assembly.Builder.installStage,1670,info,java.lang.String(name) Feature {} is defined as an installed feature-java.lang.String(name)feature.getId(),o,o,o
Karaf,468,org.apache.karaf.profile.assembly.Builder.installStage,1683,info,"java.lang.String(name)"" Conditionial "" + cond.getConditionId() + "" is blacklisted, ignoring""",o,o,o
Karaf,465,org.apache.karaf.profile.assembly.Builder.installStage,1652,info,java.lang.String(name) Loading installed repositories,o,o,o
Karaf,484,org.apache.karaf.profile.assembly.Builder.loadRepositories,2066,info,"java.lang.String(name)"" referenced feature repository "" + innerRepository + "" is blacklisted""",o,o,x
Karaf,482,org.apache.karaf.profile.assembly.Builder.loadRepositories,2044,info,"java.lang.String(name)"" feature repository "" + url + "" is blacklisted""",o,o,x
Karaf,483,org.apache.karaf.profile.assembly.Builder.loadRepositories,2053,info,"java.lang.String(name)"" adding feature repository: "" + url",o,o,x
Karaf,459,org.apache.karaf.profile.assembly.Builder.processBlacklist,1412,warn,"java.lang.String(name)Found {} which is deprecated, please use new feature processor configuration.-java.nio.file.Path(name)homeDirectory.relativize(existingBLacklistedLocation)",o,o,o
Karaf,478,org.apache.karaf.profile.assembly.Builder.startupStage,1950,info,java.lang.String(name) Loading startup repositories,o,o,o
Karaf,495,org.apache.karaf.profile.assembly.Slf4jResolverLog.doLog,36,error,java.lang.String(name)msg-java.lang.Throwable(name)throwable,x,x,x
Karaf,499,org.apache.karaf.profile.command.ProfileEdit.importPidFromLocalConfigAdmin,469,warn,java.lang.String(name)Error while importing configuration {} to profile.-java.lang.String(name)pid,o,o,o
Karaf,502,org.apache.karaf.scheduler.command.support.ScriptJob.execute,49,warn,java.lang.String(name)Error executing script-java.lang.Exception(name)e,o,o,o
Karaf,500,org.apache.karaf.scheduler.command.support.TriggerJob.run,40,warn,"java.lang.String(name)""Could not find a scheduled job with name "" + name",o,o,o
Karaf,504,org.apache.karaf.scheduler.core.QuartzJobExecutor.execute,52,debug,java.lang.String(name)Executing job {} with name {}-java.lang.Object(name)job-java.lang.Object(name)data.get(QuartzScheduler.DATA_MAP_NAME),o,o,o
Karaf,505,org.apache.karaf.scheduler.core.QuartzJobExecutor.execute,62,error,java.lang.String(name)Scheduled job {} is neither a job nor a runnable.-java.lang.Object(name)job,o,o,o
Karaf,506,org.apache.karaf.scheduler.core.QuartzJobExecutor.execute,66,error,"java.lang.String(name)""Exception during job execution of "" + job + "" : ""+ t.getMessage()-java.lang.Throwable(name)t",o,o,o
Karaf,515,org.apache.karaf.scheduler.core.QuartzScheduler.reschedule,272,debug,java.lang.String(name)Update job scheduling {} with name {} and trigger {}-java.lang.Object(name)job-java.lang.String(name)jobName-org.quartz.Trigger(name)trigger,o,o,o
Karaf,514,org.apache.karaf.scheduler.core.QuartzScheduler.schedule,240,debug,java.lang.String(name)Scheduling job {} with name {} and trigger {}-java.lang.Object(name)job-java.lang.String(name)name-org.quartz.Trigger(name)trigger,o,o,o
Karaf,513,org.apache.karaf.scheduler.core.QuartzScheduler.schedule,222,debug,java.lang.String(name)Unscheduling job with name {}-java.lang.String(name)opts.name,o,o,o
Karaf,508,org.apache.karaf.scheduler.core.WhiteboardHandler.register,143,debug,java.lang.String(name)Ignoring service {} : scheduler times is defined but is less than -1.-org.osgi.framework.ServiceReference(name)ref,o,o,o
Karaf,507,org.apache.karaf.scheduler.core.WhiteboardHandler.register,141,debug,java.lang.String(name)Ignoring service {} : scheduler period is less than 1.-org.osgi.framework.ServiceReference(name)ref,o,o,o
Karaf,509,org.apache.karaf.scheduler.core.WhiteboardHandler.register,162,debug,java.lang.String(name)Ignoring service {} : no scheduling property found.-org.osgi.framework.ServiceReference(name)ref,o,o,o
Karaf,510,org.apache.karaf.scheduler.core.WhiteboardHandler.register,166,warn,java.lang.String(name)Error scheduling job-java.lang.Exception(name)e,o,o,o
Karaf,528,org.apache.karaf.scr.examples.managed.service.impl.ManagedGreeterServiceImpl.run,153,info,java.lang.String(name)Message {}: {} {}-int(name)messageCount-java.lang.String(name)salutation-java.lang.String(name)name,x,x,o
Karaf,529,org.apache.karaf.scr.examples.managed.service.impl.ManagedGreeterServiceImpl.run,157,info,java.lang.String(name)Thread shutting down,o,o,o
Karaf,532,org.apache.karaf.scr.examples.service.impl.GreeterServiceImpl.printGreetings,43,info,java.lang.String(name){} {}-java.lang.String(name)salutation-java.lang.String(name)name,x,x,x
Karaf,537,org.apache.karaf.service.guard.impl.Activator.start,47,info,java.lang.String(name)Adding role-based security to services with filter: {}-java.lang.String(name)f,o,o,o
Karaf,539,org.apache.karaf.service.guard.impl.GuardProxyCatalog.GuardProxyCatalog,99,info,java.lang.String(name)No compulsory roles for a karaf command without the ACL as its system property is not set: {}-java.lang.String(name)karaf.secured.command.compulsory.roles,o,o,o
Karaf,540,org.apache.karaf.service.guard.impl.GuardProxyCatalog.GuardProxyCatalog,106,trace,java.lang.String(name)Creating Config Admin Tracker using filter {}-org.osgi.framework.Filter(name)caFilter,o,o,o
Karaf,541,org.apache.karaf.service.guard.impl.GuardProxyCatalog.GuardProxyCatalog,111,trace,java.lang.String(name)Creating Proxy Manager Tracker using filter {}-org.osgi.framework.Filter(name)pmFilter,o,o,o
Karaf,549,org.apache.karaf.service.guard.impl.GuardProxyCatalog.preInvoke,523,info,java.lang.String(name)Current user does not have required roles ({}) for service {} method {} and/or arguments-java.util.List<String>(name)allowedRoles-org.osgi.framework.ServiceReference<>(name)serviceReference-java.lang.reflect.Method(name)m,o,o,o
Karaf,547,org.apache.karaf.service.guard.impl.GuardProxyCatalog.preInvoke,508,info,"java.lang.String(name)Service {} has role mapping, but assigned no roles to method {}-org.osgi.framework.ServiceReference<>(name)serviceReference-java.lang.reflect.Method(name)m",o,o,o
Karaf,548,org.apache.karaf.service.guard.impl.GuardProxyCatalog.preInvoke,517,trace,java.lang.String(name)Allow user with role {} to invoke service {} method {}-java.lang.String(name)role-org.osgi.framework.ServiceReference<>(name)serviceReference-java.lang.reflect.Method(name)m,o,o,o
Karaf,545,org.apache.karaf.service.guard.impl.GuardProxyCatalog.proxyIfNotAlreadyProxied,262,debug,"java.lang.String(name)Created proxy of service {} under {} with properties {}-long(name)orgServiceID-java.lang.Object(name)actualProxyProps.get(Constants.OBJECTCLASS)-java.util.Dictionary<String,Object>(name)actualProxyProps",o,o,o
Karaf,554,org.apache.karaf.shell.commands.impl.ExecuteAction.execute,62,debug,java.lang.String(name)Waiting for process to exit...,o,o,o
Karaf,553,org.apache.karaf.shell.commands.impl.ExecuteAction.execute,56,debug,java.lang.String(name)Executing: {}-java.lang.String(name)cmd,o,o,o
Karaf,558,org.apache.karaf.shell.commands.impl.JavaAction.execute,67,info,java.lang.String(name)Invoking w/arguments: {}-java.util.List<String>(name)args,o,o,x
Karaf,556,org.apache.karaf.shell.commands.impl.JavaAction.execute,58,info,"java.lang.String(name)""Using type: "" + type",o,o,o
Karaf,557,org.apache.karaf.shell.commands.impl.JavaAction.execute,63,info,"java.lang.String(name)""Using method: "" + method",o,o,o
Karaf,559,org.apache.karaf.shell.commands.impl.JavaAction.execute,73,info,"java.lang.String(name)""Result: "" + result",o,x,o
Karaf,550,org.apache.karaf.shell.commands.impl.SourceAction.execute,69,info,"java.lang.String(name)""Printing URL: "" + url",o,o,o
Karaf,551,org.apache.karaf.shell.commands.impl.SourceAction.execute,75,info,"java.lang.String(name)""Printing file: "" + file",o,o,o
Karaf,562,org.apache.karaf.shell.compat.ArgumentCompleter.ArgumentCompleter,102,warn,"java.lang.String(name)""Duplicate @Argument annotations on class "" + type.getName() + "" for index: ""+ key+ "" see: ""+ field",o,o,o
Karaf,565,org.apache.karaf.shell.compat.ArgumentCompleter.getCompleterValues,219,warn,"java.lang.String(name)""Duplicate @CompleterMethod annotations on class "" + type.getName() + "" for index: ""+ key+ "" see: ""+ method",o,o,o
Karaf,568,org.apache.karaf.shell.compat.ArgumentCompleter.getCompleterValues,250,warn,"java.lang.String(name)""Failed to release action: "" + action + "". ""+ e-java.lang.Exception(name)e",o,o,o
Karaf,583,org.apache.karaf.shell.console.completer.CommandsCompleter.checkData,242,debug,"java.lang.String(name)""Unable to create completers for command '"" + command + ""'""-java.lang.Throwable(name)t",o,o,o
Karaf,584,org.apache.karaf.shell.console.completer.CommandsCompleter.checkData,250,debug,"java.lang.String(name)""Unable to create completers for command '"" + command + ""'""-java.lang.Throwable(name)t",o,o,o
Karaf,605,org.apache.karaf.shell.impl.action.osgi.CommandExtender.debug,73,debug,java.lang.String(name)buf.toString(),x,x,x
Karaf,607,org.apache.karaf.shell.impl.action.osgi.CommandExtender.error,92,error,java.lang.String(name)msg-java.lang.Throwable(name)t,x,x,x
Karaf,606,org.apache.karaf.shell.impl.action.osgi.CommandExtender.warn,87,warn,java.lang.String(name)buf.toString()-java.lang.Throwable(name)t,x,x,x
Karaf,610,org.apache.karaf.shell.impl.action.osgi.CommandExtension.destroy,127,warn,"java.lang.String(name)""The wait for bundle "" + bundle.getSymbolicName() + "" being started before destruction has been interrupted.""-java.lang.InterruptedException(name)e",o,o,o
Karaf,609,org.apache.karaf.shell.impl.action.osgi.CommandExtension.start,108,info,java.lang.String(name)Command registration delayed for bundle {}/{}. Missing service: {}-java.lang.String(name)bundle.getSymbolicName()-org.osgi.framework.Version(name)bundle.getVersion()-java.util.List<String>(name)state.getMissingServices(),o,o,o
Karaf,612,org.apache.karaf.shell.impl.console.JLineTerminal.handle,296,debug,java.lang.String(name)unsupported operation-java.lang.UnsupportedOperationException(name)uoe,o,o,x
Karaf,593,org.apache.karaf.shell.security.impl.SecuredCommandConfigTransformer.deleteServiceGuardConfig,149,info,java.lang.String(name)Config ACL deleted: {}. Deleting generated service ACL configs {}-java.lang.String(name)originatingPid-Configuration[](name)configs,o,o,o
Karaf,595,org.apache.karaf.shell.security.impl.SecuredCommandConfigTransformer.refreshTheAffectedShellCommandBundle,206,error,"java.lang.String(name)""can't find the command bundle for scope "" + scopeName",o,o,x
Karaf,599,org.apache.karaf.shell.security.impl.SecuredCommandConfigTransformer.refreshTheAffectedShellCommandBundle,228,error,java.lang.String(name)Problem refresh the affected shell command bundle-org.osgi.framework.InvalidSyntaxException(name)ex,o,o,o
Karaf,637,org.apache.karaf.shell.ssh.KarafAgentFactory.registerSession,117,warn,java.lang.String(name)Error starting ssh agent for local console-java.lang.Throwable(name)e,o,o,o
Karaf,638,org.apache.karaf.shell.ssh.KarafAgentFactory.unregisterSession,131,warn,java.lang.String(name)Error stopping ssh agent for local console-java.lang.Throwable(name)e,o,o,o
Karaf,641,org.apache.karaf.shell.ssh.KarafJaasAuthenticator.doLogin,97,debug,"java.lang.String(name)""User authentication failed with "" + e.getMessage()-java.lang.Exception(name)e",o,o,o
Karaf,645,org.apache.karaf.shell.ssh.keygenerator.OpenSSHKeyPairProvider.loadKeys,92,info,java.lang.String(name)Successfully loaded legacy simple key. Converted to PEM format,o,o,o
Karaf,643,org.apache.karaf.shell.ssh.keygenerator.OpenSSHKeyPairProvider.loadKeys,84,info,java.lang.String(name)Successfully loaded key pair,o,o,o
Karaf,644,org.apache.karaf.shell.ssh.keygenerator.OpenSSHKeyPairProvider.loadKeys,89,warn,java.lang.String(name)Failed to parse keypair in {}. Attempting to parse it as a legacy 'simple' key-java.nio.file.Path(name)privateKeyPath,o,o,o
Karaf,642,org.apache.karaf.shell.ssh.keygenerator.OpenSSHKeyPairProvider.loadKeys,81,warn,java.lang.String(name)Failed to parse keypair in {}. Attempting to parse it 'directly'-java.nio.file.Path(name)privateKeyPath,o,o,o
Karaf,629,org.apache.karaf.shell.ssh.KnownHostsManager.close,136,warn,"java.lang.String(name)""Error closing: "" + e.getMessage()-java.io.IOException(name)e",o,o,o
Karaf,630,org.apache.karaf.shell.ssh.ShellCommand.run,146,error,java.lang.String(name)Unable to start shell-java.lang.Exception(name)e,o,o,o
Karaf,635,org.apache.karaf.shell.ssh.SshAction.execute,111,debug,java.lang.String(name)Prompting user for login,o,o,o
Karaf,636,org.apache.karaf.shell.ssh.SshAction.execute,127,debug,java.lang.String(name)Created client: {}-org.apache.sshd.client.SshClient(name)client,o,o,o
Karaf,639,org.apache.karaf.shell.ssh.SshServerAction.execute,62,debug,java.lang.String(name)Created server: {}-org.apache.sshd.server.SshServer(name)server,o,o,o
Karaf,603,org.apache.karaf.shell.util.ShellUtil.logException,146,error,java.lang.String(name)Exception caught while executing command-java.lang.Throwable(name)t,o,o,o
Karaf,649,org.apache.karaf.system.internal.SystemServiceImpl.sleepWithMsg,98,info,java.lang.String(name)msg,x,x,x
Karaf,26,org.apache.karaf.tooling.AssemblyMojo.detectStartupKarsAndFeatures,708,info,"java.lang.String(name)"" Feature "" + framework + "" will be added as a startup feature""",o,o,o
Karaf,25,org.apache.karaf.tooling.AssemblyMojo.detectStartupKarsAndFeatures,702,info,"java.lang.String(name)"" Standard startup KAR implied from framework ("" + framework + ""): ""+ kar",o,o,o
Karaf,19,org.apache.karaf.tooling.AssemblyMojo.doExecute,507,info,java.lang.String(name)Using repositories:,o,o,o
Karaf,20,org.apache.karaf.tooling.AssemblyMojo.doExecute,509,info,"java.lang.String(name)"" "" + r",x,x,x
Karaf,97,org.apache.karaf.tooling.features.AbstractFeatureMojo.addFeatureRepo,106,debug,java.lang.Exception(name)e,x,x,x
Karaf,76,org.apache.karaf.tooling.features.AddToRepositoryMojo.copy,116,info,"java.lang.String(name)""Copying artifact: "" + artifact",o,o,o
Karaf,93,org.apache.karaf.tooling.features.GenerateDescriptorMojo.checkChanges,880,info,java.lang.String(name)saveTreeListing(),x,x,x
Karaf,95,org.apache.karaf.tooling.features.GenerateDescriptorMojo.filter,950,warn,"java.lang.String(name)""File encoding has not been set, using platform encoding "" + ReaderFactory.FILE_ENCODING + "", i.e. build is platform dependent!""",o,o,o
Karaf,87,org.apache.karaf.tooling.features.GenerateDescriptorMojo.getManifest,711,warn,"java.lang.String(name)""Manifest not present in the first entry of the zip - "" + file.getName()",o,o,o
Karaf,94,org.apache.karaf.tooling.features.GenerateDescriptorMojo.saveDependencyChanges,921,warn,java.lang.String(name)out.toString(),x,x,x
Karaf,83,org.apache.karaf.tooling.features.GenerateDescriptorMojo.writeFeatures,590,info,java.lang.String(name)...done!,x,x,x
Karaf,31,org.apache.karaf.tooling.KarMojo.createArchive,264,debug,"java.lang.String(name)Feature artifact is a SNAPSHOT, handling the maven-metadata-local.xml",o,o,o
Karaf,32,org.apache.karaf.tooling.KarMojo.createArchive,266,debug,"java.lang.String(name)""Looking for "" + metadataTarget.getAbsolutePath()",o,o,o
Karaf,35,org.apache.karaf.tooling.KarMojo.createArchive,295,warn,java.lang.String(name)It means that this SNAPSHOT could be overwritten by an older one present on remote repositories,o,o,o
Karaf,27,org.apache.karaf.tooling.KarMojo.execute,156,warn,"java.lang.String(name)Your project should use the ""kar"" packaging or configure a ""classifier"" for kar attachment",o,o,x
Karaf,107,org.apache.karaf.tooling.tracker.GenerateServiceMetadata.execute,102,info,"java.lang.String(name)""Ignoring "" + classUrl",o,x,o
Karaf,108,org.apache.karaf.tooling.tracker.GenerateServiceMetadata.execute,134,info,"java.lang.String(name)""Activator "" + activators.get(0).getName()",o,x,o
Karaf,109,org.apache.karaf.tooling.tracker.GenerateServiceMetadata.execute,143,info,"java.lang.String(name)""Service "" + clazz.getCanonicalName()",o,x,o
Karaf,50,org.apache.karaf.tooling.VerifyMojo.doExecute,364,warn,"java.lang.String(name)e.getMessage() + "": "" + id",x,x,x
Karaf,47,org.apache.karaf.tooling.VerifyMojo.execute,192,info,"java.lang.String(name)""Using repositories: "" + remoteRepositories",o,o,o
Karaf,654,org.apache.karaf.util.filesstream.FilesStream.files,154,warn,java.lang.String(name)Error generating filenames-java.io.IOException(name)e,o,o,o
Karaf,655,org.apache.karaf.util.tracker.BaseActivator.start,94,warn,java.lang.String(name)Error starting activator-java.lang.Throwable(name)e,o,o,o
Karaf,681,org.apache.karaf.webconsole.features.FeaturesPlugin.getRepositories,317,error,java.lang.String(name)e.getMessage(),x,x,x
Karaf,670,org.apache.karaf.webconsole.features.FeaturesPlugin.installFeature,181,error,java.lang.String(name)Features service is not available,o,o,o
Karaf,671,org.apache.karaf.webconsole.features.FeaturesPlugin.installFeature,187,error,java.lang.String(name)Can't install feature {}/{}-java.lang.String(name)feature-java.lang.String(name)version-java.lang.Exception(name)e,o,o,o
Karaf,677,org.apache.karaf.webconsole.features.FeaturesPlugin.refreshRepository,229,error,java.lang.String(name)Can't refresh features repository {}-java.lang.String(name)url-java.lang.Exception(name)e,o,o,o
Karaf,673,org.apache.karaf.webconsole.features.FeaturesPlugin.uninstallFeature,201,error,java.lang.String(name)Can't uninstall feature {}/{}-java.lang.String(name)feature-java.lang.String(name)version-java.lang.Exception(name)e,o,o,o
Karaf,687,org.apache.karaf.webconsole.gogo.GogoPlugin.getResource,128,error,java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,x,x
Karaf,689,org.apache.karaf.webconsole.http.HttpPlugin.start,71,info,java.lang.String(name)Http plugin activated,o,o,x
Karaf,690,org.apache.karaf.webconsole.http.HttpPlugin.stop,75,info,java.lang.String(name)Http plugin deactivated,o,o,x
Karaf,696,org.apache.karaf.webconsole.instance.InstancePlugin.getResource,185,error,"java.lang.String(name)""failed to open "" + url",o,o,x
Karaf,700,org.apache.karaf.wrapper.internal.WrapperServiceImpl.copyResourceTo,394,info,java.lang.String(name)Creating file: {}-java.lang.String(name)outFile.getPath(),o,o,o
Karaf,701,org.apache.karaf.wrapper.internal.WrapperServiceImpl.copyResourceTo,409,info,java.lang.String(name)writing: {}-java.lang.String(name)line,o,x,x
Karaf,702,org.apache.karaf.wrapper.internal.WrapperServiceImpl.copyResourceTo,431,warn,java.lang.String(name)File already exists. Move it out of the way if you wish to recreate it: {}-java.lang.String(name)outFile.getPath(),o,o,o
Wicket,59,org.apache.wicket.ajax.form.AjaxFormComponentUpdatingBehavior.checkComponent,107,warn,"java.lang.String(name)String.format(""AjaxFormComponentUpdatingBehavior is not supposed to be added in the form component at path: \""%s\"". "" + ""Use the AjaxFormChoiceComponentUpdatingBehavior instead, that is meant for choices/groups that are not one component in the html but many"",component.getPageRelativePath())",o,o,x
Wicket,64,org.apache.wicket.ajax.markup.html.form.AjaxButton.onAfterSubmit,190,warn,java.lang.String(name)unexpected invocation of #onAfterSubmit() on {}-org.apache.wicket.ajax.markup.html.form.AjaxButton(name)this,o,o,x
Wicket,65,org.apache.wicket.ajax.markup.html.form.AjaxButton.onError,201,warn,java.lang.String(name)unexpected invocation of #onError() on {}-org.apache.wicket.ajax.markup.html.form.AjaxButton(name)this,o,o,x
Wicket,63,org.apache.wicket.ajax.markup.html.form.AjaxButton.onSubmit,184,warn,java.lang.String(name)unexpected invocation of #onSubmit() on {}-org.apache.wicket.ajax.markup.html.form.AjaxButton(name)this,o,o,x
Wicket,62,org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink.onAfterSubmit,223,warn,java.lang.String(name)unexpected invocation of #onAfterSubmit() on {}-org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink(name)this,o,o,x
Wicket,60,org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink.onError,194,warn,java.lang.String(name)unexpected invocation of #onError() on {}-org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink(name)this,o,o,x
Wicket,61,org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink.onSubmit,214,warn,java.lang.String(name)unexpected invocation of #onSubmit() on {}-org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink(name)this,o,o,x
Wicket,66,org.apache.wicket.application.CompoundClassResolver.resolveClass,71,debug,java.lang.String(name)ClassResolver '{}' cannot find class: '{}'-java.lang.String(name)resolver.getClass().getName()-java.lang.String(name)className,o,o,o
Wicket,43,org.apache.wicket.Application.destroyInitializers,550,info,java.lang.String(name)[{}] destroy: {}-java.lang.String(name)getName()-org.apache.wicket.IInitializer(name)initializer,x,x,x
Wicket,44,org.apache.wicket.Application.initInitializers,563,info,java.lang.String(name)[{}] init: {}-java.lang.String(name)getName()-org.apache.wicket.IInitializer(name)initializer,x,x,x
Wicket,71,org.apache.wicket.application.ReloadingClassLoader.watchForModifications,360,debug,"java.lang.String(name)""Class file does not exist: "" + clzFile",o,o,o
Wicket,67,org.apache.wicket.application.ReloadingClassLoader.watchForModifications,329,debug,"java.lang.String(name)""clzLocation="" + clzLocation",x,x,x
Wicket,73,org.apache.wicket.authentication.strategy.DefaultAuthenticationStrategy.load,122,info,"java.lang.String(name)Error decrypting login cookie: {}. The cookie will be deleted. Possible cause is that a session-relative encryption key was used to encrypt this cookie while this decryption attempt is happening in a different session, eg user coming back to the application after session expiration-java.lang.String(name)cookieKey",o,o,o
Wicket,17,org.apache.wicket.cdi.AutoConversationManager.autoBeginIfNecessary,82,debug,java.lang.String(name)Auto-began conversation '{}' for page '{}'-java.lang.String(name)conversation.getId()-org.apache.wicket.Page(name)page,o,o,o
Wicket,18,org.apache.wicket.cdi.AutoConversationManager.autoEndIfNecessary,102,debug,java.lang.String(name)Auto-ended conversation '{}' for page '{}'-java.lang.String(name)cid-org.apache.wicket.Page(name)page,o,o,o
Wicket,12,org.apache.wicket.cdi.ConversationExpiryChecker.onBeforeRender,66,info,java.lang.String(name)Conversation {} has expired for {}-java.lang.String(name)cid-org.apache.wicket.Page(name)page,o,o,o
Wicket,20,org.apache.wicket.cdi.ConversationExpiryChecker.onBeforeRender,64,info,java.lang.String(name)Conversation {} has expired for {}-java.lang.String(name)cid-org.apache.wicket.Page(name)page,o,o,o
Wicket,2,org.apache.wicket.cdi.ConversationPropagator.activateConversationIfNeeded,201,info,java.lang.String(name)Unable to restore conversation with id {}-java.lang.String(name)cid-java.lang.String(name)e.getMessage(),o,o,o
Wicket,8,org.apache.wicket.cdi.ConversationPropagator.onDetach,342,debug,java.lang.String(name)Deactivating conversation {}-java.lang.String(name)conversation.getId(),o,o,o
Wicket,13,org.apache.wicket.cdi.ConversationPropagator.onRequestHandlerResolved,108,debug,java.lang.String(name)Activating conversation {}-java.lang.String(name)conversation.getId(),o,o,o
Wicket,0,org.apache.wicket.cdi.ConversationPropagator.onRequestHandlerResolved,144,info,java.lang.String(name)Conversation {} has expired for {}-java.lang.String(name)cid-org.apache.wicket.Page(name)page,o,o,o
Wicket,4,org.apache.wicket.cdi.ConversationPropagator.onRequestHandlerScheduled,276,debug,java.lang.String(name)Propagating non-transient conversation {} via page parameters of handler {}-java.lang.String(name)conversation.getId()-org.apache.wicket.request.IRequestHandler(name)handler,o,o,o
Wicket,7,org.apache.wicket.cdi.ConversationPropagator.onUrlMapped,330,debug,java.lang.String(name)Propagating non-transient conversation {} via url-java.lang.String(name)conversation.getId(),o,o,o
Wicket,15,org.apache.wicket.cdi.ConversationPropagator.onUrlMapped,178,debug,java.lang.String(name)Propagating non-transient conversation {} via url-java.lang.String(name)conversation.getId(),o,o,o
Wicket,31,org.apache.wicket.Component.getSizeInBytes,1807,error,"java.lang.String(name)""Exception getting size for component "" + this-java.lang.Exception(name)e",o,o,o
Wicket,37,org.apache.wicket.Component.rendered,2553,error,"java.lang.String(name)""Component is not connected to a Page. Cannot register the component as being rendered. Component: "" + toString()",o,o,o
Wicket,74,org.apache.wicket.core.request.handler.ListenerRequestHandler.respond,192,debug,java.lang.String(name)An IRequestListener was called but its page/component({}) couldn't be resolved. Scheduling re-create of the page and ignoring the listener interface...-java.lang.String(name)getComponentPath(),o,o,o
Wicket,78,org.apache.wicket.core.request.mapper.CryptoMapper.decryptEntireUrl,458,error,java.lang.String(name)Error decrypting URL-java.lang.Exception(name)e,o,o,o
Wicket,79,org.apache.wicket.core.request.mapper.CryptoMapper.decryptRequestListenerParameter,596,error,java.lang.String(name)Error decrypting encrypted request listener query parameter-java.lang.Exception(name)e,o,o,o
Wicket,81,org.apache.wicket.core.util.lang.PropertyResolver.findSetter,802,debug,"java.lang.String(name)""Can't find setter method corresponding to "" + getMethod",o,o,o
Wicket,84,org.apache.wicket.core.util.lang.PropertyResolver.findSetter,1069,debug,"java.lang.String(name)""Cannot find setter corresponding to "" + getMethod",o,o,o
Wicket,80,org.apache.wicket.core.util.lang.PropertyResolver.newValue,718,warn,"java.lang.String(name)""Cannot set new value "" + value + "" at index ""+ index+ "" for array holding elements of class ""+ clzComponentType-java.lang.Exception(name)e",o,o,o
Wicket,82,org.apache.wicket.core.util.lang.PropertyResolver.newValue,895,warn,java.lang.String(name)Null setMethod,x,x,x
Wicket,86,org.apache.wicket.core.util.lang.PropertyResolver.newValue,1086,warn,java.lang.String(name)Null setMethod,x,x,x
Wicket,93,org.apache.wicket.core.util.lang.WicketObjects.resolveClass,177,debug,"java.lang.String(name)Class not found by using objects own classloader, trying the IClassResolver",o,o,o
Wicket,95,org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck,437,warn,"java.lang.String(name)Error delegating to Externalizable : {}, path: {}-java.lang.String(name)e.getMessage()-java.lang.StringBuilder(name)currentPath()",o,o,o
Wicket,96,org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck,502,warn,"java.lang.String(name)error delegating to writeObject : {}, path: {}-java.lang.String(name)e.getMessage()-java.lang.StringBuilder(name)currentPath()",o,o,x
Wicket,100,org.apache.wicket.core.util.resource.locator.ResourceStreamLocator.locate,128,debug,java.lang.String(name)Attempting to locate resource '{}' using finder'{}'-java.lang.String(name)path-org.apache.wicket.util.file.IResourceFinder(name)finder,o,o,o
Wicket,99,org.apache.wicket.core.util.resource.WebExternalResourceStream.lastModifiedTime,98,warn,java.lang.String(name)failed to retrieve last modified timestamp-java.io.IOException(name)e,o,o,x
Wicket,101,org.apache.wicket.core.util.string.ComponentRenderer.renderComponent,406,warn,java.lang.String(name)Component '{}' with a parent '{}' is passed for standalone rendering. It is recommended to render only orphan components because they are not cleaned up/detached after the rendering.-org.apache.wicket.Component(name)component-org.apache.wicket.MarkupContainer(name)oldParent,o,o,o
Wicket,103,org.apache.wicket.core.util.watch.Nio2ModificationWatcher.register,263,debug,java.lang.String(name)Registering folder '{}' to the watching service with kinds: {}-java.nio.file.Path(name)folder-Kind[](name)watchedKinds,o,o,o
Wicket,47,org.apache.wicket.DefaultExceptionMapper.map,68,error,"java.lang.String(name)""An error occurred while handling a previous error: "" + e2.getMessage()-java.lang.RuntimeException(name)e2",o,o,o
Wicket,49,org.apache.wicket.DefaultExceptionMapper.mapExpectedExceptions,144,debug,"java.lang.String(name)Connection lost, give up responding.-java.lang.Exception(name)e",o,o,o
Wicket,51,org.apache.wicket.DefaultExceptionMapper.mapUnexpectedExceptions,170,error,java.lang.String(name)Unexpected error occurred-java.lang.Exception(name)e,o,o,o
Wicket,319,org.apache.wicket.examples.source.SourcesPage.get,214,debug,"java.lang.String(name)""Can't construct the uri as a file: "" + absolutePath",o,o,o
Wicket,318,org.apache.wicket.examples.source.SourcesPage.getObject,125,error,"java.lang.String(name)""Unable to read resource stream for: "" + source + ""; Page=""+ page.toString()-java.io.IOException(name)e",o,o,o
Wicket,322,org.apache.wicket.examples.source.SourcesPage.getPageTargetClass,466,error,java.lang.String(name)user is trying to access class: {} which is not in the scope of org.apache.wicket.examples-java.lang.String(name)pageParam,o,o,x
Wicket,321,org.apache.wicket.examples.source.SourcesPage.getPageTargetClass,459,error,java.lang.String(name)key: {} is null.-java.lang.String(name)PAGE_CLASS,o,o,o
Wicket,328,org.apache.wicket.http2.markup.head.Jetty9PushBuilder.push,61,warn,java.lang.String(name)Attempted to use HTTP2 Push but it is not supported for the current request: {}!-javax.servlet.http.HttpServletRequest(name)httpRequest,o,o,o
Wicket,327,org.apache.wicket.http2.markup.head.NoopPushBuilder.push,57,warn,java.lang.String(name)This PushBuilder does nothing. Please use one of the other implementations - Jetty9 or Tomcat8.5+,o,o,o
Wicket,334,org.apache.wicket.jmx.Initializer.init,124,error,"java.lang.String(name)""unable to find mbean server with agent id "" + agentId",o,o,x
Wicket,336,org.apache.wicket.jmx.Initializer.init,155,error,java.lang.String(name)unable to find mbean server of type '{}'-java.lang.String(name)impl,o,o,x
Wicket,337,org.apache.wicket.jmx.Initializer.init,165,info,java.lang.String(name)registering Wicket mbeans with server '{}'-javax.management.MBeanServer(name)mbeanServer,o,o,x
Wicket,333,org.apache.wicket.jmx.Initializer.init,113,warn,java.lang.String(name)not allowed to read property wicket.mbean.server.agentid due to security settings; ignoring,o,o,x
Wicket,335,org.apache.wicket.jmx.Initializer.init,137,warn,java.lang.String(name)not allowed to read property wicket.mbean.server.class due to security settings; ignoring,o,o,x
Wicket,27,org.apache.wicket.Localizer.getStringIgnoreSettings,333,warn,java.lang.String(name)Tried to retrieve a localized string for a component that has not yet been added to the page. This can sometimes lead to an invalid or no localized resource returned. Make sure you are not calling Component#getString() inside your Component's constructor. Offending component: {}-org.apache.wicket.Component(name)component,o,o,o
Wicket,125,org.apache.wicket.markup.head.filter.FilteringHeaderResponse.render,210,debug,"java.lang.String(name)A HeaderItem '{}' was rendered to the filtering header response, but did not match any filters, so it put in the <head>.-org.apache.wicket.markup.head.HeaderItem(name)item",o,o,x
Wicket,137,org.apache.wicket.markup.html.form.Form.onComponentTag,1604,warn,java.lang.String(name)Form with id '{}' is multipart. It should use method 'POST'!-java.lang.String(name)getId(),o,o,o
Wicket,141,org.apache.wicket.markup.html.form.FormComponent.updateCollectionModel,1631,debug,"java.lang.String(name)""An error occurred while trying to modify the collection attached to "" + formComponent-java.lang.UnsupportedOperationException(name)unmodifiable",o,o,o
Wicket,142,org.apache.wicket.markup.html.form.FormComponent.updateCollectionModel,1650,debug,"java.lang.String(name)""An error occurred while trying to set the collection attached to "" + formComponent-java.lang.Exception(name)noSetter",o,o,o
Wicket,313,org.apache.wicket.markup.html.form.NestedFormsPage.onError,131,info,"java.lang.String(name)getId() + "".onError""",x,x,x
Wicket,312,org.apache.wicket.markup.html.form.NestedFormsPage.onSubmit,122,info,"java.lang.String(name)getId() + "".onSubmit""",x,x,x
Wicket,126,org.apache.wicket.markup.html.PackageResourceGuard.accept,93,warn,"java.lang.String(name)""Access denied to shared (static) resource because it is a Wicket markup file: "" + path",o,o,o
Wicket,127,org.apache.wicket.markup.html.PackageResourceGuard.accept,110,warn,"java.lang.String(name)""Access denied to shared (static) resource because of the file extension: "" + path",o,o,o
Wicket,128,org.apache.wicket.markup.html.PackageResourceGuard.accept,118,warn,"java.lang.String(name)""Access denied to shared (static) resource because of the file name: "" + path",o,o,o
Wicket,130,org.apache.wicket.markup.html.PackageResourceGuard.accept,163,warn,"java.lang.String(name)""Access to root directory is by default disabled for shared resources: "" + path",o,o,o
Wicket,131,org.apache.wicket.markup.html.SecurePackageResourceGuard.accept,193,warn,"java.lang.String(name)""Access denied to shared (static) resource: "" + path",o,o,o
Wicket,134,org.apache.wicket.markup.html.WebPage.reportMissingHead,287,error,"java.lang.String(name)""You probably forgot to add a <body> or <head> tag to your markup since no Header Container was \n"" + ""found but components were found which want to write to the <head> section.\n"" + collectedHeaderOutput",o,o,o
Wicket,135,org.apache.wicket.markup.html.WebPage.reportMissingHead,290,error,java.lang.String(name)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^,x,x,x
Wicket,110,org.apache.wicket.markup.MarkupCache.onMarkupNotFound,340,debug,"java.lang.String(name)""Markup not found: "" + cacheKey",o,o,o
Wicket,105,org.apache.wicket.markup.MarkupCache.removeMarkup,129,debug,"java.lang.String(name)""Removing from cache: "" + cacheKey",o,x,o
Wicket,106,org.apache.wicket.markup.MarkupCache.removeMarkup,146,debug,"java.lang.String(name)""Removed from cache: "" + locationString",o,x,o
Wicket,107,org.apache.wicket.markup.MarkupCache.removeMarkup,174,debug,"java.lang.String(name)""Removed from watcher: "" + modifiable",o,x,o
Wicket,120,org.apache.wicket.markup.MarkupFactory.loadMarkup,426,error,"java.lang.String(name)""Markup not found: "" + markupResourceStream-org.apache.wicket.util.resource.ResourceStreamNotFoundException(name)e",o,o,o
Wicket,116,org.apache.wicket.markup.MarkupResourceStream.setWicketNamespace,272,debug,java.lang.String(name)You are using a non-standard namespace name: '{}'-java.lang.String(name)wicketNamespace,o,o,o
Wicket,147,org.apache.wicket.markup.repeater.AbstractRepeater.onBeforeRender,136,warn,java.lang.String(name)Repeater '{}' has multiple children with the same component id: '{}'-java.lang.String(name)getPageRelativePath()-java.lang.String(name)componentId,o,o,o
Wicket,151,org.apache.wicket.markup.resolver.AutoLinkResolver.newAutoComponent,462,warn,"java.lang.String(name)""Did not find corresponding java class: "" + className",o,o,o
Wicket,152,org.apache.wicket.markup.resolver.AutoLinkResolver.newAutoComponent,500,warn,"java.lang.String(name)""Did not find corresponding java class: "" + className",o,o,o
Wicket,153,org.apache.wicket.markup.resolver.AutoLinkResolver.resolve,817,debug,"java.lang.String(name)""Added autolink "" + link",o,o,o
Wicket,58,org.apache.wicket.MarkupContainer.throwException,1547,error,java.lang.String(name)msg.toString(),x,x,x
Wicket,154,org.apache.wicket.model.ChainingModel.ChainingModel,47,warn,java.lang.String(name)It is not a good idea to reference the Session instance in models directly as it may lead to serialization problems. If you need to access a property of the session via the model use the page instance as the model object and 'session.attribute' as the path.,o,o,o
Wicket,155,org.apache.wicket.model.ChainingModel.ChainingModel,53,warn,java.lang.String(name)It is not a good idea to reference non-serializable {} in a model directly as it may lead to serialization problems.-java.lang.Class<>(name)modelObject.getClass(),o,o,o
Wicket,157,org.apache.wicket.model.LoadableDetachableModel.getObject,132,debug,java.lang.String(name)loaded transient object '{}' for '{}'-T(name)transientModelObject-org.apache.wicket.model.LoadableDetachableModel<T>(name)this,o,o,x
Wicket,25,org.apache.wicket.Page.internalOnAfterConfigure,774,debug,"java.lang.String(name)""Page not allowed to render: "" + this",o,o,o
Wicket,23,org.apache.wicket.Page.isPageStateless,488,debug,java.lang.String(name)Page '{}' is not stateless because of component with path '{}'.-org.apache.wicket.Page(name)this-java.lang.String(name)statefulComponent.getPageRelativePath(),o,o,o
Wicket,164,org.apache.wicket.page.PageAccessSynchronizer.internalUnlockPages,208,debug,java.lang.String(name)'{}' released lock to page with id '{}'-java.lang.String(name)thread.getName()-int(name)lock.pageId,o,o,o
Wicket,161,org.apache.wicket.page.PageAccessSynchronizer.lockPage,112,debug,java.lang.String(name)'{}' attempting to acquire lock to page with id '{}'-java.lang.String(name)thread.getName()-int(name)pageId,o,o,o
Wicket,163,org.apache.wicket.page.PageAccessSynchronizer.lockPage,144,warn,"java.lang.String(name)Thread '{}' failed to acquire lock to page with id '{}', attempted for {} out of allowed {}. The thread that holds the lock has name '{}'.-java.lang.String(name)thread.getName()-int(name)pageId-org.apache.wicket.util.time.Duration(name)start.elapsedSince()-org.apache.wicket.util.time.Duration(name)timeout-java.lang.String(name)previous.thread.getName()",o,o,o
Wicket,168,org.apache.wicket.page.RequestAdapter.commitRequest,191,warn,java.lang.String(name)An error occurred while checking whether a page is stateless. Assuming it is stateful.-java.lang.Exception(name)x,o,o,o
Wicket,169,org.apache.wicket.pageStore.DefaultPageStore.createSerializedPage,289,warn,java.lang.String(name)Page {} cannot be serialized. See previous logs for possible reasons.-org.apache.wicket.page.IManageablePage(name)page,o,o,o
Wicket,178,org.apache.wicket.pageStore.DiskDataStore.destroy,105,debug,java.lang.String(name)Destroying...,o,x,o
Wicket,179,org.apache.wicket.pageStore.DiskDataStore.destroy,107,debug,java.lang.String(name)Destroyed.,o,x,o
Wicket,177,org.apache.wicket.pageStore.DiskDataStore.DiskDataStore,87,warn,java.lang.String(name)Cannot create file store folder for some reason.,o,o,o
Wicket,180,org.apache.wicket.pageStore.DiskDataStore.getData,125,debug,"java.lang.String(name)Returning data{} for page with id '{}' in session with id '{}'-java.lang.Object(name)pageData != null ? """" : ""(null)""-int(name)id-java.lang.String(name)sessionId",o,o,o
Wicket,188,org.apache.wicket.pageStore.DiskDataStore.loadPage,409,error,"java.lang.String(name)""Error reading from file channel "" + channel-java.io.IOException(name)e",o,o,o
Wicket,181,org.apache.wicket.pageStore.DiskDataStore.removeData,151,debug,java.lang.String(name)Removing data for page with id '{}' in session with id '{}'-int(name)id-java.lang.String(name)sessionId,o,o,o
Wicket,186,org.apache.wicket.pageStore.DiskDataStore.savePage,356,error,"java.lang.String(name)""Error writing to a channel "" + channel-java.io.IOException(name)e",o,o,o
Wicket,187,org.apache.wicket.pageStore.DiskDataStore.savePage,365,warn,java.lang.String(name)Cannot save page with id '{}' because the data file cannot be opened.-int(name)pageId,o,o,o
Wicket,183,org.apache.wicket.pageStore.DiskDataStore.storeData,184,debug,java.lang.String(name)Storing data for page with id '{}' in session with id '{}'-int(name)id-java.lang.String(name)sessionId,o,o,o
Wicket,200,org.apache.wicket.pageStore.memory.HttpSessionDataStore.removeData,110,debug,java.lang.String(name)Removed page '{}' in session '{}'-int(name)pageId-java.lang.String(name)sessionId,o,o,o
Wicket,201,org.apache.wicket.pageStore.memory.HttpSessionDataStore.removeData,122,debug,java.lang.String(name)Removed all pages in session '{}'-java.lang.String(name)sessionId,o,o,o
Wicket,203,org.apache.wicket.pageStore.memory.HttpSessionDataStore.storeData,142,error,java.lang.String(name)Cannot store the data for page with id '{}' in session with id '{}'-int(name)pageId-java.lang.String(name)sessionId,o,o,o
Wicket,205,org.apache.wicket.protocol.http.AbstractRequestLogger.requestTime,241,error,"java.lang.String(name)""Exception while determining the size of the session in the request logger: "" + e.getMessage()-java.lang.Exception(name)e",o,o,o
Wicket,210,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.checkRequest,407,debug,"java.lang.String(name)Source URI not present in request, {}-org.apache.wicket.protocol.http.CsrfAction(name)noOriginAction",o,o,o
Wicket,211,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.checkRequest,434,debug,"java.lang.String(name)Source URI conflicts with request origin, {}-org.apache.wicket.protocol.http.CsrfAction(name)conflictingOriginAction",o,o,o
Wicket,213,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.isWhitelistedHost,482,debug,java.lang.String(name)Origin: {} not parseable as an URI. Whitelisted-origin check skipped.-java.lang.String(name)sourceUri,o,o,o
Wicket,216,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.matchingOrigin,669,debug,"java.lang.String(name)CSRF Origin {} matched requested resource, allowed for page {}-java.lang.String(name)origin-java.lang.String(name)page.getClass().getName()",o,o,o
Wicket,208,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.onRequestHandlerResolved,360,debug,"java.lang.String(name)Targeted page {} was opted out of the CSRF origin checks, allowed-java.lang.String(name)targetedPage.getClass().getName()",o,o,o
Wicket,207,org.apache.wicket.protocol.http.CsrfPreventionRequestCycleListener.onRequestHandlerResolved,335,trace,"java.lang.String(name)CSRF listener is disabled, no checks performed",o,o,o
Wicket,232,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateCloseTag,315,error,"java.lang.String(name)""Found closing tag </"" + parser.getTag() + ""> when we expecting ""+ ""the closing tag </""+ expectedTag+ ""> instead""",o,o,o
Wicket,234,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateCloseTag,332,error,java.lang.String(name)Unexpected parsing error,o,o,o
Wicket,237,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateComment,381,error,"java.lang.String(name)""Found comment '"" + parser.getComment() + ""' was not expected. ""+ ""We were not expecting any more elements within the current tag""",o,o,o
Wicket,244,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateTag,469,error,"java.lang.String(name)""Found tag <"" + parser.getTag() + ""> was not expected. ""+ ""We were not expecting any more elements within the current tag""",o,o,o
Wicket,243,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateTag,462,error,"java.lang.String(name)""Found tag <"" + parser.getTag() + ""> was not expected. ""+ ""We were expecting: ""+ e.toString()",o,o,o
Wicket,247,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateText,510,error,"java.lang.String(name)""Found text '"" + parser.getText() + ""' was not expected. ""+ ""We were not expecting any more elements within the current tag""",o,o,o
Wicket,246,org.apache.wicket.protocol.http.documentvalidation.HtmlDocumentValidator.validateText,503,error,"java.lang.String(name)""Found text '"" + parser.getText() + ""' was not expected. ""+ ""We were expecting: ""+ e.toString()",o,o,o
Wicket,259,org.apache.wicket.protocol.http.servlet.XForwardedRequestWrapperFactory.newRequestWrapper,743,debug,"java.lang.String(name)""Incoming request "" + request.getRequestURI() + "" with originalRemoteAddr '""+ request.getRemoteAddr()+ ""', originalRemoteHost='""+ request.getRemoteHost()+ ""', originalSecure='""+ request.isSecure()+ ""', originalScheme='""+ request.getScheme()+ ""', original[""+ config.remoteIPHeader+ ""]='""+ request.getHeader(config.remoteIPHeader)+ "", original[""+ config.protocolHeader+ ""]='""+ (config.protocolHeader == null ? null : request.getHeader(config.protocolHeader))+ ""' will be seen as newRemoteAddr='""+ xRequest.getRemoteAddr()+ ""', newRemoteHost='""+ xRequest.getRemoteHost()+ ""', newScheme='""+ xRequest.getScheme()+ ""', newSecure='""+ xRequest.isSecure()+ ""', new[""+ config.remoteIPHeader+ ""]='""+ xRequest.getHeader(config.remoteIPHeader)+ "", new[""+ config.proxiesHeader+ ""]='""+ xRequest.getHeader(config.proxiesHeader)+ ""'""",x,x,x
Wicket,220,org.apache.wicket.protocol.http.WebApplication.getConfigurationType,800,warn,java.lang.String(name)SecurityManager doesn't allow to read the configuration type from the system properties. The configuration type will be read from the web.xml.,o,o,o
Wicket,222,org.apache.wicket.protocol.http.WebApplication.storeBufferedResponse,1005,error,"java.lang.String(name)storeBufferedResponse needs a valid session id to store the response, but a null one was found. Please report the problem to dev team and try to reproduce it in a quickstart project.",o,o,o
Wicket,225,org.apache.wicket.protocol.http.WicketFilter.init,436,warn,"java.lang.String(name)Unable to determine filter path from filter init-param, web.xml, or servlet 3.0 annotations. Assuming user will set filter path manually by calling setFilterPath(String)",o,o,o
Wicket,224,org.apache.wicket.protocol.http.WicketFilter.processRequest,173,debug,java.lang.String(name)Ignoring request {}-java.lang.StringBuffer(name)httpServletRequest.getRequestURL(),o,o,o
Wicket,340,org.apache.wicket.protocol.ws.api.AbstractWebSocketProcessor.broadcastMessage,287,debug,java.lang.String(name)Either there is no connection({}) or it is closed.-org.apache.wicket.protocol.ws.api.IWebSocketConnection(name)connection,o,o,o
Wicket,339,org.apache.wicket.protocol.ws.api.AbstractWebSocketProcessor.broadcastMessage,269,error,java.lang.String(name)An error occurred during processing of a WebSocket message-java.lang.Exception(name)x,o,o,o
Wicket,341,org.apache.wicket.protocol.ws.api.WebSocketRequestHandler.push,74,error,java.lang.String(name)An error occurred while pushing text message.-java.io.IOException(name)iox,o,o,o
Wicket,343,org.apache.wicket.protocol.ws.api.WebSocketRequestHandler.push,94,error,java.lang.String(name)An error occurred while pushing binary message.-java.io.IOException(name)iox,o,o,o
Wicket,342,org.apache.wicket.protocol.ws.api.WebSocketRequestHandler.push,79,warn,java.lang.String(name)The websocket connection is already closed. Cannot push the text message '{}'-java.lang.CharSequence(name)message,o,o,o
Wicket,344,org.apache.wicket.protocol.ws.api.WebSocketRequestHandler.push,99,warn,java.lang.String(name)The websocket connection is already closed. Cannot push the binary message '{}'-byte[](name)message,o,o,o
Wicket,345,org.apache.wicket.protocol.ws.api.WebSocketResponse.close,105,error,java.lang.String(name)An error occurred while writing response to WebSocket client.-java.io.IOException(name)iox,o,o,o
Wicket,347,org.apache.wicket.protocol.ws.api.WebSocketResponse.sendError,193,warn,java.lang.String(name)An HTTP error response in WebSocket communication would not be processed by the browser! If you need to send the error code and message to the client then configure custom WebSocketResponse via WebSocketSettings#newWebSocketResponse() factory method and override #sendError() method to write them in an appropriate format for your application. The ignored error code is '{}' and the message: '{}'.-int(name)sc-java.lang.String(name)msg,o,x,o
Wicket,354,org.apache.wicket.protocol.ws.javax.JavaxWebSocketConnection.close,71,error,java.lang.String(name)An error occurred while closing WebSocket session-java.io.IOException(name)iox,o,o,o
Wicket,355,org.apache.wicket.protocol.ws.javax.WicketEndpoint.onClose,80,debug,java.lang.String(name)Web Socket connection with id '{}' has been closed with code '{}' and reason: {}-java.lang.String(name)session.getId()-int(name)closeCode-java.lang.String(name)reasonPhrase,o,o,o
Wicket,356,org.apache.wicket.protocol.ws.javax.WicketEndpoint.onError,92,error,"java.lang.String(name)""An error occurred in web socket connection with id : "" + session.getId()-java.lang.Throwable(name)t",o,o,o
Wicket,350,org.apache.wicket.protocol.ws.javax.WicketServerEndpointConfig.modifyHandshake,146,trace,"java.lang.String(name)parameterMap: {}-java.util.Map<String,List<String>>(name)parameterMap",o,x,o
Wicket,351,org.apache.wicket.protocol.ws.javax.WicketServerEndpointConfig.modifyHandshake,154,trace,java.lang.String(name)queryString: {}-java.lang.String(name)queryString,o,x,o
Wicket,352,org.apache.wicket.protocol.ws.javax.WicketServerEndpointConfig.modifyHandshake,162,trace,java.lang.String(name)requestURI: {}-java.net.URI(name)requestURI,o,x,o
Wicket,353,org.apache.wicket.protocol.ws.javax.WicketServerEndpointConfig.modifyHandshake,169,trace,java.lang.String(name)userPrincipal: {}-java.security.Principal(name)userPrincipal,o,x,o
Wicket,262,org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler,317,error,"java.lang.String(name)""Error during request processing. URL="" + request.getUrl()-java.lang.Exception(name)exception",o,o,o
Wicket,268,org.apache.wicket.request.cycle.RequestCycle.handleDetachException,698,error,java.lang.String(name)Error detaching RequestCycle-java.lang.RuntimeException(name)exception,o,o,o
Wicket,265,org.apache.wicket.request.cycle.RequestCycle.handleException,364,warn,java.lang.String(name)Handling the following exception-java.lang.Exception(name)e,o,o,o
Wicket,266,org.apache.wicket.request.cycle.RequestCycle.handleException,365,warn,java.lang.String(name)********************************,x,x,x
Wicket,261,org.apache.wicket.request.cycle.RequestCycle.processRequest,247,debug,"java.lang.String(name)No suitable handler found for URL {}, falling back to container to process this request-org.apache.wicket.request.Url(name)request.getUrl()",o,o,o
Wicket,269,org.apache.wicket.request.cycle.RequestCycleListenerCollection.onException,140,debug,"java.lang.String(name){} exception handlers available for exception {}, using the first handler-int(name)handlers.size()-java.lang.Exception(name)ex",o,o,o
Wicket,361,org.apache.wicket.request.mapper.CompoundRequestMapper.logMappers,177,debug,java.lang.String(name)No compatible mapper found for URL '{}'-java.lang.String(name)url,o,o,o
Wicket,363,org.apache.wicket.request.mapper.CompoundRequestMapper.logMappers,185,debug,java.lang.String(name)Multiple compatible mappers found for URL '{}'-java.lang.String(name)url,o,o,o
Wicket,364,org.apache.wicket.request.mapper.CompoundRequestMapper.logMappers,188,debug,java.lang.String(name) * {}-org.apache.wicket.request.mapper.MapperWithScore(name)compatibleMapper,x,x,x
Wicket,280,org.apache.wicket.request.resource.caching.FilenameWithVersionResourceCachingStrategy.decorateUrl,139,error,java.lang.String(name)A resource with name '{}' contains the version prefix '{}' so the un-decoration will not work. Either use a different version prefix or rename this resource.-java.lang.String(name)filename-java.lang.String(name)getVersionPrefix(),o,o,o
Wicket,281,org.apache.wicket.request.resource.caching.version.MessageDigestResourceVersion.getVersion,152,warn,"java.lang.String(name)""unable to compute hash for "" + resource-java.io.IOException(name)e",o,o,x
Wicket,282,org.apache.wicket.request.resource.caching.version.MessageDigestResourceVersion.getVersion,157,warn,"java.lang.String(name)""unable to locate resource for "" + resource-org.apache.wicket.util.resource.ResourceStreamNotFoundException(name)e",o,o,x
Wicket,276,org.apache.wicket.request.resource.ClassScanner.scanClass,76,warn,java.lang.String(name)Error accessing object property-java.lang.Exception(name)e,o,o,o
Wicket,274,org.apache.wicket.request.resource.PackageResource.compressResponse,473,error,java.lang.String(name)Error while compressing the content-java.lang.Exception(name)e,o,o,o
Wicket,271,org.apache.wicket.request.resource.PackageResource.newResourceResponse,389,debug,java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,x,x
Wicket,272,org.apache.wicket.request.resource.PackageResource.newResourceResponse,394,debug,java.lang.String(name)e.getMessage()-org.apache.wicket.util.resource.ResourceStreamNotFoundException(name)e,x,x,x
Wicket,275,org.apache.wicket.request.resource.PackageResource.sendResourceError,522,warn,java.lang.String(name)msg,x,x,x
Wicket,278,org.apache.wicket.request.resource.ResourceReferenceRegistry._registerResourceReference,156,warn,java.lang.String(name){} cannot be added to the registry.-java.lang.String(name)reference.getClass().getName(),o,o,o
Wicket,277,org.apache.wicket.request.resource.ResourceStreamResource.close,232,error,java.lang.String(name)Couldn't close ResourceStream-java.io.IOException(name)e,o,o,o
Wicket,360,org.apache.wicket.request.UrlRenderer.removeCommonPrefixes,344,debug,java.lang.String(name)Segments '{}' do not start with common prefix '{}'-java.util.List<String>(name)segments-org.apache.wicket.request.Url(name)commonPrefix,o,o,o
Wicket,53,org.apache.wicket.RequestListenerInterface.invoke,199,warn,java.lang.String(name)behavior not enabled; ignore call. Behavior {} at component {}-org.apache.wicket.behavior.Behavior(name)behavior-org.apache.wicket.Component(name)component,o,o,x
Wicket,54,org.apache.wicket.RequestListenerInterface.registerRequestListenerInterface,264,info,"java.lang.String(name)""registered listener interface "" + this",o,o,x
Wicket,290,org.apache.wicket.resource.bundles.ConcatBundleResource.reportError,267,warn,java.lang.String(name)message,x,x,x
Wicket,285,org.apache.wicket.resource.PropertiesFactory.addToWatcher,265,info,"java.lang.String(name)""A properties files has changed. Removing all entries "" + ""from the cache. Resource: "" + resourceStream",o,o,o
Wicket,284,org.apache.wicket.resource.PropertiesFactory.loadFromLoader,239,warn,"java.lang.String(name)""Unable to find resource "" + resourceStream-java.lang.Exception(name)e",o,o,o
Wicket,296,org.apache.wicket.response.filter.EmptySrcAttributeCheckFilter.filter,68,warn,"java.lang.String(name)""[...]"" + responseBuffer.substring(from,to) + ""[...]""",x,x,x
Wicket,297,org.apache.wicket.response.filter.XmlCleaningResponseFilter.stripNonValidXMLCharacters,115,debug,"java.lang.String(name)An invalid character '{}' found at position '{}' in '{}'-java.lang.String(name)String.format(""0x%X"",codePoint)-int(name)i-java.lang.String(name)new String(chars)",o,o,o
Wicket,300,org.apache.wicket.serialize.java.JavaSerializer.resolveClass,202,debug,"java.lang.String(name)Class not found by the object outputstream itself, trying the IClassResolver",o,o,o
Wicket,301,org.apache.wicket.serialize.java.JavaSerializer.resolveProxyClass,279,debug,"java.lang.String(name)Proxy Class not found by the object outputstream itself, trying the IClassResolver",o,o,o
Wicket,302,org.apache.wicket.serialize.java.JavaSerializer.writeObjectOverride,401,error,"java.lang.String(name)""error writing object "" + obj + "": ""+ e.getMessage()-java.lang.Exception(name)e",o,o,x
Wicket,304,org.apache.wicket.session.HttpSessionStore.valueUnbound,430,debug,java.lang.String(name)Wicket application with name '{}' not found.-java.lang.String(name)applicationKey,o,o,o
Wicket,303,org.apache.wicket.session.HttpSessionStore.valueUnbound,420,debug,java.lang.String(name)Session unbound: {}-java.lang.String(name)sessionId,o,o,o
Wicket,307,org.apache.wicket.util.cookies.CookieUtils.remove,219,debug,"java.lang.String(name)""Removed Cookie: "" + cookie.getName()",o,o,o
Wicket,365,org.apache.wicket.util.crypt.AbstractCrypt.decryptUrlSafe,71,debug,"java.lang.String(name)""Error decoding text: "" + text-java.lang.Exception(name)ex",o,o,o
Wicket,379,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,84,debug,java.lang.String(name)exception: -java.lang.Exception(name)e,x,x,x
Wicket,372,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,75,warn,"java.lang.String(name)failed, Wicket will fallback on a dummy implementation",o,o,x
Wicket,374,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,77,warn,java.lang.String(name)This is NOT recommended for production systems.,o,o,o
Wicket,369,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,72,warn,java.lang.String(name)************************** WARNING **************************,x,x,x
Wicket,373,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,76,warn,"java.lang.String(name)""\t("" + NoCrypt.class.getName() + "")""",x,x,x
Wicket,371,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,74,warn,"java.lang.String(name)""\t"" + cryptClass",x,x,x
Wicket,378,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,81,warn,"java.lang.String(name)""\t"" + e.getMessage()",x,x,x
Wicket,381,org.apache.wicket.util.crypt.ClassCryptFactory.newCrypt,90,warn,java.lang.String(name)*************************************************************,x,x,x
Wicket,383,org.apache.wicket.util.encoding.UrlDecoder.decode,168,info,java.lang.String(name)Illegal hex characters in escape (%) pattern in '{}'. The escape character (%) will be ignored. NumberFormatException: {} -java.lang.String(name)s-java.lang.String(name)e.getMessage(),o,o,o
Wicket,384,org.apache.wicket.util.file.WebXmlFile.getFilterPath,92,error,java.lang.String(name)Error reading servlet/filter path from web.xml-java.lang.Exception(name)ex,o,o,o
Wicket,357,org.apache.wicket.util.instrument.InstrumentationObjectSizeOfStrategy.sizeOf,128,error,"java.lang.String(name)""An error occurred while calculating the size of object: "" + obj-java.io.IOException(name)e",o,o,o
Wicket,389,org.apache.wicket.util.io.IOUtils.closeQuietly,125,debug,"java.lang.String(name)""closing resource failed: "" + e.getMessage()-java.io.IOException(name)e",o,o,x
Wicket,390,org.apache.wicket.util.lang.Threads.dumpAllThreads,69,warn,java.lang.String(name)dump.toString(),x,x,x
Wicket,391,org.apache.wicket.util.lang.Threads.dumpSingleThread,114,warn,"java.lang.String(name)MapVariableInterpolator.interpolate(FORMAT,variables)-org.apache.wicket.util.lang.ThreadDump(name)throwable",x,x,x
Wicket,394,org.apache.wicket.util.listener.ListenerCollection.notifyIgnoringExceptions,100,error,"java.lang.String(name)""Error invoking listener: "" + listener-java.lang.Exception(name)e",o,o,o
Wicket,395,org.apache.wicket.util.listener.ListenerCollection.reversedNotifyIgnoringExceptions,124,error,"java.lang.String(name)""Error invoking listener: "" + listener-java.lang.Exception(name)e",o,o,o
Wicket,396,org.apache.wicket.util.resource.ZipResourceStream.zipDir,141,debug,java.lang.String(name)Adding: '{}'-java.lang.String(name)file,o,o,o
Wicket,397,org.apache.wicket.util.string.StringValue.toBoolean,413,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a boolean: %s"",text,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,398,org.apache.wicket.util.string.StringValue.toChar,464,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a character: %s"",text,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,399,org.apache.wicket.util.string.StringValue.toDouble,522,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a double: %s"",text,x.getMessage())-java.lang.Exception(name)x",o,o,o
Wicket,400,org.apache.wicket.util.string.StringValue.toDuration,575,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a Duration: %s"",text,x.getMessage())-java.lang.Exception(name)x",o,o,o
Wicket,404,org.apache.wicket.util.string.StringValue.toEnum,921,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a %s: %s"",text,eClass,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,401,org.apache.wicket.util.string.StringValue.toInt,622,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to an integer: %s"",text,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,402,org.apache.wicket.util.string.StringValue.toLong,689,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a long: %s"",text,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,403,org.apache.wicket.util.string.StringValue.toTime,863,debug,"java.lang.String(name)String.format(""An error occurred while converting '%s' to a Time: %s"",text,x.getMessage())-org.apache.wicket.util.string.StringValueConversionException(name)x",o,o,o
Wicket,406,org.apache.wicket.util.thread.Task.run,120,error,"java.lang.String(name)""Unhandled exception thrown by user code in task "" + name-java.lang.Exception(name)e",o,o,o
Wicket,408,org.apache.wicket.util.thread.Task.run,136,error,java.lang.String(name)Task '{}' terminated-java.lang.String(name)name-java.lang.Exception(name)x,o,o,o
Wicket,405,org.apache.wicket.util.thread.Task.run,110,trace,java.lang.String(name)Run the job: '{}'-org.apache.wicket.util.thread.ICode(name)code,o,o,o
Wicket,407,org.apache.wicket.util.thread.Task.run,126,trace,java.lang.String(name)Finished with job: '{}'-org.apache.wicket.util.thread.ICode(name)code,o,o,o
Wicket,411,org.apache.wicket.velocity.Initializer.getVelocityProperties,108,error,java.lang.String(name)e.getMessage()-java.io.IOException(name)e,x,x,x
Wicket,410,org.apache.wicket.velocity.Initializer.init,60,info,java.lang.String(name)Initialized Velocity successfully,o,o,o
Zookeeper,233,org.apache.zookeeper.cli.SetQuotaCommand.checkIfChildQuota,205,debug,java.lang.String(name)child removed during quota check-org.apache.zookeeper.NoNodeException(name)ne,o,o,x
Zookeeper,235,org.apache.zookeeper.client.StaticHostProvider.resolve,148,error,java.lang.String(name)Unable to resolve address: {}-java.lang.String(name)address.toString()-java.net.UnknownHostException(name)e,o,o,o
Zookeeper,241,org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslClient,257,error,java.lang.String(name)Exception while trying to create SASL client.-java.lang.Exception(name)e,o,o,o
Zookeeper,244,org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken,319,debug,java.lang.String(name)saslClient.evaluateChallenge(len={})-int(name)saslToken.length,x,x,x
Zookeeper,243,org.apache.zookeeper.client.ZooKeeperSaslClient.respondToServer,275,error,java.lang.String(name)SASL authentication failed using login context '{}'.-java.lang.String(name)this.getLoginContext()-javax.security.sasl.SaslException(name)e,o,o,o
Zookeeper,246,org.apache.zookeeper.client.ZooKeeperSaslClient.sendSaslPacket,348,debug,java.lang.String(name)ClientCnxn:sendSaslPacket:length={}-int(name)saslToken.length,x,x,x
Zookeeper,247,org.apache.zookeeper.client.ZooKeeperSaslClient.sendSaslPacket,363,debug,java.lang.String(name)ClientCnxn:sendSaslPacket:length={}-int(name)saslToken.length,x,x,x
Zookeeper,157,org.apache.zookeeper.ClientCnxn.disconnect,1446,warn,java.lang.String(name)Got interrupted while waiting for the sender thread to close-java.lang.InterruptedException(name)ex,o,o,o
Zookeeper,143,org.apache.zookeeper.ClientCnxn.logStartConnect,1121,info,java.lang.String(name)Opening socket connection to server {}.-java.net.InetSocketAddress(name)addr,o,o,o
Zookeeper,154,org.apache.zookeeper.ClientCnxn.onConnected,1377,error,java.lang.String(name)Read/write client got connected to read-only server,o,o,o
Zookeeper,150,org.apache.zookeeper.ClientCnxn.pingRwServer,1300,warn,java.lang.String(name)Exception while seeking for r/w server.-java.io.IOException(name)e,o,o,x
Zookeeper,140,org.apache.zookeeper.ClientCnxn.primeConnection,1043,debug,java.lang.String(name)Session establishment request sent on {}-java.net.SocketAddress(name)clientCnxnSocket.getRemoteSocketAddress(),o,o,o
Zookeeper,131,org.apache.zookeeper.ClientCnxn.processEvent,599,warn,java.lang.String(name)Somehow a null cb got to EventThread!,o,o,x
Zookeeper,137,org.apache.zookeeper.ClientCnxn.readResponse,899,debug,java.lang.String(name)Got {} for session id 0x{}-org.apache.zookeeper.WatchedEvent(name)we-java.lang.String(name)Long.toHexString(sessionId),o,o,o
Zookeeper,128,org.apache.zookeeper.ClientCnxn.run,552,error,java.lang.String(name)Event thread exiting due to interruption-java.lang.InterruptedException(name)e,o,o,o
Zookeeper,129,org.apache.zookeeper.ClientCnxn.run,555,info,java.lang.String(name)EventThread shut down for session: 0x{}-java.lang.String(name)Long.toHexString(getSessionId()),o,o,o
Zookeeper,199,org.apache.zookeeper.ClientCnxnSocket.readConnectResult,150,warn,java.lang.String(name)Connected to an old server; r-o mode will be unavailable,o,o,o
Zookeeper,195,org.apache.zookeeper.ClientCnxnSocketNetty.initSSL,454,info,java.lang.String(name)SSL handler added for channel: {}-io.netty.channel.Channel(name)pipeline.channel(),o,o,o
Zookeeper,205,org.apache.zookeeper.ClientCnxnSocketNIO.cleanup,193,debug,java.lang.String(name)Ignoring exception during shutdown input-java.io.IOException(name)e,o,o,o
Zookeeper,206,org.apache.zookeeper.ClientCnxnSocketNIO.cleanup,198,debug,java.lang.String(name)Ignoring exception during shutdown output-java.io.IOException(name)e,o,o,o
Zookeeper,207,org.apache.zookeeper.ClientCnxnSocketNIO.cleanup,203,debug,java.lang.String(name)Ignoring exception during socket close-java.io.IOException(name)e,o,o,o
Zookeeper,208,org.apache.zookeeper.ClientCnxnSocketNIO.cleanup,208,debug,java.lang.String(name)Ignoring exception during channel close-java.io.IOException(name)e,o,o,o
Zookeeper,212,org.apache.zookeeper.ClientCnxnSocketNIO.close,232,warn,java.lang.String(name)Ignoring exception during selector close-java.io.IOException(name)e,o,o,o
Zookeeper,213,org.apache.zookeeper.ClientCnxnSocketNIO.connect,270,error,java.lang.String(name)Unable to open socket to {}-java.net.InetSocketAddress(name)addr,o,o,o
Zookeeper,259,org.apache.zookeeper.common.AtomicFileOutputStream.abort,120,warn,java.lang.String(name)Unable to delete tmp file during abort {}-java.io.File(name)tmpFile,o,o,o
Zookeeper,258,org.apache.zookeeper.common.AtomicFileOutputStream.abort,116,warn,java.lang.String(name)Unable to abort file {}-java.io.File(name)tmpFile-java.io.IOException(name)ioe,o,o,o
Zookeeper,257,org.apache.zookeeper.common.AtomicFileOutputStream.close,102,warn,java.lang.String(name)Unable to delete tmp file {}-java.io.File(name)tmpFile,o,o,o
Zookeeper,277,org.apache.zookeeper.common.NettyUtils.getClientReachableLocalInetAddressCount,147,debug,java.lang.String(name)Detected {} local network addresses: {}-int(name)validInetAddresses.size()-java.util.Set<InetAddress>(name)validInetAddresses,o,o,o
Zookeeper,271,org.apache.zookeeper.common.PathTrie.deletePath,250,debug,java.lang.String(name){}-org.apache.zookeeper.common.TrieNode(name)parent,x,x,x
Zookeeper,272,org.apache.zookeeper.common.PathTrie.existsNode,284,debug,java.lang.String(name){}-org.apache.zookeeper.common.TrieNode(name)parent,x,x,x
Zookeeper,273,org.apache.zookeeper.common.PathTrie.findMaxPrefix,311,debug,java.lang.String(name){}-java.lang.String(name)element,x,x,x
Zookeeper,253,org.apache.zookeeper.common.SSLContextAndOptions.getHandshakeDetectionTimeoutMillis,201,warn,"java.lang.String(name)Invalid value for {}: {}, using the default value of {}-java.lang.String(name)x509Util.getSslHandshakeDetectionTimeoutMillisProperty()-int(name)result-int(name)5000",o,o,o
Zookeeper,262,org.apache.zookeeper.common.X509Util.createSSLContextAndOptions,309,debug,java.lang.String(name)Loading SSLContext supplier from property '{}'-java.lang.String(name)sslContextSupplierClassProperty,o,o,o
Zookeeper,267,org.apache.zookeeper.common.X509Util.getDefaultCipherSuitesForJavaVersion,547,debug,"java.lang.String(name)Could not parse java version {}, using Java8 optimized cipher suites-java.lang.String(name)javaVersion",o,o,o
Zookeeper,260,org.apache.zookeeper.common.X509Util.getSslHandshakeTimeoutMillis,297,error,java.lang.String(name)Error creating SSL context and options-org.apache.zookeeper.common.SSLContextException(name)e,o,o,o
Zookeeper,270,org.apache.zookeeper.common.X509Util.handleWatchEvent,641,debug,java.lang.String(name)Ignoring watch event and keeping previous default SSL context. Event kind: {} with context: {}-java.nio.file.Kind<>(name)event.kind()-(name)event.context(),o,o,o
Zookeeper,269,org.apache.zookeeper.common.X509Util.handleWatchEvent,631,debug,java.lang.String(name)Attempting to reset default SSL context after receiving watch event: {} with context: {}-java.nio.file.Kind<>(name)event.kind()-(name)event.context(),o,o,o
Zookeeper,281,org.apache.zookeeper.common.ZKConfig.setProperty,180,debug,java.lang.String(name)key {}'s value {} is replaced with new value {}-java.lang.String(name)key-java.lang.String(name)oldValue-java.lang.String(name)value,o,o,x
Zookeeper,254,org.apache.zookeeper.common.ZKTrustManager.performHostVerification,154,debug,java.lang.String(name)Failed to verify host address: {} attempting to verify host name with reverse dns lookup-java.lang.String(name)hostAddress-javax.net.ssl.SSLException(name)addressVerificationException,o,o,o
Zookeeper,255,org.apache.zookeeper.common.ZKTrustManager.performHostVerification,161,error,java.lang.String(name)Failed to verify host address: {}-java.lang.String(name)hostAddress-javax.net.ssl.SSLException(name)addressVerificationException,o,o,o
Zookeeper,256,org.apache.zookeeper.common.ZKTrustManager.performHostVerification,162,error,java.lang.String(name)Failed to verify hostname: {}-java.lang.String(name)hostName-javax.net.ssl.SSLException(name)hostnameVerificationException,o,o,o
Zookeeper,215,org.apache.zookeeper.CreateMode.fromFlag,137,error,java.lang.String(name)errMsg,x,x,x
Zookeeper,14,org.apache.zookeeper.graph.Log4JSource.size,129,trace,"java.lang.String(name)""saved pos () = "" + pos",o,x,x
Zookeeper,15,org.apache.zookeeper.graph.Log4JSource.size,153,trace,"java.lang.String(name)""size() = "" + count",x,x,x
Zookeeper,3,org.apache.zookeeper.graph.LogSkipList.findMarkBefore,89,trace,"java.lang.String(name)""return "" + last",x,x,x
Zookeeper,5,org.apache.zookeeper.graph.RandomAccessFileReader.RandomAccessFileReader,52,debug,"java.lang.String(name)""Opened file("" + f + "") coulds get FD""",o,o,x
Zookeeper,7,org.apache.zookeeper.graph.RandomAccessFileReader.read,116,trace,"java.lang.String(name)""read(buf, off="" + off + "", len=""+ len",x,x,x
Zookeeper,8,org.apache.zookeeper.graph.RandomAccessFileReader.read,130,trace,"java.lang.String(name)""tocopy="" + tocopy",x,x,x
Zookeeper,9,org.apache.zookeeper.graph.RandomAccessFileReader.read,142,trace,"java.lang.String(name)""read="" + read",x,x,x
Zookeeper,43,org.apache.zookeeper.inspector.gui.nodeviewer.NodeViewerACL.nodeSelectionChanged,99,error,"java.lang.String(name)""Error retrieving ACL Information for node: "" + NodeViewerACL.this.selectedNode-java.lang.InterruptedException(name)e",o,o,o
Zookeeper,41,org.apache.zookeeper.inspector.gui.nodeviewer.NodeViewerMetaData.nodeSelectionChanged,100,error,"java.lang.String(name)""Error retrieving meta data for node: "" + NodeViewerMetaData.this.selectedNode-java.lang.InterruptedException(name)e",o,o,o
Zookeeper,27,org.apache.zookeeper.inspector.gui.ZooInspectorAboutDialog.ZooInspectorAboutDialog,61,error,"java.lang.String(name)Error loading about.html, file may be corrupt-java.io.IOException(name)e",o,o,o
Zookeeper,28,org.apache.zookeeper.inspector.gui.ZooInspectorConnectionPropertiesDialog.ZooInspectorConnectionPropertiesDialog,177,error,java.lang.String(name)An Error occurred loading connection properties from file-java.io.IOException(name)ex,o,o,x
Zookeeper,37,org.apache.zookeeper.inspector.gui.ZooInspectorNodeViewersDialog.ZooInspectorNodeViewersDialog,351,error,java.lang.String(name)An error occurred while instaniating the node viewer. -java.lang.Exception(name)ex,o,o,o
Zookeeper,38,org.apache.zookeeper.inspector.gui.ZooInspectorNodeViewersDialog.ZooInspectorNodeViewersDialog,436,error,java.lang.String(name)Error saving node viewer configuration from file.-java.io.IOException(name)ex,o,o,o
Zookeeper,39,org.apache.zookeeper.inspector.gui.ZooInspectorNodeViewersDialog.ZooInspectorNodeViewersDialog,476,error,java.lang.String(name)Error loading node viewer configuration from file.-java.lang.Exception(name)ex,o,o,o
Zookeeper,36,org.apache.zookeeper.inspector.gui.ZooInspectorNodeViewersDialog.ZooInspectorNodeViewersDialog,172,error,"java.lang.String(name)""Error instantiating class: "" + data-java.lang.Exception(name)e",o,o,o
Zookeeper,58,org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.deleteNode,539,error,"java.lang.String(name)""Error occurred deleting node: "" + nodePath-java.lang.Exception(name)e",o,o,o
Zookeeper,49,org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.disconnect,207,error,java.lang.String(name)Error occurred while disconnecting from ZooKeeper server-java.lang.Exception(name)e,o,o,o
Zookeeper,51,org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.getACLs,355,error,"java.lang.String(name)""Error occurred retrieving ACLs of node: "" + nodePath-java.lang.InterruptedException(name)e",o,o,o
Zookeeper,62,org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.getDefaultNodeViewerConfiguration,851,warn,java.lang.String(name)List of default node viewers is empty,o,o,o
Zookeeper,56,org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.getSessionMeta,480,error,java.lang.String(name)Error occurred retrieving session meta data.-java.lang.Exception(name)e,o,o,o
Zookeeper,297,org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans,111,error,java.lang.String(name)Problems while registering log4j jmx beans!-java.lang.Exception(name)e,o,o,o
Zookeeper,302,org.apache.zookeeper.jmx.MBeanRegistry.makeObjectName,218,warn,"java.lang.String(name)Invalid name ""{}"" for class {}-java.lang.StringBuilder(name)beanName-java.lang.Class<>(name)bean.getClass()",o,o,o
Zookeeper,298,org.apache.zookeeper.jmx.MBeanRegistry.register,110,warn,java.lang.String(name)Failed to register MBean {}-java.lang.String(name)bean.getName(),o,o,o
Zookeeper,300,org.apache.zookeeper.jmx.MBeanRegistry.unregister,153,warn,java.lang.String(name)Error during unregister of [{}]-java.lang.String(name)bean.getName()-javax.management.JMException(name)e,o,o,o
Zookeeper,183,org.apache.zookeeper.Login.getRefreshTime,335,info,java.lang.String(name)TGT expires: {}-java.lang.String(name)tgt.getEndTime().toString(),o,x,x
Zookeeper,169,org.apache.zookeeper.Login.Login,184,error,java.lang.String(name)next refresh: {} is later than expiry {}. This may indicate a clock skew problem. Check that this host and the KDC's hosts' clocks are in sync. Exiting refresh thread.-java.util.Date(name)nextRefreshDate-java.util.Date(name)expiryDate,o,o,x
Zookeeper,179,org.apache.zookeeper.Login.Login,267,error,java.lang.String(name)Failed to refresh TGT: refresh thread exiting now.-javax.security.auth.login.LoginException(name)le,o,o,x
Zookeeper,175,org.apache.zookeeper.Login.Login,230,error,"java.lang.String(name)Interrupted while renewing TGT, exiting Login thread",o,o,x
Zookeeper,178,org.apache.zookeeper.Login.Login,262,error,java.lang.String(name)Could not refresh TGT for principal: {}.-java.lang.String(name)principal-javax.security.auth.login.LoginException(name)le,o,o,x
Zookeeper,177,org.apache.zookeeper.Login.Login,258,error,java.lang.String(name)Interrupted during login retry after LoginException:-javax.security.auth.login.LoginException(name)le,o,o,o
Zookeeper,170,org.apache.zookeeper.Login.Login,195,info,java.lang.String(name)refreshing now because expiry is before next scheduled refresh time.,o,o,x
Zookeeper,168,org.apache.zookeeper.Login.Login,172,warn,java.lang.String(name)TGT refresh thread time adjusted from : {} to : {} since the former is sooner than the minimum refresh interval ({} seconds) from now.-java.util.Date(name)until-java.util.Date(name)newuntil-long(name)60,o,o,x
Zookeeper,176,org.apache.zookeeper.Login.Login,234,warn,java.lang.String(name)Could not renew TGT due to problem running shell command: '{} {}'. Exiting refresh thread.-java.lang.String(name)cmd-java.lang.String(name)kinitArgs-java.lang.Exception(name)e,x,o,x
Zookeeper,166,org.apache.zookeeper.Login.Login,141,warn,java.lang.String(name)No TGT found: will try again at {}-java.util.Date(name)nextRefreshDate,o,o,x
Zookeeper,180,org.apache.zookeeper.Login.shutdown,289,warn,java.lang.String(name)error while waiting for Login thread to shutdown.-java.lang.InterruptedException(name)e,o,o,x
Zookeeper,79,org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider.add,285,error,java.lang.String(name)invalid delta {} for metric {}-long(name)delta-java.lang.String(name)name-java.lang.IllegalArgumentException(name)err,o,o,x
Zookeeper,75,org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider.configure,74,info,"java.lang.String(name)Initializing metrics, configuration: {}-java.util.Properties(name)configuration",o,o,o
Zookeeper,78,org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider.stop,123,error,java.lang.String(name)Cannot safely stop Jetty server-java.lang.Exception(name)err,o,o,o
Zookeeper,1218,org.apache.zookeeper.PortAssignment.setupPortRange,138,warn,java.lang.String(name)Error parsing threadid from {}.-java.lang.String(name)cmdLine-java.lang.NumberFormatException(name)e,o,o,x
Zookeeper,90,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.becomeLeader,265,info,java.lang.String(name)Becoming leader with node: {}-java.lang.String(name)getLeaderOffer().getNodePath(),o,o,o
Zookeeper,88,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.becomeReady,243,debug,java.lang.String(name)We're behind {} in line and they're alive. Keeping an eye on them.-java.lang.String(name)neighborLeaderOffer.getNodePath(),o,o,o
Zookeeper,89,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.becomeReady,253,info,java.lang.String(name)We were behind {} but it looks like they died. Back to determination.-java.lang.String(name)neighborLeaderOffer.getNodePath(),o,o,o
Zookeeper,87,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.becomeReady,230,info,java.lang.String(name){} not elected leader. Watching node: {}-java.lang.String(name)getLeaderOffer().getNodePath()-java.lang.String(name)neighborLeaderOffer.getNodePath(),o,o,o
Zookeeper,86,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.determineElectionStatus,211,debug,java.lang.String(name)There are {} leader offers. I am {} in line.-int(name)leaderOffers.size()-int(name)i,o,x,x
Zookeeper,92,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.process,331,debug,java.lang.String(name)Node {} deleted. Need to run through the election process.-java.lang.String(name)event.getPath(),o,o,o
Zookeeper,82,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.start,123,info,java.lang.String(name)Starting leader election support,o,o,o
Zookeeper,83,org.apache.zookeeper.recipes.leader.LeaderElectionSupport.stop,151,info,java.lang.String(name)Stopping leader election support,o,o,o
Zookeeper,97,org.apache.zookeeper.recipes.lock.ProtocolSupport.retryOperation,134,debug,java.lang.String(name)Attempt {} failed with connection loss. Reconnecting...-int(name)i,o,o,o
Zookeeper,95,org.apache.zookeeper.recipes.lock.ZNodeName.ZNodeName,52,warn,java.lang.String(name)Array out of bounds for {}.-int(name)idx-java.lang.ArrayIndexOutOfBoundsException(name)e,o,o,o
Zookeeper,110,org.apache.zookeeper.recipes.queue.DistributedQueue.orderedChildren,82,warn,java.lang.String(name)Found child node with improper format : {}-java.lang.String(name)childName-java.lang.NumberFormatException(name)e,o,o,o
Zookeeper,109,org.apache.zookeeper.recipes.queue.DistributedQueue.orderedChildren,75,warn,java.lang.String(name)Found child node with improper name: {}-java.lang.String(name)childName,o,o,o
Zookeeper,63,org.apache.zookeeper.retry.ZooKeeperRetry.create,83,warn,java.lang.String(name)ZooKeeper connection lost. Trying to reconnect.,o,o,o
Zookeeper,126,org.apache.zookeeper.SaslClientCallbackHandler.handle,60,warn,"java.lang.String(name)Could not login: the {} is being asked for a password, but the ZooKeeper {} code does not currently support obtaining a password from the user. Make sure that the {} is configured to use a ticket cache (using the JAAS configuration setting 'useTicketCache=true)' and restart the {}. If you still get this message after that, the TGT in the ticket cache has expired and must be manually refreshed. To do so, first determine if you are using a password or a keytab. If the former, run kinit in a Unix shell in the environment of the user who is running this Zookeeper {} using the command 'kinit <princ>' (where <princ> is the name of the {}'s Kerberos principal). If the latter, do 'kinit -k -t <keytab> <princ>' (where <princ> is the name of the Kerberos principal, and <keytab> is the location of the keytab file). After manually refreshing your cache, restart this {}. If you continue to see this message after manually refreshing your cache, ensure that your KDC host's clock is in sync with this host's clock.-java.lang.String(name)entity-java.lang.String(name)entity-java.lang.String(name)entity-java.lang.String(name)entity-java.lang.String(name)entity-java.lang.String(name)entity-java.lang.String(name)entity",x,x,x
Zookeeper,629,org.apache.zookeeper.server.admin.JettyAdminServer.JettyAdminServer,128,info,java.lang.String(name)Successfully loaded private key from {}-java.lang.String(name)privateKeyPath,o,o,o
Zookeeper,630,org.apache.zookeeper.server.admin.JettyAdminServer.JettyAdminServer,129,info,java.lang.String(name)Successfully loaded certificate authority from {}-java.lang.String(name)certAuthPath,o,o,o
Zookeeper,639,org.apache.zookeeper.server.admin.UnifiedConnectionFactory.newConnection,76,warn,java.lang.String(name)Incoming connection has no data,o,o,o
Zookeeper,648,org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback,119,info,java.lang.String(name)Successfully authenticated client: authenticationID={}; authorizationID={}.-java.lang.String(name)authenticationID-java.lang.String(name)authorizationID,o,o,o
Zookeeper,649,org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback,135,info,java.lang.String(name)Setting authorizedID: {}-java.lang.StringBuilder(name)userNameBuilder,o,o,o
Zookeeper,646,org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handlePasswordCallback,106,warn,java.lang.String(name)No password found for user: {}-java.lang.String(name)userName,o,o,o
Zookeeper,657,org.apache.zookeeper.server.auth.X509AuthenticationProvider.handleAuthentication,160,info,java.lang.String(name)Authenticated Id '{}' as super user-java.lang.String(name)clientId,o,o,o
Zookeeper,658,org.apache.zookeeper.server.auth.X509AuthenticationProvider.handleAuthentication,166,info,java.lang.String(name)Authenticated Id '{}' for Scheme '{}'-java.lang.String(name)authInfo.getId()-java.lang.String(name)authInfo.getScheme(),o,o,o
Zookeeper,618,org.apache.zookeeper.server.BlueThrottle.logWeighedThrottlingSetting,132,info,java.lang.String(name)Weighed connection throttling is enabled. But it will only be effective if connection throttling is enabled,o,o,o
Zookeeper,669,org.apache.zookeeper.server.command.FourLetterCommands.isEnabled,223,info,"java.lang.String(name)The list of known four letter word commands is : {}-java.util.List<Map<Integer,String>>(name)Arrays.asList(cmd2String)",o,o,o
Zookeeper,670,org.apache.zookeeper.server.command.FourLetterCommands.isEnabled,224,info,java.lang.String(name)The list of enabled four letter word commands is : {}-java.util.List<Set<String>>(name)Arrays.asList(whiteListedCommands),o,o,o
Zookeeper,358,org.apache.zookeeper.server.ConnectionBean.terminateSession,103,warn,java.lang.String(name)Unable to closeSession() for session: 0x{}-java.lang.String(name)getSessionId()-java.lang.Exception(name)e,o,o,o
Zookeeper,624,org.apache.zookeeper.server.ContainerManager.checkContainers,118,info,java.lang.String(name)Attempting to delete candidate container: {}-java.lang.String(name)containerPath,o,o,o
Zookeeper,623,org.apache.zookeeper.server.ContainerManager.start,86,error,java.lang.String(name)Error checking containers-java.lang.Throwable(name)e,o,o,o
Zookeeper,622,org.apache.zookeeper.server.ContainerManager.start,83,info,java.lang.String(name)interrupted,o,x,x
Zookeeper,575,org.apache.zookeeper.server.DataTree.deserializeZxidDigest,1698,warn,"java.lang.String(name)Got EOF exception while reading the digest, likely due to the reading an older snapshot.",o,o,x
Zookeeper,571,org.apache.zookeeper.server.DataTree.killSession,1155,warn,java.lang.String(name)Unexpected extra paths under session {} which are not in txn 0x{}-java.util.Set<String>(name)paths2DeleteLocal-java.lang.String(name)Long.toHexString(zxid),o,o,x
Zookeeper,566,org.apache.zookeeper.server.DataTree.processTxn,1044,debug,java.lang.String(name)Failed: {}:{}-org.apache.zookeeper.txn.TxnHeader(name)header-org.apache.jute.Record(name)txn-org.apache.zookeeper.KeeperException(name)e,o,x,o
Zookeeper,567,org.apache.zookeeper.server.DataTree.processTxn,1047,debug,java.lang.String(name)Failed: {}:{}-org.apache.zookeeper.txn.TxnHeader(name)header-org.apache.jute.Record(name)txn-java.io.IOException(name)e,o,x,o
Zookeeper,562,org.apache.zookeeper.server.DataTree.updateCountBytes,382,error,java.lang.String(name)Missing count node for stat {}-java.lang.String(name)statNode,o,o,o
Zookeeper,563,org.apache.zookeeper.server.DataTree.updateCountBytes,397,error,java.lang.String(name)Missing count node for quota {}-java.lang.String(name)quotaNode,o,o,o
Zookeeper,310,org.apache.zookeeper.server.FinalRequestProcessor.processRequest,186,debug,java.lang.String(name){}-org.apache.zookeeper.server.Request(name)request,x,x,x
Zookeeper,313,org.apache.zookeeper.server.FinalRequestProcessor.processRequest,572,error,java.lang.String(name)FIXMSG-java.io.IOException(name)e,x,x,x
Zookeeper,24,org.apache.zookeeper.server.jersey.resources.SessionsResource.createSession,107,error,java.lang.String(name)Failed while trying to create a new session-java.io.IOException(name)e,o,o,o
Zookeeper,23,org.apache.zookeeper.server.jersey.RestMain.start,80,error,"java.lang.String(name)""Unable to load keystore: "" + jks-java.net.URISyntaxException(name)e1",o,o,o
Zookeeper,20,org.apache.zookeeper.server.jersey.ZooKeeperService.close,159,error,java.lang.String(name)Interrupted while closing ZooKeeper connection.-java.lang.InterruptedException(name)e,o,o,o
Zookeeper,320,org.apache.zookeeper.server.NettyServerCnxn.close,112,debug,java.lang.String(name)close in progress for session id: 0x{}-java.lang.String(name)Long.toHexString(sessionId),o,o,x
Zookeeper,318,org.apache.zookeeper.server.NettyServerCnxn.close,97,debug,java.lang.String(name)close called for session id: 0x{}-java.lang.String(name)Long.toHexString(sessionId),o,o,x
Zookeeper,319,org.apache.zookeeper.server.NettyServerCnxn.close,108,debug,java.lang.String(name)cnxns size:{}-int(name)factory.cnxns.size(),o,o,x
Zookeeper,327,org.apache.zookeeper.server.NettyServerCnxn.processMessage,342,debug,java.lang.String(name)allocating queue,o,x,x
Zookeeper,329,org.apache.zookeeper.server.NettyServerCnxn.processMessage,350,debug,java.lang.String(name)not throttled,o,x,x
Zookeeper,335,org.apache.zookeeper.server.NettyServerCnxn.processQueuedBuffer,391,debug,java.lang.String(name)Processed queue - no bytes remaining,o,o,o
Zookeeper,337,org.apache.zookeeper.server.NettyServerCnxn.processQueuedBuffer,400,debug,java.lang.String(name)queue empty,o,o,x
Zookeeper,342,org.apache.zookeeper.server.NettyServerCnxn.receiveMessage,465,debug,java.lang.String(name)got conn req request from {}-java.net.InetSocketAddress(name)getRemoteSocketAddress(),o,x,x
Zookeeper,338,org.apache.zookeeper.server.NettyServerCnxn.receiveMessage,431,trace,java.lang.String(name)message readable {} bb len {} {}-int(name)message.readableBytes()-int(name)bb.remaining()-java.nio.ByteBuffer(name)bb,x,o,x
Zookeeper,607,org.apache.zookeeper.server.NettyServerCnxnFactory.closeAll,508,debug,java.lang.String(name)closeAll(),x,x,x
Zookeeper,606,org.apache.zookeeper.server.NettyServerCnxnFactory.initSSL,502,debug,java.lang.String(name)SSL handler added for channel: {}-io.netty.channel.Channel(name)p.channel(),o,o,o
Zookeeper,601,org.apache.zookeeper.server.NettyServerCnxnFactory.NettyServerCnxnFactory,435,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.client.portUnification-boolean(name)usePortUnification,x,x,x
Zookeeper,580,org.apache.zookeeper.server.NettyServerCnxnFactory.newNonSslHandler,166,debug,java.lang.String(name)creating plaintext handler for session {}-long(name)cnxn.getSessionId(),o,o,x
Zookeeper,579,org.apache.zookeeper.server.NettyServerCnxnFactory.newSslHandler,156,debug,java.lang.String(name)creating ssl handler for session {}-long(name)cnxn.getSessionId(),o,o,x
Zookeeper,588,org.apache.zookeeper.server.NettyServerCnxnFactory.userEventTriggered,252,debug,java.lang.String(name)Issued a read after queuedBuffer drained,o,o,o
Zookeeper,369,org.apache.zookeeper.server.NIOServerCnxn.checkFourLetterWord,496,debug,java.lang.String(name)Command {} is not executed because it is not in the whitelist.-java.lang.String(name)cmd,o,o,o
Zookeeper,362,org.apache.zookeeper.server.NIOServerCnxn.doIO,317,warn,java.lang.String(name)trying to do i/o on a null socket for session: 0x{}-java.lang.String(name)Long.toHexString(sessionId),o,o,x
Zookeeper,468,org.apache.zookeeper.server.NIOServerCnxnFactory.cleanupSelectionKey,148,debug,java.lang.String(name)ignoring exception during selectionkey cancel-java.lang.Exception(name)ex,o,o,x
Zookeeper,467,org.apache.zookeeper.server.NIOServerCnxnFactory.closeSelector,139,warn,java.lang.String(name)ignored exception during selector close.-java.io.IOException(name)e,o,o,x
Zookeeper,469,org.apache.zookeeper.server.NIOServerCnxnFactory.fastCloseSock,159,warn,"java.lang.String(name)Unable to set socket linger to 0, socket close may stall in CLOSE_WAIT-java.net.SocketException(name)e",o,o,o
Zookeeper,472,org.apache.zookeeper.server.NIOServerCnxnFactory.run,209,info,java.lang.String(name)accept thread exitted run method,o,o,x
Zookeeper,698,org.apache.zookeeper.server.persistence.FileSnap.deserialize,96,warn,java.lang.String(name)problem reading snap file {}-java.io.File(name)snap-java.io.IOException(name)e,o,o,x
Zookeeper,682,org.apache.zookeeper.server.persistence.FileTxnLog.createInputArchive,735,debug,java.lang.String(name)Created new input archive: {}-java.io.File(name)logFile,o,o,o
Zookeeper,690,org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits,318,error,java.lang.String(name){}(highestZxid) > {}(next log) for type {}-long(name)highestZxid-long(name)hdr.getZxid()-int(name)hdr.getType(),x,x,x
Zookeeper,691,org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits,344,info,java.lang.String(name){} txns loaded in {} ms-int(name)txnLoaded-long(name)loadTime,o,o,x
Zookeeper,686,org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore,238,info,"java.lang.String(name)Initialize file found, an empty database will not block voting participation",o,o,o
Zookeeper,689,org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore,279,warn,"java.lang.String(name)Highest txn zxid 0x{} is not covering the snapshot digest zxid 0x{}, which might lead to inconsistent state-java.lang.String(name)Long.toHexString(highestZxid)-java.lang.String(name)Long.toHexString(snapshotZxidDigest.getZxid())",o,o,x
Zookeeper,694,org.apache.zookeeper.server.persistence.FileTxnSnapLog.save,462,info,java.lang.String(name)Deleted empty snapshot file: {}-java.lang.String(name)snapshotFile.getAbsolutePath(),o,o,o
Zookeeper,701,org.apache.zookeeper.server.persistence.SnapStream.isValidGZipStream,246,error,java.lang.String(name)Unable to open file {}-java.lang.String(name)f.getName()-java.io.FileNotFoundException(name)e,o,o,o
Zookeeper,535,org.apache.zookeeper.server.PrepRequestProcessor.pRequest,752,info,java.lang.String(name)Got user-level KeeperException when processing {} aborting remaining multi ops. Error Path:{} Error:{}-java.lang.String(name)request.toString()-java.lang.String(name)e.getPath()-java.lang.String(name)e.getMessage(),o,o,o
Zookeeper,531,org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn,481,warn,java.lang.String(name)msg,x,x,x
Zookeeper,532,org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn,487,warn,java.lang.String(name)msg2,x,x,x
Zookeeper,524,org.apache.zookeeper.server.PrepRequestProcessor.validatePathForCreate,284,info,java.lang.String(name)Invalid path {} with session 0x{}-java.lang.String(name)path-java.lang.String(name)Long.toHexString(sessionId),o,o,o
Zookeeper,1223,org.apache.zookeeper.server.quorum.auth.MiniKdc.MiniKdc,225,info,java.lang.String(name) {}: {}-(name)entry.getKey()-(name)entry.getValue(),x,x,x
Zookeeper,1221,org.apache.zookeeper.server.quorum.auth.MiniKdc.MiniKdc,222,info,java.lang.String(name)Configuration:,o,x,o
Zookeeper,1222,org.apache.zookeeper.server.quorum.auth.MiniKdc.MiniKdc,223,info,java.lang.String(name)---------------------------------------------------------------,x,x,x
Zookeeper,1224,org.apache.zookeeper.server.quorum.auth.MiniKdc.MiniKdc,227,info,java.lang.String(name)---------------------------------------------------------------,x,x,x
Zookeeper,1152,org.apache.zookeeper.server.quorum.auth.QuorumAuth.getStatus,72,error,java.lang.String(name)Unknown status:{}!-int(name)status,o,o,o
Zookeeper,1147,org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.authenticate,122,warn,java.lang.String(name)Unknown status:{}!-org.apache.zookeeper.server.quorum.auth.Status(name)qpStatus,o,o,o
Zookeeper,1149,org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.checkAuthStatus,142,info,"java.lang.String(name)Successfully completed the authentication using SASL. server addr: {}, status: {}-java.net.SocketAddress(name)sock.getRemoteSocketAddress()-org.apache.zookeeper.server.quorum.auth.Status(name)qpStatus",o,o,x
Zookeeper,1150,org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.createSaslToken,181,debug,java.lang.String(name)saslClient.evaluateChallenge(len={})-int(name)saslToken.length,x,x,x
Zookeeper,1151,org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.createSaslToken,197,error,java.lang.String(name)error,x,x,x
Zookeeper,1159,org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler.handleAuthorizeCallback,146,debug,"java.lang.String(name)SASL authorization completed, authorized flag set to {}-boolean(name)ac.isAuthorized()",o,o,o
Zookeeper,1157,org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler.handleAuthorizeCallback,135,error,"java.lang.String(name)SASL authorization completed, {} is not authorized to connect-java.lang.String(name)components[1]",o,o,o
Zookeeper,1153,org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler.SaslQuorumServerCallbackHandler,60,error,java.lang.String(name)errorMessage,x,x,x
Zookeeper,888,org.apache.zookeeper.server.quorum.AuthFastLeaderElection.process,690,warn,java.lang.String(name)unknown type {}-int(name)m.type,o,o,x
Zookeeper,877,org.apache.zookeeper.server.quorum.AuthFastLeaderElection.run,363,error,java.lang.String(name)Empty ack semaphore,o,o,x
Zookeeper,878,org.apache.zookeeper.server.quorum.AuthFastLeaderElection.run,373,warn,java.lang.String(name)No such address in the ensemble configuration {}-java.net.SocketAddress(name)responsePacket.getSocketAddress(),o,o,o
Zookeeper,976,org.apache.zookeeper.server.quorum.CommitProcessor.start,437,info,"java.lang.String(name)Configuring CommitProcessor with {} worker threads.-java.lang.Object(name)numWorkerThreads > 0 ? numWorkerThreads : ""no""",o,o,o
Zookeeper,1017,org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader,1078,warn,"java.lang.String(name)Notification state unrecoginized: {} (n.state), {}(n.sid)-org.apache.zookeeper.server.quorum.ServerState(name)n.state-long(name)n.sid",x,o,o
Zookeeper,991,org.apache.zookeeper.server.quorum.FastLeaderElection.run,315,error,java.lang.String(name)Something went wrong while processing config received from {}-long(name)response.sid,o,o,o
Zookeeper,1160,org.apache.zookeeper.server.quorum.flexible.QuorumHierarchical.QuorumHierarchical,139,info,"java.lang.String(name){}, {}, {}-int(name)serverWeight.size()-int(name)serverGroup.size()-int(name)groupWeight.size()",x,x,x
Zookeeper,846,org.apache.zookeeper.server.quorum.Follower.followLeader,95,error,"java.lang.String(name)""Proposed leader epoch "" + ZxidUtils.zxidToString(newEpochZxid) + "" is less than our accepted epoch ""+ ZxidUtils.zxidToString(self.getAcceptedEpoch())",o,o,o
Zookeeper,849,org.apache.zookeeper.server.quorum.Follower.followLeader,141,info,java.lang.String(name)Disconnected from leader (with address: {}). Was connected for {}ms. Sync state: {}-java.net.InetSocketAddress(name)leaderAddr-long(name)connectionDuration-boolean(name)completedSync,x,o,o
Zookeeper,1112,org.apache.zookeeper.server.quorum.FollowerRequestProcessor.shutdown,148,info,java.lang.String(name)Shutting down,o,x,o
Zookeeper,777,org.apache.zookeeper.server.quorum.Leader.processAck,928,debug,java.lang.String(name)outstanding is 0,o,x,x
Zookeeper,762,org.apache.zookeeper.server.quorum.Leader.run,468,warn,java.lang.String(name)Error closing socket-java.io.IOException(name)e,o,o,o
Zookeeper,1118,org.apache.zookeeper.server.quorum.Learner.connectToLeader,309,warn,"java.lang.String(name)Unexpected exception, tries={}, remaining init limit={}, connecting to {}-int(name)tries-int(name)remainingTimeout-java.net.InetSocketAddress(name)addr-java.io.IOException(name)e",o,o,o
Zookeeper,1114,org.apache.zookeeper.server.quorum.Learner.findLeader,227,warn,java.lang.String(name)Couldn't find the leader with id = {}-long(name)current.getId(),o,o,o
Zookeeper,1126,org.apache.zookeeper.server.quorum.Learner.syncWithLeader,463,error,"java.lang.String(name)Got unexpected packet from leader: {}, exiting ... -java.lang.String(name)LearnerHandler.packetToString(qp)",o,o,o
Zookeeper,750,org.apache.zookeeper.server.quorum.LearnerHandler.ping,1086,warn,java.lang.String(name)Closing connection to peer due to transaction timeout.,o,o,o
Zookeeper,718,org.apache.zookeeper.server.quorum.LearnerHandler.run,574,info,"java.lang.String(name)Sending snapshot last zxid of peer is 0x{}, zxid of leader is 0x{}, send zxid of db as 0x{}, {} concurrent snapshot sync, snapshot sync was {} from throttle-java.lang.String(name)Long.toHexString(peerLastZxid)-java.lang.String(name)Long.toHexString(leaderLastZxid)-java.lang.String(name)Long.toHexString(zxidToSend)-int(name)syncThrottler.getSyncInProgress()-java.lang.Object(name)exemptFromThrottle ? ""exempt"" : ""not exempt""",o,o,x
Zookeeper,724,org.apache.zookeeper.server.quorum.LearnerHandler.run,710,warn,"java.lang.String(name)unexpected quorum packet, type: {}-java.lang.String(name)packetToString(qp)",o,o,x
Zookeeper,711,org.apache.zookeeper.server.quorum.LearnerHandler.sendPackets,370,warn,java.lang.String(name)Error closing socket for handler {}-org.apache.zookeeper.server.quorum.LearnerHandler(name)this-java.io.IOException(name)ie,o,o,o
Zookeeper,742,org.apache.zookeeper.server.quorum.LearnerHandler.syncFollower,936,error,java.lang.String(name)Unhandled scenario for peer sid: {} fall back to use snapshot-long(name)getSid(),o,o,o
Zookeeper,736,org.apache.zookeeper.server.quorum.LearnerHandler.syncFollower,873,info,java.lang.String(name)Using committedLog for peer sid: {}-long(name)getSid(),o,o,o
Zookeeper,1030,org.apache.zookeeper.server.quorum.Observer.findLearnerMaster,150,warn,java.lang.String(name)requested next learner master {} is no longer valid-org.apache.zookeeper.server.quorum.QuorumServer(name)prescribedLearnerMaster,o,o,x
Zookeeper,1038,org.apache.zookeeper.server.quorum.Observer.waitForReconnectDelayHelper,247,info,java.lang.String(name)Waiting for {} ms before reconnecting with the leader-long(name)randomDelay,o,o,o
Zookeeper,802,org.apache.zookeeper.server.quorum.ObserverMaster.startForwarding,317,info,"java.lang.String(name)finished syncing observer from retained commit queue: sid {}, queue head 0x{}, queue tail 0x{}, sync position 0x{}, num packets used {}, num bytes used {}-long(name)learnerHandler.getSid()-java.lang.String(name)Long.toHexString(queueHeadZxid)-java.lang.String(name)Long.toHexString(packet.getZxid())-java.lang.String(name)Long.toHexString(lastSeenZxid)-long(name)packet.getZxid() - lastSeenZxid-long(name)queueBytesUsed",o,o,x
Zookeeper,863,org.apache.zookeeper.server.quorum.ObserverZooKeeperServer.ObserverZooKeeperServer,55,info,java.lang.String(name)syncEnabled ={}-boolean(name)syncRequestProcessorEnabled,x,x,x
Zookeeper,1135,org.apache.zookeeper.server.quorum.ProposalRequestProcessor.shutdown,87,info,java.lang.String(name)Shutting down,o,x,o
Zookeeper,1100,org.apache.zookeeper.server.quorum.QuorumCnxManager.addToSendQueue,1242,debug,java.lang.String(name)Trying to remove from an empty Queue. Ignoring exception.-java.util.NoSuchElementException(name)ne,o,o,o
Zookeeper,1101,org.apache.zookeeper.server.quorum.QuorumCnxManager.addToSendQueue,1249,error,java.lang.String(name)Unable to insert an element in the queue -java.lang.IllegalStateException(name)ie,o,o,o
Zookeeper,1056,org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne,618,debug,java.lang.String(name)There is a connection already for server {}-long(name)sid,o,o,o
Zookeeper,1058,org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne,635,info,java.lang.String(name)SSL handshake complete with {} - {} - {}-java.net.SocketAddress(name)sslSock.getRemoteSocketAddress()-java.lang.String(name)sslSock.getSession().getProtocol()-java.lang.String(name)sslSock.getSession().getCipherSuite(),o,o,o
Zookeeper,1060,org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne,657,warn,java.lang.String(name)Cannot open channel to {} at election address {}-long(name)sid-java.net.InetSocketAddress(name)electionAddr-java.nio.channels.UnresolvedAddressException(name)e,o,o,o
Zookeeper,1067,org.apache.zookeeper.server.quorum.QuorumCnxManager.halt,749,warn,java.lang.String(name)Got interrupted before joining the listener-java.lang.InterruptedException(name)ex,o,o,o
Zookeeper,1055,org.apache.zookeeper.server.quorum.QuorumCnxManager.handleConnection,554,debug,java.lang.String(name)Create new connection to server: {}-java.lang.Long(name)sid,o,o,o
Zookeeper,1065,org.apache.zookeeper.server.quorum.QuorumCnxManager.haveDelivered,728,debug,java.lang.String(name)Queue size: {}-int(name)queue.size(),o,o,o
Zookeeper,1045,org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection,329,error,"java.lang.String(name)Exception while connecting, id: {}, addr: {}, closing learner connection-java.lang.Long(name)sid-java.net.SocketAddress(name)sock.getRemoteSocketAddress()-java.io.IOException(name)e",o,o,x
Zookeeper,1046,org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnectionAsync,346,debug,"java.lang.String(name)Connection request to server id: {} is already in progress, so skipping this request-java.lang.Long(name)sid",o,o,o
Zookeeper,1071,org.apache.zookeeper.server.quorum.QuorumCnxManager.Listener,846,info,java.lang.String(name)'{}' contains invalid value: {}(must be >= 0). Use default value of {} instead.-java.lang.String(name)zookeeper.electionPortBindRetry-java.lang.Integer(name)maxRetry-int(name)3,x,o,o
Zookeeper,1081,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,940,error,java.lang.String(name)As I'm leaving the listener thread after {} errors. I won't be able to participate in leader election any longer: {}.Use {} property to increase retry count.-int(name)numRetries-java.lang.String(name)formatInetAddr(self.getElectionAddress())-java.lang.String(name)zookeeper.electionPortBindRetry,o,x,x
Zookeeper,1092,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,1097,error,java.lang.String(name)Failed to send last message. Shutting down thread.-java.io.IOException(name)e,o,o,o
Zookeeper,1093,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,1110,error,java.lang.String(name)No queue of incoming messages for server {}-java.lang.Long(name)sid,o,o,o
Zookeeper,1076,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,914,warn,"java.lang.String(name)The socket is listening for the election accepted and it timed out unexpectedly, but will retry.see ZOOKEEPER-2836",o,o,x
Zookeeper,1098,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,1204,warn,"java.lang.String(name)Connection broken for id {}, my id = {}-java.lang.Long(name)sid-long(name)QuorumCnxManager.this.mySid-java.lang.Exception(name)e",o,o,o
Zookeeper,1099,org.apache.zookeeper.server.quorum.QuorumCnxManager.run,1210,warn,java.lang.String(name)Interrupting SendWorker,o,o,o
Zookeeper,1090,org.apache.zookeeper.server.quorum.QuorumCnxManager.send,1063,error,java.lang.String(name)BufferUnderflowException -java.nio.BufferUnderflowException(name)be,x,x,x
Zookeeper,1049,org.apache.zookeeper.server.quorum.QuorumCnxManager.startConnection,423,info,"java.lang.String(name)Have smaller server identifier, so dropping the connection: ({}, {})-java.lang.Long(name)sid-long(name)self.getId()",o,o,o
Zookeeper,950,org.apache.zookeeper.server.quorum.QuorumPeer.getSyncEnabled,1891,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.observer.syncEnabled-boolean(name)Boolean.getBoolean(SYNC_ENABLED),x,x,x
Zookeeper,915,org.apache.zookeeper.server.quorum.QuorumPeer.run,1311,info,java.lang.String(name)LOOKING,x,x,x
Zookeeper,921,org.apache.zookeeper.server.quorum.QuorumPeer.run,1374,info,java.lang.String(name)OBSERVING,x,x,x
Zookeeper,923,org.apache.zookeeper.server.quorum.QuorumPeer.run,1393,info,java.lang.String(name)FOLLOWING,x,x,x
Zookeeper,925,org.apache.zookeeper.server.quorum.QuorumPeer.run,1405,info,java.lang.String(name)LEADING,x,x,x
Zookeeper,897,org.apache.zookeeper.server.quorum.QuorumPeer.run,726,warn,java.lang.String(name)Got more than just an xid! Len = {}-int(name)packet.getLength(),o,o,o
Zookeeper,919,org.apache.zookeeper.server.quorum.QuorumPeer.run,1350,warn,java.lang.String(name)Unexpected exception-java.lang.Exception(name)e,o,x,o
Zookeeper,920,org.apache.zookeeper.server.quorum.QuorumPeer.run,1367,warn,java.lang.String(name)Unexpected exception-java.lang.Exception(name)e,o,x,o
Zookeeper,922,org.apache.zookeeper.server.quorum.QuorumPeer.run,1378,warn,java.lang.String(name)Unexpected exception-java.lang.Exception(name)e,o,x,o
Zookeeper,924,org.apache.zookeeper.server.quorum.QuorumPeer.run,1397,warn,java.lang.String(name)Unexpected exception-java.lang.Exception(name)e,o,x,o
Zookeeper,926,org.apache.zookeeper.server.quorum.QuorumPeer.run,1411,warn,java.lang.String(name)Unexpected exception-java.lang.Exception(name)e,o,x,o
Zookeeper,944,org.apache.zookeeper.server.quorum.QuorumPeer.setLastSeenQuorumVerifier,1781,error,java.lang.String(name)Error writing next dynamic config file to disk-java.io.IOException(name)e,o,o,o
Zookeeper,946,org.apache.zookeeper.server.quorum.QuorumPeer.setQuorumVerifier,1814,error,java.lang.String(name)Error closing file-java.io.IOException(name)e,o,o,o
Zookeeper,961,org.apache.zookeeper.server.quorum.QuorumPeer.updateLearnerType,2276,info,java.lang.String(name)Becoming an observer,o,x,o
Zookeeper,958,org.apache.zookeeper.server.quorum.QuorumPeer.validateLearnerMaster,2261,info,java.lang.String(name)could not find learner master address={}-java.lang.String(name)desiredMaster,o,o,x
Zookeeper,959,org.apache.zookeeper.server.quorum.QuorumPeer.validateLearnerMaster,2263,warn,java.lang.String(name)could not find learner master sid={}-java.lang.Long(name)sid,o,o,x
Zookeeper,798,org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig,144,info,java.lang.String(name)Starting quorum peer,o,o,o
Zookeeper,799,org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig,221,warn,java.lang.String(name)Quorum Peer interrupted-java.lang.InterruptedException(name)e,o,o,o
Zookeeper,752,org.apache.zookeeper.server.quorum.ReadOnlyRequestProcessor.run,96,error,java.lang.String(name)IO exception while sending response-java.io.IOException(name)e,o,o,o
Zookeeper,809,org.apache.zookeeper.server.quorum.ReadOnlyZooKeeperServer.registerJMX,87,warn,java.lang.String(name)Failed to register with JMX-java.lang.Exception(name)e,o,o,o
Zookeeper,808,org.apache.zookeeper.server.quorum.ReadOnlyZooKeeperServer.startup,77,info,java.lang.String(name)Read-only server started,o,o,o
Zookeeper,807,org.apache.zookeeper.server.quorum.ReadOnlyZooKeeperServer.startup,70,warn,java.lang.String(name)Not starting Read-only server as startup follows shutdown!,o,o,o
Zookeeper,811,org.apache.zookeeper.server.quorum.ReadOnlyZooKeeperServer.unregisterJMX,111,warn,java.lang.String(name)Failed to unregister with JMX-java.lang.Exception(name)e,o,o,o
Zookeeper,754,org.apache.zookeeper.server.quorum.SendAckRequestProcessor.processRequest,48,warn,"java.lang.String(name)Closing connection to leader, exception during packet send-java.io.IOException(name)e",o,o,o
Zookeeper,868,org.apache.zookeeper.server.quorum.UnifiedServerSocket.detectMode,266,info,java.lang.String(name)Accepted TLS connection from {} - {} - {}-java.net.SocketAddress(name)sslSocket.getRemoteSocketAddress()-java.lang.String(name)sslSocket.getSession().getProtocol()-java.lang.String(name)sslSocket.getSession().getCipherSuite(),o,o,o
Zookeeper,306,org.apache.zookeeper.server.RateLogger.rateLimitLog,86,warn,java.lang.String(name)Message:{} Value:{}-java.lang.String(name)msg-java.lang.String(name)value,o,x,o
Zookeeper,551,org.apache.zookeeper.server.ServerCnxnFactory.createFactory,162,info,java.lang.String(name)Using {} as server connection factory-java.lang.String(name)serverCnxnFactoryName,o,o,o
Zookeeper,354,org.apache.zookeeper.server.SessionTrackerImpl.shutdown,247,info,java.lang.String(name)Shutting down,o,x,o
Zookeeper,520,org.apache.zookeeper.server.SyncRequestProcessor.shutdown,267,warn,java.lang.String(name)Got request processor exception during shutdown,o,o,o
Zookeeper,521,org.apache.zookeeper.server.TxnLogProposalIterator.next,84,error,java.lang.String(name)Unable to read txnlog from disk-java.io.IOException(name)e,o,o,o
Zookeeper,1172,org.apache.zookeeper.server.util.JvmPauseMonitor.run,198,warn,"java.lang.String(name)formatMessage(extraSleepTime,gcTimesAfterSleep,gcTimesBeforeSleep)",x,x,x
Zookeeper,1244,org.apache.zookeeper.server.util.PortForwarder.run,245,error,java.lang.String(name)Interrupted to:{}-int(name)to-java.lang.InterruptedException(name)e,o,o,o
Zookeeper,1233,org.apache.zookeeper.server.util.PortForwarder.run,132,error,java.lang.String(name)Unexpected exception-java.net.SocketException(name)e,o,x,o
Zookeeper,1234,org.apache.zookeeper.server.util.PortForwarder.run,135,error,java.lang.String(name)Unexpected exception-java.io.IOException(name)e,o,x,o
Zookeeper,1231,org.apache.zookeeper.server.util.PortForwarder.run,124,error,java.lang.String(name)socket timeout-java.net.SocketTimeoutException(name)e,o,o,x
Zookeeper,1241,org.apache.zookeeper.server.util.PortForwarder.run,222,warn,java.lang.String(name)connection exception local:{} from:{} to:{}-int(name)sock.getLocalPort()-int(name)sock.getPort()-int(name)to-java.net.ConnectException(name)e,o,o,x
Zookeeper,1242,org.apache.zookeeper.server.util.PortForwarder.run,231,warn,java.lang.String(name)unexpected exception local:{} from:{} to:{}-int(name)sock.getLocalPort()-int(name)sock.getPort()-int(name)to-java.io.IOException(name)e,o,o,x
Zookeeper,1232,org.apache.zookeeper.server.util.PortForwarder.run,129,warn,java.lang.String(name)Interrupted-java.lang.InterruptedException(name)e,o,x,o
Zookeeper,1180,org.apache.zookeeper.server.util.RequestPathMetricsCollector.shutdown,173,info,java.lang.String(name)shutdown scheduledExecutor,o,o,x
Zookeeper,1194,org.apache.zookeeper.server.watch.WatcherCleaner.run,158,info,java.lang.String(name)Processing {} dead watchers-int(name)total,o,o,o
Zookeeper,1191,org.apache.zookeeper.server.watch.WatcherCleaner.WatcherCleaner,87,info,"java.lang.String(name)watcherCleanThreshold={}, watcherCleanIntervalInSeconds={}, watcherCleanThreadsNum={}, maxInProcessingDeadWatchers={}-int(name)watcherCleanThreshold-int(name)watcherCleanIntervalInSeconds-int(name)watcherCleanThreadsNum-int(name)maxInProcessingDeadWatchers",o,x,x
Zookeeper,387,org.apache.zookeeper.server.ZKDatabase.calculateTxnLogSizeLimit,357,error,java.lang.String(name)Unable to get size of most recent snapshot,o,o,o
Zookeeper,391,org.apache.zookeeper.server.ZKDatabase.getProposalsFromTxnLog,400,error,java.lang.String(name)Unable to read txnlog from disk-java.io.IOException(name)e,o,o,o
Zookeeper,379,org.apache.zookeeper.server.ZKDatabase.ZKDatabase,124,error,"java.lang.String(name)Error parsing {}, using default value {}-java.lang.String(name)zookeeper.snapshotSizeFactor-double(name)0.33",o,o,o
Zookeeper,383,org.apache.zookeeper.server.ZKDatabase.ZKDatabase,151,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.commitLogCount-int(name)commitLogCount,x,x,x
Zookeeper,378,org.apache.zookeeper.server.ZKDatabase.ZKDatabase,118,warn,"java.lang.String(name)The configured {} is invalid, going to use the default {}-java.lang.String(name)zookeeper.snapshotSizeFactor-double(name)0.33",o,o,o
Zookeeper,461,org.apache.zookeeper.server.ZooKeeperServer.authWriteRequest,2028,debug,java.lang.String(name)Request has an invalid ACL check-org.apache.zookeeper.InvalidACLException(name)e,o,o,o
Zookeeper,413,org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit,1013,warn,"java.lang.String(name)Exception while establishing session, closing-java.lang.Exception(name)e",o,o,o
Zookeeper,426,org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest,1330,warn,java.lang.String(name)Connection request from old client {}; will be dropped if server is in r-o mode-java.net.InetSocketAddress(name)cnxn.getRemoteSocketAddress(),o,o,o
Zookeeper,441,org.apache.zookeeper.server.ZooKeeperServer.processPacket,1549,debug,java.lang.String(name)Authentication succeeded for scheme: {}-java.lang.String(name)scheme,o,o,o
Zookeeper,440,org.apache.zookeeper.server.ZooKeeperServer.processPacket,1544,warn,java.lang.String(name)Caught runtime exception from AuthenticationProvider: {}-java.lang.String(name)scheme-java.lang.RuntimeException(name)e,o,o,o
Zookeeper,444,org.apache.zookeeper.server.ZooKeeperServer.processPacket,1560,warn,java.lang.String(name)Authentication failed for scheme: {}-java.lang.String(name)scheme,o,o,o
Zookeeper,446,org.apache.zookeeper.server.ZooKeeperServer.processSasl,1614,debug,java.lang.String(name)Size of client SASL token: {}-int(name)clientToken.length,o,o,o
Zookeeper,453,org.apache.zookeeper.server.ZooKeeperServer.processSasl,1660,debug,java.lang.String(name)Size of server SASL response: {}-int(name)responseToken.length,o,o,o
Zookeeper,447,org.apache.zookeeper.server.ZooKeeperServer.processSasl,1625,info,java.lang.String(name)adding SASL authorization for authorizationID: {}-java.lang.String(name)authorizationID,o,o,x
Zookeeper,450,org.apache.zookeeper.server.ZooKeeperServer.processSasl,1639,warn,"java.lang.String(name)Closing client connection due to server requires client SASL authenticaiton,but client SASL authentication has failed, or client is not configured with SASL authentication.",o,o,o
Zookeeper,451,org.apache.zookeeper.server.ZooKeeperServer.processSasl,1645,warn,java.lang.String(name)Closing client connection due to SASL authentication failure.,o,o,o
Zookeeper,431,org.apache.zookeeper.server.ZooKeeperServer.setFlushDelay,1406,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.flushDelay-long(name)delay,x,x,x
Zookeeper,436,org.apache.zookeeper.server.ZooKeeperServer.setLargeRequestMaxBytes,1443,info,java.lang.String(name)The max bytes for all large requests are set to {}-int(name)largeRequestMaxBytes,o,o,o
Zookeeper,434,org.apache.zookeeper.server.ZooKeeperServer.setLargeRequestMaxBytes,1439,warn,java.lang.String(name)Invalid max bytes for all large requests {}. It should be a positive number.-int(name)bytes,o,o,o
Zookeeper,435,org.apache.zookeeper.server.ZooKeeperServer.setLargeRequestMaxBytes,1440,warn,java.lang.String(name)Will not change the setting. The max bytes stay at {}-int(name)largeRequestMaxBytes,o,o,o
Zookeeper,438,org.apache.zookeeper.server.ZooKeeperServer.setLargeRequestThreshold,1457,info,java.lang.String(name)The large request threshold is set to {}-int(name)largeRequestThreshold,o,o,o
Zookeeper,437,org.apache.zookeeper.server.ZooKeeperServer.setLargeRequestThreshold,1453,warn,java.lang.String(name)Invalid large request threshold {}. It should be -1 or positive. Setting to -1 -int(name)threshold,o,o,o
Zookeeper,433,org.apache.zookeeper.server.ZooKeeperServer.setMaxBatchSize,1424,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.maxBatchSize-int(name)size,x,x,x
Zookeeper,432,org.apache.zookeeper.server.ZooKeeperServer.setMaxWriteQueuePollTime,1415,info,java.lang.String(name){}={}-java.lang.String(name)zookeeper.maxWriteQueuePollTime-long(name)maxTime,x,x,x
Zookeeper,405,org.apache.zookeeper.server.ZooKeeperServer.shutdown,774,info,java.lang.String(name)shutting down,o,x,x
Zookeeper,416,org.apache.zookeeper.server.ZooKeeperServer.submitRequestNow,1091,warn,java.lang.String(name)Received packet at server of unknown type {}-int(name)si.type,o,o,o
Zookeeper,395,org.apache.zookeeper.server.ZooKeeperServer.ZooKeeperServer,319,info,java.lang.String(name)Created server with tickTime {} minSessionTimeout {} maxSessionTimeout {} clientPortListenBacklog {} datadir {} snapdir {}-int(name)tickTime-int(name)getMinSessionTimeout()-int(name)getMaxSessionTimeout()-int(name)getClientPortListenBacklog()-java.io.File(name)txnLogFactory.getDataDir()-java.io.File(name)txnLogFactory.getSnapDir(),o,o,o
Zookeeper,359,org.apache.zookeeper.server.ZooKeeperServerListenerImpl.notifyStopping,43,info,"java.lang.String(name)Thread {} exits, error code {}-java.lang.String(name)threadName-int(name)exitCode",o,o,o
Zookeeper,495,org.apache.zookeeper.server.ZooKeeperServerMain.main,76,error,"java.lang.String(name)Unable to access datadir, exiting abnormally-org.apache.zookeeper.server.persistence.DatadirException(name)e",o,o,o
Zookeeper,496,org.apache.zookeeper.server.ZooKeeperServerMain.main,80,error,"java.lang.String(name)Unable to start AdminServer, exiting abnormally-org.apache.zookeeper.server.admin.AdminServerException(name)e",o,o,o
Zookeeper,498,org.apache.zookeeper.server.ZooKeeperServerMain.main,87,info,java.lang.String(name)Exiting normally,o,o,o
Zookeeper,308,org.apache.zookeeper.server.ZooKeeperThread.handleException,55,warn,java.lang.String(name)Exception occurred from thread {}-java.lang.String(name)thName-java.lang.Throwable(name)e,o,o,o
Zookeeper,317,org.apache.zookeeper.server.ZooTrace.logRequest,82,trace,"java.lang.String(name)header + "":"" + rp+ request.toString()",x,x,x
Zookeeper,315,org.apache.zookeeper.server.ZooTrace.setTextTraceLevel,61,info,java.lang.String(name)Set text trace mask to 0x{}-java.lang.String(name)Long.toHexString(mask),o,o,o
Zookeeper,217,org.apache.zookeeper.Shell.runCommand,217,warn,java.lang.String(name)Interrupted while reading the error stream-java.lang.InterruptedException(name)ie,o,o,o
Zookeeper,218,org.apache.zookeeper.Shell.runCommand,235,warn,java.lang.String(name)Error while closing the input stream-java.io.IOException(name)ioe,o,o,o
Zookeeper,219,org.apache.zookeeper.Shell.runCommand,243,warn,java.lang.String(name)Error while closing the error stream-java.io.IOException(name)ioe,o,o,o
Zookeeper,216,org.apache.zookeeper.Shell.runCommand,196,warn,java.lang.String(name)Error reading the error stream-java.io.IOException(name)ioe,o,o,o
Zookeeper,1212,org.apache.zookeeper.util.SecurityUtils.createSaslServer,237,error,java.lang.String(name)Zookeeper Quorum member experienced a PrivilegedActionException exception while creating a SaslServer using a JAAS principal context-java.security.PrivilegedActionException(name)e,o,o,o
Zookeeper,1214,org.apache.zookeeper.util.SecurityUtils.createSaslServer,250,error,java.lang.String(name)Zookeeper Quorum member failed to create a SaslServer to interact with a client during session initiation-javax.security.sasl.SaslException(name)e,o,o,o
Zookeeper,1211,org.apache.zookeeper.util.SecurityUtils.createSaslServer,230,error,java.lang.String(name)Zookeeper Server failed to create a SaslServer to interact with a client during session initiation-javax.security.sasl.SaslException(name)e,o,o,o
Zookeeper,125,org.apache.zookeeper.ZooKeeper.getRemoveWatchesRequest,3101,warn,"java.lang.String(name)""unknown type "" + opCode",o,o,x
Zookeeper,220,org.apache.zookeeper.ZookeeperBanner.printBanner,42,info,java.lang.String(name)line,x,x,x